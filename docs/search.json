[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiki",
    "section": "",
    "text": "In summer 2010, I was walking by the (now closed) pet store in Williamsburg, Brooklyn, and I fell in love with a puppy in the window. There was something about her and somehow I knew right away that she was special and that I needed to get her right that second. She was the best impulse purchase of my life and she turned out to be the most amazing, lovable and gentle creature one could imagine. Kiki was absolutely perfect and we loved her more than anything.\n    She moved with us across country (and then came back), flew on a plane to visit her grandparents in Arizona for Christmases and birthdays, and traveled internationally (OK, we drove with her to Canada a few times, but it still counts). When she got sick and could not longer walk far, we got a stroller to take her on walks because she loved being outside and she loved people watching. She was a fighter - once she was given 6 months to live and she made it into 2 years and we fought for her each and every extra day.\n    It’s been 1 year, 1 month, and 19 days since she’s been gone but I love her just as much and I know I always will."
  },
  {
    "objectID": "mp1.html",
    "href": "mp1.html",
    "title": "Mini-Project 1",
    "section": "",
    "text": "In this part, I created the base table for data analysis using the code provided in the assignment. I also modified column names (Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (Task 2)."
  },
  {
    "objectID": "mp1.html#task-1---creating-syntatic-names",
    "href": "mp1.html#task-1---creating-syntatic-names",
    "title": "Mini-Project 1",
    "section": "Task 1 - Creating Syntatic Names",
    "text": "Task 1 - Creating Syntatic Names"
  },
  {
    "objectID": "mp1.html#task-2---recoding-the-mode-column",
    "href": "mp1.html#task-2---recoding-the-mode-column",
    "title": "Mini-Project 1",
    "section": "",
    "text": "# A tibble: 3 × 2\n  Agency                                                     total_vrm\n  &lt;chr&gt;                                                          &lt;dbl&gt;\n1 MTA New York City Transit                                10832855350\n2 New Jersey Transit Corporation                            5645525525\n3 Los Angeles County Metropolitan Transportation Authority  4354016659\n\n\n\n\n# A tibble: 3 × 2\n  Mode              total_vrm\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Bus             49444494088\n2 Demand Response 17955073508\n3 Heavy Rail      14620362107\n\n\n\n\n# A tibble: 3 × 1\n    total_vrm\n        &lt;dbl&gt;\n1 14620362107\n2          NA\n3          NA"
  },
  {
    "objectID": "mp1.html#testing-gt-package",
    "href": "mp1.html#testing-gt-package",
    "title": "Mini-Project 1",
    "section": "",
    "text": "#if(!require(“gt”)) install.packages(“gt”) #library(gt)\n#sample_n(USAGE, 100) |&gt; # gt()|&gt; # tab_header( # title=“Table 1” # )"
  },
  {
    "objectID": "mp1.html#part-1---getting-data-ready-for-analysis.",
    "href": "mp1.html#part-1---getting-data-ready-for-analysis.",
    "title": "Mini-Project 1",
    "section": "",
    "text": "In this part, I created the base table for data analysis using the provided in the assignment code. I also modified column names (Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (Task 2)."
  },
  {
    "objectID": "mp1.html#part-2---analyzing-transit-data.",
    "href": "mp1.html#part-2---analyzing-transit-data.",
    "title": "Mini-Project 1",
    "section": "Part 2 - Analyzing transit data.",
    "text": "Part 2 - Analyzing transit data.\n\nQ1. What transit agency had the most total VRM in this dataset?\nNot surprisingly, MTA New York City Transit has the largest total VRM in this dataset. Its total of 10.8B+ trips is almost double the total miles recorded by New Jersey Transit Corporation, the agency with the second largest result.\n\n\n# A tibble: 3 × 2\n  Agency                                                     total_vrm\n  &lt;chr&gt;                                                          &lt;dbl&gt;\n1 MTA New York City Transit                                10832855350\n2 New Jersey Transit Corporation                            5645525525\n3 Los Angeles County Metropolitan Transportation Authority  4354016659\n\n\n\n\nQ2. What transit mode had the most total VRM in this dataset?\nWith over 49 billion miles, bus has the most total VRM in this dataset.\n\n\n# A tibble: 3 × 2\n  Mode              total_vrm\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Bus             49444494088\n2 Demand Response 17955073508\n3 Heavy Rail      14620362107\n\n\n\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nThere were slightly over 180 million trips on NYC Subway in May 2024.\n\n\n# A tibble: 1 × 1\n  total_trips\n        &lt;dbl&gt;\n1   180458819\n\n\n\n\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020?\nNYC subway ridership fell by 92% between April 2019 and April 2020 because of Covid pandemic.\n\n\n# A tibble: 1 × 3\n  apr19_trips apr20_trips pct_change\n        &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1   232223929    20254269     -0.913"
  },
  {
    "objectID": "mp1.html#task-1-2---getting-data-ready-for-analysis.",
    "href": "mp1.html#task-1-2---getting-data-ready-for-analysis.",
    "title": "Mini-Project 1",
    "section": "",
    "text": "In this part, I created the base table for data analysis using the code provided in the assignment. I also modified column names (Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (Task 2)."
  },
  {
    "objectID": "mp1.html#task-3---analyzing-transit-data.",
    "href": "mp1.html#task-3---analyzing-transit-data.",
    "title": "Mini-Project 1",
    "section": "Task 3 - Analyzing transit data.",
    "text": "Task 3 - Analyzing transit data.\n\nQ1. What transit agency had the most total VRM in this dataset?\nNot surprisingly, MTA New York City Transit has the largest total mileage in this dataset. Its total of 10.8B+ trips is almost double of the amount attributed to New Jersey Transit Corporation, the agency with the second largest result.\n\n\n\n\n\n\n\n\nTop 3 Transit Agencies by Total VRM\n\n\nAgency\nTOTAL_VRM\n\n\n\n\nMTA New York City Transit\n10,832,855,350\n\n\nNew Jersey Transit Corporation\n5,645,525,525\n\n\nLos Angeles County Metropolitan Transportation Authority\n4,354,016,659\n\n\n\n\n\n\n\n\n\nQ2. What transit mode had the most total VRM in this dataset?\nWith over 49 billion miles, bus has the most total VRM in this dataset.\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(TOTAL_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_VRM = comma(TOTAL_VRM, digits = 0)) |&gt;\n  slice_max(TOTAL_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Modes by Total VRM\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Modes by Total VRM\n\n\nMode\nTOTAL_VRM\n\n\n\n\nBus\n49,444,494,088\n\n\nDemand Response\n17,955,073,508\n\n\nHeavy Rail\n14,620,362,107\n\n\n\n\n\n\n\n\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nThere were slightly over 180 million trips recorded on NYC Subway in May 2024.\n\n\n\n\n\n\n\n\nNYC Subway Trips in May'24\n\n\nTOTAL_TRIPS\n\n\n\n\n180,458,819\n\n\n\n\n\n\n\n\n\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020?\nNYC subway ridership fell by 91% between April 2019 and April 2020 because of Covid pandemic.\n\n\n\n\n\n\n\n\nChange in NYC Subway Ridership\n\n\nAPR19_TRIPS\nAPR20_TRIPS\nPCT_CHANGE\n\n\n\n\n232,223,929\n20,254,269\n-91%"
  },
  {
    "objectID": "mp1.html#task-4---additional-analysis-of-transit-data.",
    "href": "mp1.html#task-4---additional-analysis-of-transit-data.",
    "title": "Mini-Project 1",
    "section": "Task 4 - Additional analysis of transit data.",
    "text": "Task 4 - Additional analysis of transit data."
  },
  {
    "objectID": "mp1.html#task-5---summary-data-for-2022.",
    "href": "mp1.html#task-5---summary-data-for-2022.",
    "title": "Mini-Project 1",
    "section": "Task 5 - Summary data for 2022.",
    "text": "Task 5 - Summary data for 2022.\nIn this part, I created a summary usage table for 2022 and joined it with available financial data."
  },
  {
    "objectID": "mp1.html#task-6---analysis-of-2022-data.",
    "href": "mp1.html#task-6---analysis-of-2022-data.",
    "title": "Mini-Project 1",
    "section": "Task 6 - Analysis of 2022 data.",
    "text": "Task 6 - Analysis of 2022 data.\n\nQ1. Which transit system (agency and mode) had the most UPT in 2022?\nNot surprisingly, NYC Subway had the largest volume of trips in 2022.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nUPT_TOTAL\n\n\n\n\nMTA New York City Transit_Heavy Rail\n1,793,073,801\n\n\nMTA New York City Transit_Bus\n458,602,305\n\n\nLos Angeles County Metropolitan Transportation Authority_Bus\n193,637,448\n\n\n\n\n\n\n\n\n\nQ2.Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nIn 2022, ferryboat managed by Port Imperial Ferry Corp. had the highest farebox recovery ratio of 1.43.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Farebox Recovery\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFAREBOX_RECOVERY\n\n\n\n\nPort Imperial Ferry Corporation_Ferryboat\n1.43\n\n\nHyannis Harbor Tours, Inc._Ferryboat\n1.41\n\n\nTrans-Bridge Lines, Inc._Commuter Bus\n1.33\n\n\n\n\n\n\n\n\n\nQ3.Which transit system (agency and mode) has the lowest expenses per UPT?\nIn 2022, North Carolina State University Bus had the lowest expenses per UPT of $1.18 per unlinked passenger trip.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nEXPENSES_PER_UPT\n\n\n\n\nNorth Carolina State University_Bus\n$1.18\n\n\nAnaheim Transportation Network_Bus\n$1.28\n\n\nUniversity of Iowa_Bus\n$1.54\n\n\n\n\n\n\n\n\n\nQ4.Which transit system (agency and mode) has the highest total fares per UPT?\nIn 2022, ferryboat managed by Cape May Lewes Ferry garnered the highest total fares per UPT of $9.23 per unlinked passenger trip.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFARES_PER_UPT\n\n\n\n\nCape May Lewes Ferry_Ferryboat\n$9.23\n\n\nVirginia Railway Express_Commuter Rail\n$9.01\n\n\nPort Imperial Ferry Corporation_Ferryboat\n$8.90\n\n\n\n\n\n\n\n\n\nQ5.Which transit system (agency and mode) has the lowest expenses per VRM?\nIn 2022, Vanpool managed by Metropolitan Transportation Commission achieved the lowest expenses per VRM of $0.44 per vehicle revenue mile.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per VRM\n\n\n2022\n\n\nTRANSIT_SYSTEM\nEXPENSES_PER_VRM\n\n\n\n\nMetropolitan Transportation Commission_Vanpool\n$0.44\n\n\nSan Joaquin Council_Vanpool\n$0.50\n\n\nSan Diego Association of Governments_Vanpool\n$0.54\n\n\n\n\n\n\n\n\n\nQ6.Which transit system (agency and mode) has the highest total fares per VRM?\nIn 2022, ferryboat managed by Cape May Lewes Ferry achieved higher fares per VRM than any other large transit system with UPT of at least 400,000.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per VRM\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFARES_PER_VRM\n\n\n\n\nCape May Lewes Ferry_Ferryboat\n$93.01\n\n\nWoods Hole, Martha's Vineyard and Nantucket Steamship Authority_Ferryboat\n$91.68\n\n\nAnaheim Transportation Network_Bus\n$9.42\n\n\n\n\n\n\n\n\n\nBased on the farebox recovery ratio, Ferryboat managed by Port Imperial Ferry Corporation appeared to be the most efficient transit system in 2022 among large transit systems with total annual UPT of 400,000+."
  },
  {
    "objectID": "mp1.html#task-4---additional-findings.",
    "href": "mp1.html#task-4---additional-findings.",
    "title": "Mini-Project 1",
    "section": "Task 4 - Additional findings.",
    "text": "Task 4 - Additional findings.\n\nFor additional analysis, I took a deeper look at the NYC transit system data and discovered the following:\n\nBased on the share of UPT, subway has been, by far, the most popular mode of transportation. In any given year, its share of UPT is at least 2X of the second most used transit mode.\nMoreover, based on the changes in the share of UPT, NYC subway usage has been slowly increasing over the years. Between 2002 and 2023, it gained 9 percentage points in relative share, going from 68.1% of total to 76.6% of total, respectively.\nAt the same time, there has been a corresponding decrease in Bus trips. Between 2002 and 2023, it lost 10 percentage points in relative share, going from 31.9% of total to 21.6% of total, respectively.\nBus Rapid Transit and Commuter Bus are newer modes of transportation, with data on their usage not available until 2012.\n\n\n\n\n\n\n\n\n\nNYC Transit System - Relative Shares of UPT by Transportation Mode\n\n\nYEAR\nBus\nDemand Response\nHeavy Rail\nBus Rapid Transit\nCommuter Bus\n\n\n\n\n2002\n31.9%\n0.0%\n68.1%\nNA\nNA\n\n\n2003\n31.0%\n0.0%\n69.0%\nNA\nNA\n\n\n2004\n29.0%\n0.1%\n71.0%\nNA\nNA\n\n\n2005\n30.4%\n0.1%\n69.5%\nNA\nNA\n\n\n2006\n29.0%\n0.1%\n70.9%\nNA\nNA\n\n\n2007\n26.5%\n0.1%\n73.4%\nNA\nNA\n\n\n2008\n26.2%\n0.2%\n73.7%\nNA\nNA\n\n\n2009\n26.3%\n0.2%\n73.5%\nNA\nNA\n\n\n2010\n25.2%\n0.2%\n74.6%\nNA\nNA\n\n\n2011\n24.1%\n0.1%\n75.7%\nNA\nNA\n\n\n2012\n22.5%\n0.1%\n76.0%\n0.9%\n0.4%\n\n\n2013\n22.2%\n0.2%\n76.7%\n0.6%\n0.4%\n\n\n2014\n21.4%\n0.2%\n77.5%\n0.6%\n0.4%\n\n\n2015\n21.6%\n0.2%\n77.3%\n0.6%\n0.4%\n\n\n2016\n21.4%\n0.2%\n77.2%\n0.8%\n0.4%\n\n\n2017\n20.1%\n0.2%\n78.5%\n0.9%\n0.4%\n\n\n2018\n20.5%\n0.2%\n78.0%\n0.9%\n0.4%\n\n\n2019\n20.0%\n0.1%\n78.7%\n0.9%\n0.3%\n\n\n2020\n26.2%\n0.2%\n72.2%\n1.1%\n0.3%\n\n\n2021\n22.7%\n0.1%\n75.9%\n0.9%\n0.3%\n\n\n2022\n20.1%\n0.1%\n78.7%\n0.7%\n0.4%\n\n\n2023\n21.6%\n0.1%\n76.8%\n1.2%\n0.3%\n\n\n2024\n23.0%\n0.1%\n75.2%\n1.4%\n0.4%"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "",
    "text": "In this paper, we will look at high-level fiscal performance of US public transit systems. This work is not meant to be an in-depth analysis of the stated topic but rather an introductory, high-level overview. We will use data from the National Transit Database to analyze main indicators of usage and financial performance and determine what constitutes efficiency.\nData sources used and data availability and limitations are documented in detail in the assignment and could be accessed here."
  },
  {
    "objectID": "mp01.html#task-1-2---getting-data-ready-for-analysis.",
    "href": "mp01.html#task-1-2---getting-data-ready-for-analysis.",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 1 & 2 - Getting data ready for analysis.",
    "text": "Task 1 & 2 - Getting data ready for analysis.\nIn this part, I created the base table for data analysis using the code provided in the assignment. I also modified column names (Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (Task 2). Please use the\n\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\n\n# Let's start with Fare Revenue\n\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\n\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\n\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"UPT\"\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"VRM\"\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(\n    `NTD ID`, `Agency`, `UZA Name`,\n    `Mode`, `3 Mode`, Month\n  ) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n## Task 1 - Creating syntatic names\n\nnames(USAGE)[3] &lt;- \"Metro_Area\"\n\n## Task 2 - Recoding the Mode column\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nsample_n(USAGE, 100) |&gt;\n  mutate(Month = as.character(Month)) |&gt;\n  DT::datatable(\n    filter='top'\n  )"
  },
  {
    "objectID": "mp01.html#task-3---analyzing-transit-data.",
    "href": "mp01.html#task-3---analyzing-transit-data.",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 3 - Analyzing transit data.",
    "text": "Task 3 - Analyzing transit data.\n\nQ1. What transit agency had the most total VRM in this dataset?\nNot surprisingly, MTA New York City Transit has the largest total mileage in this dataset. Its total of 10.8B+ trips is almost double of the amount attributed to New Jersey Transit Corporation, the agency with the second largest result.\n\n# installing and loading additional libraries\n\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(TOTAL_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_VRM = comma(TOTAL_VRM, digits = 0)) |&gt;\n  slice_max(TOTAL_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Agencies by Total VRM\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Agencies by Total VRM\n\n\nAgency\nTOTAL_VRM\n\n\n\n\nMTA New York City Transit\n10,832,855,350\n\n\nNew Jersey Transit Corporation\n5,645,525,525\n\n\nLos Angeles County Metropolitan Transportation Authority\n4,354,016,659\n\n\n\n\n\n\n\n\n\nQ2. What transit mode had the most total VRM in this dataset?\nWith over 49 billion miles, bus has the most total VRM in this dataset.\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(TOTAL_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_VRM = comma(TOTAL_VRM, digits = 0)) |&gt;\n  slice_max(TOTAL_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Modes by Total VRM\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Modes by Total VRM\n\n\nMode\nTOTAL_VRM\n\n\n\n\nBus\n49,444,494,088\n\n\nDemand Response\n17,955,073,508\n\n\nHeavy Rail\n14,620,362,107\n\n\n\n\n\n\n\n\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nThere were slightly over 180 million trips recorded on NYC Subway in May 2024.\n\nUSAGE |&gt;\n  dplyr::filter(\n    Mode == \"Heavy Rail\",\n    Agency == \"MTA New York City Transit\",\n    Month == \"2024-05-01\"\n  ) |&gt;\n  summarize(TOTAL_TRIPS = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_TRIPS = comma(TOTAL_TRIPS, digits = 0)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Subway Trips in May'24\"\n  )\n\n\n\n\n\n\n\nNYC Subway Trips in May'24\n\n\nTOTAL_TRIPS\n\n\n\n\n180,458,819\n\n\n\n\n\n\n\n\n\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020?\nNYC subway ridership fell by 91% between April 2019 and April 2020 because of Covid pandemic.\n\nUSAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\" & Mode == \"Heavy Rail\") |&gt;\n  summarise(\n    APR19_TRIPS = sum(dplyr::case_when(Month == \"2019-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    APR20_TRIPS = sum(dplyr::case_when(Month == \"2020-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    PCT_CHANGE = (APR20_TRIPS - APR19_TRIPS) / APR19_TRIPS\n  ) |&gt;\n  mutate(\n    PCT_CHANGE = scales::percent(PCT_CHANGE),\n    APR19_TRIPS = comma(APR19_TRIPS, digits = 0),\n    APR20_TRIPS = comma(APR20_TRIPS, digits = 0)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Change in NYC Subway Ridership\"\n  )\n\n\n\n\n\n\n\nChange in NYC Subway Ridership\n\n\nAPR19_TRIPS\nAPR20_TRIPS\nPCT_CHANGE\n\n\n\n\n232,223,929\n20,254,269\n-91%"
  },
  {
    "objectID": "mp01.html#task-4---additional-findings.",
    "href": "mp01.html#task-4---additional-findings.",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 4 - Additional findings.",
    "text": "Task 4 - Additional findings.\n\nFor additional analysis, I took a deeper look at the NYC transit system data and discovered the following:\n\nBased on the share of UPT, subway has been, by far, the most popular mode of transportation. In any given year, its share of UPT is at least 2X of the second most used transit mode.\nMoreover, based on the changes in the share of UPT, NYC subway usage has been slowly increasing over the years. Between 2002 and 2023, it gained 9 percentage points in relative share, going from 68.1% of total to 76.6% of total, respectively.\nAt the same time, there has been a corresponding decrease in Bus trips. Between 2002 and 2023, it lost 10 percentage points in relative share, going from 31.9% of total to 21.6% of total, respectively.\nBus Rapid Transit and Commuter Bus are newer modes of transportation, with data on their usage not available until 2012.\n\n\n# create df with annual totals\n\nnyc_annual_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR) |&gt;\n  summarize(TOTAL_TRIPS_ALL = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# create df with annual totals by mode\n\nnyc_mode_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR, Mode) |&gt;\n  summarize(TOTAL_TRIPS = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# join 2 dfs and calculate shares by mode\n\nnyc_joined_df &lt;- nyc_mode_df |&gt;\n  left_join(nyc_annual_df, by = \"YEAR\")\n\nnyc_joined_df2 &lt;- nyc_joined_df |&gt;\n  mutate(SHARE = TOTAL_TRIPS / TOTAL_TRIPS_ALL) |&gt;\n  mutate(SHARE = scales::percent(SHARE, accuracy = 0.1)) |&gt;\n  select(-TOTAL_TRIPS, -TOTAL_TRIPS_ALL)\n\n# pivot wide\n\nnyc_mode_df_pivoted &lt;- pivot_wider(nyc_joined_df2,\n  id_cols = YEAR,\n  names_from = Mode,\n  values_from = SHARE\n)\n\nnyc_mode_df_pivoted |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Transit System - Relative Shares of UPT by Transportation Mode\"\n  )\n\n\n\n\n\n\n\nNYC Transit System - Relative Shares of UPT by Transportation Mode\n\n\nYEAR\nBus\nDemand Response\nHeavy Rail\nBus Rapid Transit\nCommuter Bus\n\n\n\n\n2002\n31.9%\n0.0%\n68.1%\nNA\nNA\n\n\n2003\n31.0%\n0.0%\n69.0%\nNA\nNA\n\n\n2004\n29.0%\n0.1%\n71.0%\nNA\nNA\n\n\n2005\n30.4%\n0.1%\n69.5%\nNA\nNA\n\n\n2006\n29.0%\n0.1%\n70.9%\nNA\nNA\n\n\n2007\n26.5%\n0.1%\n73.4%\nNA\nNA\n\n\n2008\n26.2%\n0.2%\n73.7%\nNA\nNA\n\n\n2009\n26.3%\n0.2%\n73.5%\nNA\nNA\n\n\n2010\n25.2%\n0.2%\n74.6%\nNA\nNA\n\n\n2011\n24.1%\n0.1%\n75.7%\nNA\nNA\n\n\n2012\n22.5%\n0.1%\n76.0%\n0.9%\n0.4%\n\n\n2013\n22.2%\n0.2%\n76.7%\n0.6%\n0.4%\n\n\n2014\n21.4%\n0.2%\n77.5%\n0.6%\n0.4%\n\n\n2015\n21.6%\n0.2%\n77.3%\n0.6%\n0.4%\n\n\n2016\n21.4%\n0.2%\n77.2%\n0.8%\n0.4%\n\n\n2017\n20.1%\n0.2%\n78.5%\n0.9%\n0.4%\n\n\n2018\n20.5%\n0.2%\n78.0%\n0.9%\n0.4%\n\n\n2019\n20.0%\n0.1%\n78.7%\n0.9%\n0.3%\n\n\n2020\n26.2%\n0.2%\n72.2%\n1.1%\n0.3%\n\n\n2021\n22.7%\n0.1%\n75.9%\n0.9%\n0.3%\n\n\n2022\n20.1%\n0.1%\n78.7%\n0.7%\n0.4%\n\n\n2023\n21.6%\n0.1%\n76.8%\n1.2%\n0.3%\n\n\n2024\n23.0%\n0.1%\n75.2%\n1.4%\n0.4%"
  },
  {
    "objectID": "mp01.html#task-5---summary-data-for-2022.",
    "href": "mp01.html#task-5---summary-data-for-2022.",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 5 - Summary data for 2022.",
    "text": "Task 5 - Summary data for 2022.\nIn this part, I created a summary usage table for 2022 and joined it with available financial data.\n\n### 2022 table\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  select(\n    `NTD ID`,\n    Agency,\n    Metro_Area,\n    Mode,\n    UPT,\n    VRM,\n    Month\n  ) |&gt;\n  filter(year(Month) == \"2022\") |&gt;\n  group_by(\n    `NTD ID`,\n    Agency,\n    Metro_Area,\n    Mode\n  ) |&gt;\n  summarise(\n    TOTAL_VRM = sum(VRM, na.rm = TRUE),\n    TOTAL_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\n# recode modes values\n\nFINANCIALS2 &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nUSAGE_AND_FINANCIALS &lt;- left_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS2,\n  join_by(`NTD ID`, Mode)\n) |&gt;\n  drop_na()\n\n\nsample_n(USAGE_AND_FINANCIALS, 1132) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-6---analysis-of-2022-data.",
    "href": "mp01.html#task-6---analysis-of-2022-data.",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 6 - Analysis of 2022 data.",
    "text": "Task 6 - Analysis of 2022 data.\n\nQ1. Which transit system (agency and mode) had the most UPT in 2022?\nNot surprisingly, NYC Subway had the largest volume of trips in 2022.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  summarise(UPT_TOTAL = sum(TOTAL_UPT)) |&gt;\n  mutate(UPT_TOTAL = comma(UPT_TOTAL, digits = 0)) |&gt;\n  slice_max(UPT_TOTAL, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nUPT_TOTAL\n\n\n\n\nMTA New York City Transit_Heavy Rail\n1,793,073,801\n\n\nMTA New York City Transit_Bus\n458,602,305\n\n\nLos Angeles County Metropolitan Transportation Authority_Bus\n193,637,448\n\n\n\n\n\n\n\n\n\nQ2.Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nIn 2022, ferryboat managed by Port Imperial Ferry Corp. had the highest farebox recovery ratio of 1.43.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  filter(sum(TOTAL_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(FAREBOX_RECOVERY = sum(`Total Fares`, na.rm = TRUE) / sum(Expenses, na.rm = TRUE)) |&gt;\n  mutate(FAREBOX_RECOVERY = comma(FAREBOX_RECOVERY, digits = 2)) |&gt;\n  ungroup() |&gt;\n  slice_max(FAREBOX_RECOVERY, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Farebox Recovery\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by Farebox Recovery\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFAREBOX_RECOVERY\n\n\n\n\nPort Imperial Ferry Corporation_Ferryboat\n1.43\n\n\nHyannis Harbor Tours, Inc._Ferryboat\n1.41\n\n\nTrans-Bridge Lines, Inc._Commuter Bus\n1.33\n\n\n\n\n\n\n\n\n\nQ3.Which transit system (agency and mode) has the lowest expenses per UPT?\nIn 2022, North Carolina State University Bus had the lowest expenses per UPT of $1.18 per unlinked passenger trip.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  filter(sum(TOTAL_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(EXPENSES_PER_UPT = sum(Expenses, na.rm = TRUE) / sum(TOTAL_UPT, na.rm = TRUE)) |&gt;\n  mutate(EXPENSES_PER_UPT = scales::dollar(EXPENSES_PER_UPT)) |&gt;\n  ungroup() |&gt;\n  slice_min(EXPENSES_PER_UPT, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Expenses per UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nEXPENSES_PER_UPT\n\n\n\n\nNorth Carolina State University_Bus\n$1.18\n\n\nAnaheim Transportation Network_Bus\n$1.28\n\n\nUniversity of Iowa_Bus\n$1.54\n\n\n\n\n\n\n\n\n\nQ4.Which transit system (agency and mode) has the highest total fares per UPT?\nIn 2022, ferryboat managed by Cape May Lewes Ferry garnered the highest total fares per UPT of $9.23 per unlinked passenger trip.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  filter(sum(TOTAL_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(FARES_PER_UPT = sum(`Total Fares`, na.rm = TRUE) / sum(TOTAL_UPT, na.rm = TRUE)) |&gt;\n  mutate(FARES_PER_UPT = scales::dollar(FARES_PER_UPT)) |&gt;\n  ungroup() |&gt;\n  slice_max(FARES_PER_UPT, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Fares per UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per UPT\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFARES_PER_UPT\n\n\n\n\nCape May Lewes Ferry_Ferryboat\n$9.23\n\n\nVirginia Railway Express_Commuter Rail\n$9.01\n\n\nPort Imperial Ferry Corporation_Ferryboat\n$8.90\n\n\n\n\n\n\n\n\n\nQ5.Which transit system (agency and mode) has the lowest expenses per VRM?\nIn 2022, Vanpool managed by Metropolitan Transportation Commission achieved the lowest expenses per VRM of $0.44 per vehicle revenue mile.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  filter(sum(TOTAL_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(EXPENSES_PER_VRM = sum(Expenses, na.rm = TRUE) / sum(TOTAL_VRM, na.rm = TRUE)) |&gt;\n  mutate(EXPENSES_PER_VRM = scales::dollar(EXPENSES_PER_VRM)) |&gt;\n  ungroup() |&gt;\n  slice_min(EXPENSES_PER_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Expenses per VRM\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per VRM\n\n\n2022\n\n\nTRANSIT_SYSTEM\nEXPENSES_PER_VRM\n\n\n\n\nMetropolitan Transportation Commission_Vanpool\n$0.44\n\n\nSan Joaquin Council_Vanpool\n$0.50\n\n\nSan Diego Association of Governments_Vanpool\n$0.54\n\n\n\n\n\n\n\n\n\nQ6.Which transit system (agency and mode) has the highest total fares per VRM?\nIn 2022, ferryboat managed by Cape May Lewes Ferry achieved higher fares per VRM than any other large transit system with UPT of at least 400,000.\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(TRANSIT_SYSTEM, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(TRANSIT_SYSTEM) |&gt;\n  filter(sum(TOTAL_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(FARES_PER_VRM = sum(`Total Fares`, na.rm = TRUE) / sum(TOTAL_VRM, na.rm = TRUE)) |&gt;\n  mutate(FARES_PER_VRM = scales::dollar(FARES_PER_VRM)) |&gt;\n  ungroup() |&gt;\n  slice_max(FARES_PER_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Fares per VRM\",\n    subtitle = \"2022\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per VRM\n\n\n2022\n\n\nTRANSIT_SYSTEM\nFARES_PER_VRM\n\n\n\n\nCape May Lewes Ferry_Ferryboat\n$93.01\n\n\nWoods Hole, Martha's Vineyard and Nantucket Steamship Authority_Ferryboat\n$91.68\n\n\nAnaheim Transportation Network_Bus\n$9.42\n\n\n\n\n\n\n\n\n\nBased on the farebox recovery ratio, Ferryboat managed by Port Imperial Ferry Corporation appeared to be the most efficient transit system in 2022 among large transit systems with total annual UPT of 400,000+."
  },
  {
    "objectID": "mp01.html#data-preparation",
    "href": "mp01.html#data-preparation",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Data Preparation",
    "text": "Data Preparation\nIn this part, I created the base table for data analysis using the code provided in the assignment. I also modified column names (per Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (per Task 2). The base table is provided for review after the code block (please note NTD ID and 3 Modes columns are excluded from the preview).\n\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\n\n# Let's start with Fare Revenue\n\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\n\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\n\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"UPT\"\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"VRM\"\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(\n    `NTD ID`, `Agency`, `UZA Name`,\n    `Mode`, `3 Mode`, Month\n  ) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n## Task 1 - Creating syntatic names\n\nnames(USAGE)[3] &lt;- \"Metro_Area\"\n\n## Task 2 - Recoding the Mode column\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# base table sampled\n\nsample_n(USAGE, 1000) |&gt;\n  select(-`NTD ID`, -`3 Mode`) |&gt;\n  mutate(Month = as.character(Month)) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp01.html#initial-analysys-of-transit-data",
    "href": "mp01.html#initial-analysys-of-transit-data",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Initial analysys of transit data",
    "text": "Initial analysys of transit data\nIn this part, I conducted the initial analysis of transit data by addressing 5 questions provided in Task 3.\n\nQ1. What transit agency had the most total VRM in this dataset?\nNot surprisingly, MTA New York City Transit has the largest total mileage in this dataset. Its total of 10.8B+ trips is almost double of the amount attributed to New Jersey Transit Corporation, the agency with the second largest result.\n\n\n\n\n\n\n\n\nTop 3 Transit Agencies by Total VRM\n\n\nAgency\nTOTAL_VRM\n\n\n\n\nMTA New York City Transit\n10,832,855,350\n\n\nNew Jersey Transit Corporation\n5,645,525,525\n\n\nLos Angeles County Metropolitan Transportation Authority\n4,354,016,659\n\n\n\n\n\n\n\n\n# installing and loading additional libraries\n\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(TOTAL_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_VRM = comma(TOTAL_VRM, digits = 0)) |&gt;\n  slice_max(TOTAL_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Agencies by Total VRM\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Agencies by Total VRM\n\n\nAgency\nTOTAL_VRM\n\n\n\n\nMTA New York City Transit\n10,832,855,350\n\n\nNew Jersey Transit Corporation\n5,645,525,525\n\n\nLos Angeles County Metropolitan Transportation Authority\n4,354,016,659\n\n\n\n\n\n\n\n\n\nQ2. What transit mode had the most total VRM in this dataset?\nWith over 49 billion miles, bus has the most total VRM in this dataset.\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(TOTAL_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_VRM = comma(TOTAL_VRM, digits = 0)) |&gt;\n  slice_max(TOTAL_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Modes by Total VRM\"\n  )\n\n\n\n\n\n\n\nTop 3 Transit Modes by Total VRM\n\n\nMode\nTOTAL_VRM\n\n\n\n\nBus\n49,444,494,088\n\n\nDemand Response\n17,955,073,508\n\n\nHeavy Rail\n14,620,362,107\n\n\n\n\n\n\n\n\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nThere were slightly over 180 million trips recorded on NYC Subway in May 2024.\n\nUSAGE |&gt;\n  dplyr::filter(\n    Mode == \"Heavy Rail\",\n    Agency == \"MTA New York City Transit\",\n    Month == \"2024-05-01\"\n  ) |&gt;\n  summarize(TOTAL_TRIPS = sum(UPT, na.rm = TRUE)) |&gt;\n  mutate(TOTAL_TRIPS = comma(TOTAL_TRIPS, digits = 0)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Subway Trips in May'24\"\n  )\n\n\n\n\n\n\n\nNYC Subway Trips in May'24\n\n\nTOTAL_TRIPS\n\n\n\n\n180,458,819\n\n\n\n\n\n\n\n\n\nQ5. How much did NYC subway ridership fall between April 2019 and April 2020?\nNYC subway ridership fell by 91% between April 2019 and April 2020 because of Covid pandemic.\n\nUSAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\" & Mode == \"Heavy Rail\") |&gt;\n  summarise(\n    APR19_TRIPS = sum(dplyr::case_when(Month == \"2019-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    APR20_TRIPS = sum(dplyr::case_when(Month == \"2020-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    PCT_CHANGE = (APR20_TRIPS - APR19_TRIPS) / APR19_TRIPS\n  ) |&gt;\n  mutate(\n    PCT_CHANGE = scales::percent(PCT_CHANGE),\n    APR19_TRIPS = comma(APR19_TRIPS, digits = 0),\n    APR20_TRIPS = comma(APR20_TRIPS, digits = 0)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Change in NYC Subway Ridership\"\n  )\n\n\n\n\n\n\n\nChange in NYC Subway Ridership\n\n\nAPR19_TRIPS\nAPR20_TRIPS\nPCT_CHANGE\n\n\n\n\n232,223,929\n20,254,269\n-91%"
  },
  {
    "objectID": "mp01.html#initial-analysis-of-transit-data",
    "href": "mp01.html#initial-analysis-of-transit-data",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Initial analysis of transit data",
    "text": "Initial analysis of transit data\nIn this part, I conducted the initial analysis of transit data by addressing the questions provided in Task 3.\n\nQ1. What transit agency had the most total VRM in this dataset?\nNot surprisingly, MTA New York City Transit has the largest total mileage in this dataset. Its total of 10.8B+ vehicle revenue miles is almost double of the amount attributed to New Jersey Transit Corporation, the agency with the second largest result.\n\n\n\n\n\n\n\n\nTop 3 Transit Agencies by Total VRM\n\n\nAgency\nTotal_VRM\n\n\n\n\nMTA New York City Transit\n10,832,855,350\n\n\nNew Jersey Transit Corporation\n5,645,525,525\n\n\nLos Angeles County Metropolitan Transportation Authority\n4,354,016,659\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\n# installing and loading additional libraries\n\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  mutate(Total_VRM = comma(Total_VRM, digits = 0)) |&gt;\n  slice_max(Total_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Agencies by Total VRM\"\n  )\n\n\n\nQ2. What transit mode had the most total VRM in this dataset?\nWith over 49 billion vehicle revenue miles, bus has the most total VRM of all transit modes.\n\n\n\n\n\n\n\n\nTop 3 Transit Modes by Total VRM\n\n\nMode\nTotal_VRM\n\n\n\n\nBus\n49,444,494,088\n\n\nDemand Response\n17,955,073,508\n\n\nHeavy Rail\n14,620,362,107\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(Total_VRM = sum(VRM, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  mutate(Total_VRM = comma(Total_VRM, digits = 0)) |&gt;\n  slice_max(Total_VRM, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Modes by Total VRM\"\n  )\n\n\n\nQ3. How many trips were taken on the NYC Subway (Heavy Rail) in May 2024?\nThere were slightly over 180 million trips recorded on NYC Subway in May 2024.\n\n\n\n\n\n\n\n\nNYC Subway Trips in May'24\n\n\nTotal_UPT\n\n\n\n\n180,458,819\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE |&gt;\n  dplyr::filter(\n    Mode == \"Heavy Rail\",\n    Agency == \"MTA New York City Transit\",\n    Month == \"2024-05-01\"\n  ) |&gt;\n  summarize(Total_UPT = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  mutate(Total_UPT = comma(Total_UPT, digits = 0)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Subway Trips in May'24\"\n  )\n\n\n\nQ5. How much did NYC subway ridership fall between April’19 and April’20?\nNYC subway ridership fell by 91% between April 2019 and April 2020 because of the Covid’19 pandemic.\n\n\n\n\n\n\n\n\nChange in NYC Subway Ridership\n\n\nApril19_Trips\nApril20_Trips\nPct_Change\n\n\n\n\n232,223,929\n20,254,269\n-91%\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE |&gt;\n  filter(Agency == \"MTA New York City Transit\" & Mode == \"Heavy Rail\") |&gt;\n  summarise(\n    April19_Trips = sum(dplyr::case_when(Month == \"2019-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    April20_Trips = sum(dplyr::case_when(Month == \"2020-04-01\" ~ UPT, TRUE ~ 0), na.rm = TRUE),\n    Pct_Change = (April20_Trips - April19_Trips) / April19_Trips\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    Pct_Change = scales::percent(Pct_Change),\n    April19_Trips = comma(April19_Trips, digits = 0),\n    April20_Trips = comma(April20_Trips, digits = 0)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Change in NYC Subway Ridership\"\n  )"
  },
  {
    "objectID": "mp01.html#task-4---additional-analysis-of-transit-data",
    "href": "mp01.html#task-4---additional-analysis-of-transit-data",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Task 4 - Additional analysis of transit data",
    "text": "Task 4 - Additional analysis of transit data\nIn this part, I took a deeper look at the NYC transit system data and discovered the following:\n1) Based on the share of UPT, subway has been, by far, the most popular mode of transportation. In any given year, its relative share of UPT is at least 2X of the second most used transit mode.\n2) Moreover, based on the changes in the share of UPT, NYC subway usage has been slowly increasing over the years. Between 2002 and 2023, it gained 9 percentage points in relative share, going from 68.1% of total to 76.6% of total, respectively.\n3) At the same time, there has been a corresponding decrease in Bus trips. Between 2002 and 2023, it lost 10 percentage points in relative share, going from 31.9% of total to 21.6% of total, respectively.\n4) Bus Rapid Transit and Commuter Bus are newer modes of transportation, with data on their usage not available until 2012.\n\n\n\n\n\n\n\n\nNYC Transit System - Relative Shares of UPT by Transportation Mode\n\n\nYEAR\nBus\nDemand Response\nHeavy Rail\nBus Rapid Transit\nCommuter Bus\n\n\n\n\n2002\n31.9%\n0.0%\n68.1%\nNA\nNA\n\n\n2003\n31.0%\n0.0%\n69.0%\nNA\nNA\n\n\n2004\n29.0%\n0.1%\n71.0%\nNA\nNA\n\n\n2005\n30.4%\n0.1%\n69.5%\nNA\nNA\n\n\n2006\n29.0%\n0.1%\n70.9%\nNA\nNA\n\n\n2007\n26.5%\n0.1%\n73.4%\nNA\nNA\n\n\n2008\n26.2%\n0.2%\n73.7%\nNA\nNA\n\n\n2009\n26.3%\n0.2%\n73.5%\nNA\nNA\n\n\n2010\n25.2%\n0.2%\n74.6%\nNA\nNA\n\n\n2011\n24.1%\n0.1%\n75.7%\nNA\nNA\n\n\n2012\n22.5%\n0.1%\n76.0%\n0.9%\n0.4%\n\n\n2013\n22.2%\n0.2%\n76.7%\n0.6%\n0.4%\n\n\n2014\n21.4%\n0.2%\n77.5%\n0.6%\n0.4%\n\n\n2015\n21.6%\n0.2%\n77.3%\n0.6%\n0.4%\n\n\n2016\n21.4%\n0.2%\n77.2%\n0.8%\n0.4%\n\n\n2017\n20.1%\n0.2%\n78.5%\n0.9%\n0.4%\n\n\n2018\n20.5%\n0.2%\n78.0%\n0.9%\n0.4%\n\n\n2019\n20.0%\n0.1%\n78.7%\n0.9%\n0.3%\n\n\n2020\n26.2%\n0.2%\n72.2%\n1.1%\n0.3%\n\n\n2021\n22.7%\n0.1%\n75.9%\n0.9%\n0.3%\n\n\n2022\n20.1%\n0.1%\n78.7%\n0.7%\n0.4%\n\n\n2023\n21.6%\n0.1%\n76.8%\n1.2%\n0.3%\n\n\n2024\n23.0%\n0.1%\n75.2%\n1.4%\n0.4%\n\n\n\n\n\n\n\nPlease see below for the code used to generate aforementioned results:\n\n# create df with annual totals\n\nnyc_annual_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR) |&gt;\n  summarize(TOTAL_TRIPS_ALL = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# create df with annual totals by mode\n\nnyc_mode_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR, Mode) |&gt;\n  summarize(TOTAL_TRIPS = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# join 2 dfs and calculate shares by mode\n\nnyc_joined_df &lt;- nyc_mode_df |&gt;\n  left_join(nyc_annual_df, by = \"YEAR\")\n\nnyc_joined_df2 &lt;- nyc_joined_df |&gt;\n  mutate(SHARE = TOTAL_TRIPS / TOTAL_TRIPS_ALL) |&gt;\n  mutate(SHARE = scales::percent(SHARE, accuracy = 0.1)) |&gt;\n  select(-TOTAL_TRIPS, -TOTAL_TRIPS_ALL)\n\n# pivot wide\n\nnyc_mode_df_pivoted &lt;- pivot_wider(nyc_joined_df2,\n  id_cols = YEAR,\n  names_from = Mode,\n  values_from = SHARE\n)\n\nnyc_mode_df_pivoted |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Transit System - Relative Shares of UPT by Transportation Mode\"\n  )"
  },
  {
    "objectID": "mp01.html#financial-data-prepation",
    "href": "mp01.html#financial-data-prepation",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Financial data prepation",
    "text": "Financial data prepation\nIn this part, I created a summary usage table for 2022 and joined it with available financial data (per Task 5). The resulting dataset is provided for review after the code block.\n\n# code used to generate financial and usage summary table for 2022\n\n# create 2022 usage data\n\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  select(\n    `NTD ID`,\n    Agency,\n    Metro_Area,\n    Mode,\n    UPT,\n    VRM,\n    Month\n  ) |&gt;\n  filter(year(Month) == \"2022\") |&gt;\n  group_by(\n    `NTD ID`,\n    Agency,\n    Metro_Area,\n    Mode\n  ) |&gt;\n  summarise(\n    Total_VRM = sum(VRM, na.rm = TRUE),\n    Total_UPT = sum(UPT, na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\n# recode modes values in financial data\n\nFINANCIALS2 &lt;- FINANCIALS |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# join financial and usage data\n\nUSAGE_AND_FINANCIALS &lt;- left_join(\n  USAGE_2022_ANNUAL,\n  FINANCIALS2,\n  join_by(`NTD ID`, Mode)\n) |&gt;\n  drop_na()\n\n# review the data\n\nsample_n(USAGE_AND_FINANCIALS, 1132) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp01.html#analysis-of-2022-financial-and-usage-data",
    "href": "mp01.html#analysis-of-2022-financial-and-usage-data",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Analysis of 2022 financial and usage data",
    "text": "Analysis of 2022 financial and usage data\nIn this part, I addressed the questions outlined in Task 6. Please note that this analysis will be limited to large transit systems only (large transit systems are defined as systems with at least 400,000 total annual UPT).\n\nQ1. Which transit system (agency and mode) had the most UPT in 2022?\nNot surprisingly, NYC Subway had the largest volume of trips in 2022.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by UPT\n\n\n2022\n\n\nTransit_System\nUPT\n\n\n\n\nMTA New York City Transit_Heavy Rail\n1,793,073,801\n\n\nMTA New York City Transit_Bus\n458,602,305\n\n\nLos Angeles County Metropolitan Transportation Authority_Bus\n193,637,448\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(UPT = sum(Total_UPT)) |&gt;\n  ungroup() |&gt;\n  mutate(UPT = comma(UPT, digits = 0)) |&gt;\n  slice_max(UPT, n = 3) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\nQ2.Which transit system (agency and mode) had the highest farebox recovery, defined as the highest ratio of Total Fares to Expenses?\nIn 2022, ferryboat managed by Port Imperial Ferry Corp. had the highest farebox recovery ratio of 1.43.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Farebox Recovery\n\n\n2022\n\n\nTransit_System\nFarebox_Recovery\n\n\n\n\nPort Imperial Ferry Corporation_Ferryboat\n142.8%\n\n\nHyannis Harbor Tours, Inc._Ferryboat\n141.3%\n\n\nTrans-Bridge Lines, Inc._Commuter Bus\n133.3%\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(Farebox_Recovery = sum(`Total Fares`, na.rm = TRUE) / sum(Expenses, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_max(Farebox_Recovery, n = 3) |&gt;\n  mutate(Farebox_Recovery = scales::percent(Farebox_Recovery, accuracy = 0.1)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Farebox Recovery\",\n    subtitle = \"2022\"\n  )\n\n\n\nQ3.Which transit system (agency and mode) has the lowest expenses per UPT?\nIn 2022, North Carolina State University Bus had the lowest expenses per UPT of $1.18 per unlinked passenger trip.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per UPT\n\n\n2022\n\n\nTransit_System\nExpenses_per_UPT\n\n\n\n\nNorth Carolina State University_Bus\n$1.18\n\n\nAnaheim Transportation Network_Bus\n$1.28\n\n\nUniversity of Iowa_Bus\n$1.54\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(Expenses_Per_UPT = sum(Expenses, na.rm = TRUE) / sum(Total_UPT, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_min(Expenses_Per_UPT, n = 3) |&gt;\n  mutate(Expenses_Per_UPT = scales::dollar(Expenses_Per_UPT)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Expenses per UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\nQ4.Which transit system (agency and mode) has the highest total fares per UPT?\nIn 2022, commuter bus managed by Hampton Jitney Inc. garnered the highest total fares per UPT of $41.30 per unlinked passenger trip.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per UPT\n\n\n2022\n\n\nTransit_System\nFares_per_UPT\n\n\n\n\nHampton Jitney, Inc._Commuter Bus\n$41.30\n\n\nPennsylvania Department of Transportation_Commuter Rail\n$32.26\n\n\nHyannis Harbor Tours, Inc._Ferryboat\n$29.56\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(Fares_per_UPT = sum(`Total Fares`, na.rm = TRUE) / sum(Total_UPT, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_max(Fares_per_UPT, n = 3) |&gt;\n  mutate(Fares_per_UPT = scales::dollar(Fares_per_UPT)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Fares per UPT\",\n    subtitle = \"2022\"\n  )\n\n\n\nQ5.Which transit system (agency and mode) has the lowest expenses per VRM?\nIn 2022, Vanpool managed by Metropolitan Transportation Commission achieved the lowest expenses per VRM of $0.44 per vehicle revenue mile.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Expenses per VRM\n\n\n2022\n\n\nTransit_System\nExpenses_per_VRM\n\n\n\n\nMetropolitan Transportation Commission_Vanpool\n$0.44\n\n\nSan Joaquin Council_Vanpool\n$0.50\n\n\nSan Diego Association of Governments_Vanpool\n$0.54\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(Expenses_per_VRM = sum(Expenses, na.rm = TRUE) / sum(Total_VRM, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_min(Expenses_per_VRM, n = 3) |&gt;\n  mutate(Expenses_per_VRM = scales::dollar(Expenses_per_VRM)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Expenses per VRM\",\n    subtitle = \"2022\"\n  )\n\n\n\nQ6.Which transit system (agency and mode) has the highest total fares per VRM?\nIn 2022, ferryboat managed by Jacksonville Transporation Authority achieved higher fares per VRM than any other large transit system with UPT of at least 400,000.\n\n\n\n\n\n\n\n\nTop 3 Transit Systems by Fares per VRM\n\n\n2022\n\n\nTransit_System\nFares_per_VRM\n\n\n\n\nJacksonville Transportation Authority_Ferryboat\n$157.70\n\n\nChattanooga Area Regional Transportation Authority_Inclined Plane\n$149.30\n\n\nHyannis Harbor Tours, Inc._Ferryboat\n$137.64\n\n\n\n\n\n\n\nPlease see below for the code used to generate the aforementioned results:\n\nUSAGE_AND_FINANCIALS |&gt;\n  unite(Transit_System, c(\"Agency\", \"Mode\")) |&gt;\n  group_by(Transit_System) |&gt;\n  filter(sum(Total_UPT, na.rm = TRUE) &gt;= 400000) |&gt;\n  summarise(Fares_per_VRM = sum(`Total Fares`, na.rm = TRUE) / sum(Total_VRM, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  slice_max(Fares_per_VRM, n = 3) |&gt;\n  mutate(Fares_per_VRM = scales::dollar(Fares_per_VRM)) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 3 Transit Systems by Fares per VRM\",\n    subtitle = \"2022\"\n  )\n\n\n\nConclusion\nThe farebox recovery ratio is one of the key metrics used to evaluate financial performance of transit systems (Source). With the farebox recovery ratio of 142.8%, the ferryboat managed by Port Imperial Ferry Corporation appeared to be the most efficient large transit system in 2022."
  },
  {
    "objectID": "mp01.html#additional-analysis-of-transit-data",
    "href": "mp01.html#additional-analysis-of-transit-data",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Additional Analysis of Transit Data",
    "text": "Additional Analysis of Transit Data\nIn this part, I conducted additional analysis of transit usage data as instructed in Task 4. Focusing on MTA New York City Transit, I analyzed the relative popularity of different transporation options over the years and discovered the following:\n1) Based on the share of UPT, subway has been, by far, the most popular mode of transportation. In any given year, its relative share of UPT is at least 2X of that of the second most used transit mode, Bus.\n2) Moreover, based on the changes in the share of UPT, NYC subway usage has been slowly increasing over the years. Between 2002 and 2023, it gained 9 percentage points in relative share, going from 68.1% of total to 76.8% of total, respectively.\n3) At the same time, there has been a corresponding decrease in Bus trips. Between 2002 and 2023, it lost 10 percentage points in relative share, going from 31.9% of total to 21.6% of total, respectively.\n4) Bus Rapid Transit and Commuter Bus are newer modes of transportation, with data on their usage not available until 2012.\n\n\n\n\n\n\n\n\nNYC Transit System - Relative Shares of UPT by Transportation Mode\n\n\nYEAR\nBus\nDemand Response\nHeavy Rail\nBus Rapid Transit\nCommuter Bus\n\n\n\n\n2002\n31.9%\n0.0%\n68.1%\nNA\nNA\n\n\n2003\n31.0%\n0.0%\n69.0%\nNA\nNA\n\n\n2004\n29.0%\n0.1%\n71.0%\nNA\nNA\n\n\n2005\n30.4%\n0.1%\n69.5%\nNA\nNA\n\n\n2006\n29.0%\n0.1%\n70.9%\nNA\nNA\n\n\n2007\n26.5%\n0.1%\n73.4%\nNA\nNA\n\n\n2008\n26.2%\n0.2%\n73.7%\nNA\nNA\n\n\n2009\n26.3%\n0.2%\n73.5%\nNA\nNA\n\n\n2010\n25.2%\n0.2%\n74.6%\nNA\nNA\n\n\n2011\n24.1%\n0.1%\n75.7%\nNA\nNA\n\n\n2012\n22.5%\n0.1%\n76.0%\n0.9%\n0.4%\n\n\n2013\n22.2%\n0.2%\n76.7%\n0.6%\n0.4%\n\n\n2014\n21.4%\n0.2%\n77.5%\n0.6%\n0.4%\n\n\n2015\n21.6%\n0.2%\n77.3%\n0.6%\n0.4%\n\n\n2016\n21.4%\n0.2%\n77.2%\n0.8%\n0.4%\n\n\n2017\n20.1%\n0.2%\n78.5%\n0.9%\n0.4%\n\n\n2018\n20.5%\n0.2%\n78.0%\n0.9%\n0.4%\n\n\n2019\n20.0%\n0.1%\n78.7%\n0.9%\n0.3%\n\n\n2020\n26.2%\n0.2%\n72.2%\n1.1%\n0.3%\n\n\n2021\n22.7%\n0.1%\n75.9%\n0.9%\n0.3%\n\n\n2022\n20.1%\n0.1%\n78.7%\n0.7%\n0.4%\n\n\n2023\n21.6%\n0.1%\n76.8%\n1.2%\n0.3%\n\n\n2024\n23.0%\n0.1%\n75.2%\n1.4%\n0.4%\n\n\n\n\n\n\n\n(Relative share of UPT is calculated as UPT of a given mode over total annual UPT)\nPlease see below for the code used to generate aforementioned results:\n\n# create df with annual totals\n\nnyc_annual_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR) |&gt;\n  summarize(TOTAL_TRIPS_ALL = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# create df with annual totals by mode\n\nnyc_mode_df &lt;- USAGE |&gt;\n  mutate(YEAR = format(as.Date(Month), \"%Y\")) |&gt;\n  dplyr::filter(Agency == \"MTA New York City Transit\") |&gt;\n  group_by(YEAR, Mode) |&gt;\n  summarize(TOTAL_TRIPS = sum(UPT, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# join 2 dfs and calculate shares by mode\n\nnyc_joined_df &lt;- nyc_mode_df |&gt;\n  left_join(nyc_annual_df, by = \"YEAR\")\n\nnyc_joined_df2 &lt;- nyc_joined_df |&gt;\n  mutate(SHARE = TOTAL_TRIPS / TOTAL_TRIPS_ALL) |&gt;\n  mutate(SHARE = scales::percent(SHARE, accuracy = 0.1)) |&gt;\n  select(-TOTAL_TRIPS, -TOTAL_TRIPS_ALL)\n\n# pivot wide\n\nnyc_mode_df_pivoted &lt;- pivot_wider(nyc_joined_df2,\n  id_cols = YEAR,\n  names_from = Mode,\n  values_from = SHARE\n)\n\nnyc_mode_df_pivoted |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"NYC Transit System - Relative Shares of UPT by Transportation Mode\"\n  )"
  },
  {
    "objectID": "mp01.html#initial-data-preparation",
    "href": "mp01.html#initial-data-preparation",
    "title": "Mini-Project 01: Fiscal Characteristics of Major US Public Transit Systems",
    "section": "Initial data preparation",
    "text": "Initial data preparation\nIn this part, I created the base table for data analysis using the code provided in the assignment. I also modified column naming convention (as instructed in Task 1) and recoded the values in the ‘Mode’ column to make them easier to understand and use for analysis (as instructed in Task 2). The base table is provided for review after the code block (please note NTD ID and 3 Modes columns are excluded from the preview).\n\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\n\n# Let's start with Fare Revenue\n\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(\n    -`State/Parent NTD ID`,\n    -`Reporter Type`,\n    -`Reporting Module`,\n    -`TOS`,\n    -`Passenger Paid Fares`,\n    -`Organization Paid Fares`\n  ) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`) |&gt;\n  group_by(\n    `NTD ID`, # Sum over different `TOS` for the same `Mode`\n    `Agency Name`, # These are direct operated and sub-contracted\n    `Mode`\n  ) |&gt; # of the same transit modality\n  # Not a big effect in most munis (significant DO\n  # tends to get rid of sub-contractors), but we'll sum\n  # to unify different passenger experiences\n  summarize(`Total Fares` = sum(`Total Fares`)) |&gt;\n  ungroup()\n\n# Next, expenses\n\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(\n    `NTD ID`,\n    `Agency`,\n    `Total`,\n    `Mode`\n  ) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`))\n\n# Monthly Transit Numbers\n\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"UPT\"\n  ) |&gt;\n  drop_na() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet = \"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(\n    -`Legacy NTD ID`,\n    -`Reporter Type`,\n    -`Mode/Type of Service Status`,\n    -`UACE CD`,\n    -`TOS`\n  ) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`),\n    names_to = \"Month\",\n    values_to = \"VRM\"\n  ) |&gt;\n  drop_na() |&gt;\n  group_by(\n    `NTD ID`, `Agency`, `UZA Name`,\n    `Mode`, `3 Mode`, Month\n  ) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(Month = my(Month)) # Parse _m_onth _y_ear date specs\n\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))\n\n## Task 1 - Creating syntatic names\n\nnames(USAGE)[3] &lt;- \"Metro_Area\"\n\n## Task 2 - Recoding the Mode column\n\nUSAGE &lt;- USAGE |&gt;\n  mutate(Mode = case_when(\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    Mode == \"MG\" ~ \"Monorail Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# base table sampled\n\nsample_n(USAGE, 1000) |&gt;\n  select(-`NTD ID`, -`3 Mode`) |&gt;\n  mutate(Month = as.character(Month)) |&gt;\n  DT::datatable()"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project 02: Business of Show Business",
    "section": "",
    "text": "In this paper, we will analyze the IMDB data to answer some questions, design a measurement framework for evaluating performance and identify an opportunity for investment.\n\nData Prep\nIn this section, we obtain and prepare data for analysis. Because of the memory and performance issues, we will use small files and further down-select data to enable a more fluid analysis. We will drop titles with fewer than 100 ratings and individuals who worked on only 1 title.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n\nReading data in\n\n\nShow the code\n## read files in and create dataframes\n\n\nname_basics &lt;- read.csv((\"name_basics_small.csv\"))\ntitle_basics &lt;- read.csv(\"title_basics_small.csv\")\ntitle_episodes &lt;- read.csv(\"title_episodes_small.csv\")\ntitle_ratings &lt;- read.csv(\"title_ratings_small.csv\")\ntitle_crew &lt;- read.csv(\"title_crew_small.csv\")\ntitle_principals &lt;- read.csv(\"title_principals_small.csv\")\n\n# drop records with fewer than 2 titles from name_basics df\n\nname_basics &lt;- name_basics |&gt;\n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\n# drop records with fewer than 100 ratings from title_ratings df\n\ntitle_ratings &lt;- title_ratings |&gt;\n  filter(numVotes &gt;= 100)\n\n\nFurthermore, to ensure consistency across all data sets, we will apply the same filtering, i.e., excluding titles with fewer than 100 ratings, to the rest of the title tables:\n\n\nShow the code\n# filtering title basics df\n\ntitle_basics &lt;- title_basics |&gt;\n  semi_join(\n    title_ratings,\n    join_by(tconst == tconst)\n  )\n\n# filtering title crew df\n\ntitle_crew &lt;- title_crew |&gt;\n  semi_join(\n    title_ratings,\n    join_by(tconst == tconst)\n  )\n\n# filtering title episodes df on title id\n\ntitle_episodes_1 &lt;- title_episodes |&gt;\n  semi_join(\n    title_ratings,\n    join_by(tconst == tconst)\n  )\n\n# filtering title episodes df on parent title id\n\ntitle_episodes_2 &lt;- title_episodes |&gt;\n  semi_join(\n    title_ratings,\n    join_by(parentTconst == tconst)\n  )\n\n# combining filtered title episodes dfs\n\ntitle_episodes &lt;- bind_rows(\n  title_episodes_1,\n  title_episodes_2\n) |&gt;\n  distinct()\n\n## filtering title principals df\n\ntitle_principals &lt;- title_principals |&gt;\n  semi_join(\n    title_ratings,\n    join_by(tconst == tconst)\n  )\n\n# remove dfs we no longer need\n\nrm(title_episodes_1)\nrm(title_episodes_2)\n\n\n\n\nTask 1\n\nCorrect the column types of the title tables using a combination of mutate and the coercion functions as.numeric and as.logical.\n\nTitle Basics\n\nglimpse(title_basics)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;chr&gt; \"1894\", \"1892\", \"1892\", \"1892\", \"1893\", \"1894\", \"1894\",…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nColumns startYear,endYear and runtimeMinutes are formatted as character/string in the original data set and need to be changed to be numeric.\n\n## recode column types and rename columns\n\ntitle_basics &lt;- title_basics |&gt;\n  mutate(\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes),\n    isAdult = as.logical(isAdult)\n  ) |&gt;\n  rename(\n    start_year = startYear,\n    end_year = endYear,\n    runtime_minutes = runtimeMinutes\n  )\n\nglimpse(title_basics)\n\nRows: 372,198\nColumns: 9\n$ tconst          &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"t…\n$ titleType       &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", …\n$ primaryTitle    &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierro…\n$ originalTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierro…\n$ isAdult         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ start_year      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, …\n$ end_year        &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ runtime_minutes &lt;dbl&gt; 1, 5, 5, 12, 1, 1, 1, 1, 45, 1, 1, 1, 1, 1, 2, 1, 1, 1…\n$ genres          &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Com…\n\n\nTitle Crew\n\nglimpse(title_crew)\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nThere is no need to correct any data types here.\nTitle Episodes\n\nglimpse(title_episodes)\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nseasonNumber and episodeNumber columns need to be converted to numeric format.\n\n## recode column types and rename columns\n\ntitle_episodes &lt;- title_episodes |&gt;\n  mutate(\n    seasonNumber = as.numeric(seasonNumber),\n    episodeNumber = as.numeric(episodeNumber)\n  ) |&gt;\n  rename(\n    season_number = seasonNumber,\n    episode_number = episodeNumber\n  )\n\nglimpse(title_episodes)\n\nRows: 3,007,178\nColumns: 4\n$ tconst         &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt…\n$ parentTconst   &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt…\n$ season_number  &lt;dbl&gt; 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 8, 1, 10, 6, 2, 8, …\n$ episode_number &lt;dbl&gt; 3, 4, 6, 10, 4, 20, 5, 2, 20, 6, 2, 3, 2, 10, 17, 5, 1,…\n\n\nTitle Principals\n\nglimpse(title_principals)\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;int&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…\n\n\nThere is no need to correct data types here.\nTitle Ratings\n\nglimpse(title_ratings)\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;int&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\nThere is no need to correct data types here.\nName Basics\n\nglimpse(name_basics)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nbirthYear and deathYear columns need to be formatted as numeric.\n\n## recode column types and rename columns\n\nname_basics &lt;- name_basics |&gt;\n  mutate(\n    birthYear = as.numeric(birthYear),\n    deathYear = as.numeric(deathYear)\n  ) |&gt;\n  rename(\n    birth_year = birthYear,\n    death_year = deathYear\n  )\n\nglimpse(name_basics)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birth_year        &lt;dbl&gt; 1899, 1924, 1934, 1949, 1918, 1915, 1899, 1924, 1925…\n$ death_year        &lt;dbl&gt; 1987, 2014, NA, 1982, 2007, 1982, 1957, 2004, 1984, …\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\n\n\nTask 2 - Instructor-Provided Questions\n\nQ1. How many movies are in our data set? How many TV series? How many TV episodes?\nTo answer this question, we will use the title basics data set, which contains release and production information.\nContent type is captured in the titleType column. We have 131,662 movies, 29,789 TV Series and 155,722 TV episodes.\n\n# get a count of records by content types\n\ndf1 &lt;- title_basics |&gt;\n  group_by(titleType) |&gt;\n  summarize(number_of_records = n()) |&gt;\n  ungroup() |&gt;\n  mutate(number_of_records = comma(number_of_records, digits = 0)) |&gt;\n  rename(title_type = titleType) |&gt;\n  arrange(desc(number_of_records))\n\n# plot the resulting df\n\nfig_content_count_type &lt;- plot_ly(\n  data = df1,\n  y = ~ reorder(title_type, number_of_records),\n  x = ~number_of_records,\n  type = \"bar\",\n  orientation = \"h\",\n  marker = list(color = \"cerulean\"),\n  width = 500,\n  height = 300\n)\n\nfig_content_count_type &lt;- fig_content_count_type |&gt;\n  layout(\n    title = \"Number of Titles by Content Type\",\n    xaxis = list(title = \"Number of Records\"),\n    yaxis = list(title = \"\")\n  )\n\n\nfig_content_count_type\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\n\n\nQ2. Who is the oldest living person in our data set?\nTo answer this question, we will use the name basics table, which has birth and death records. However, a quick examination of data highlights certain irregularities in death records. It appears that we are missing actual death records for a number of individuals who, despite being born prior to the 20th century, are appear to still be alive.\n\n# list living persons by year of birth\n\n# Subset of data - 10 oldest presumably living persons\nname_basics |&gt;\n  filter(is.na(death_year) & !is.na(birth_year)) |&gt;\n  arrange(birth_year) |&gt;\n  head(10) |&gt;\n  gt()\n\n\n\n\n\n\n\nnconst\nprimaryName\nbirth_year\ndeath_year\nprimaryProfession\nknownForTitles\n\n\n\n\nnm5671597\nRobert De Visée\n1655\nNA\ncomposer,soundtrack\ntt2219674,tt1743724,tt0441074,tt14426058\n\n\nnm7807390\nWilliam Sandys\n1767\nNA\ncomposer,soundtrack\ntt4396584,tt3747572,tt4555594,tt0071007\n\n\nnm1441282\nRichard Dybeck\n1811\nNA\nsoundtrack\ntt0021783,tt0022126,tt0036372,tt0037562\n\n\nnm6711738\nAlbert Monnier\n1815\nNA\nwriter\ntt0329972,tt3966780,tt6793558,tt15175930\n\n\nnm1227803\nC. Hostrup\n1818\nNA\nwriter,composer,actor\ntt0031361,tt0134089,tt0844680,tt14463014\n\n\nnm1329526\nEdouard Martin\n1825\nNA\nwriter\ntt0200268,tt0329972,tt3966780,tt0036496\n\n\nnm1197286\nIon Ivanovici\n1845\nNA\ncomposer,soundtrack\ntt0043412,tt0040391,tt1324061,tt0083697\n\n\nnm0179107\nAttilio Corbell\n1850\nNA\nactor\ntt0009508,tt0009121,tt0182770,tt0007472\n\n\nnm0843185\nAndré Sylvane\n1850\nNA\nwriter\ntt0019480,tt0155273,tt0159028,tt0167460\n\n\nnm0242243\nCharles Dungan\n1853\nNA\nactor\ntt0267008,tt0008259,tt0008876,tt0003634\n\n\n\n\n\n\n\n\n# create a df with records of living persons\n\ndf3 &lt;- name_basics |&gt;\n  filter(is.na(death_year) & !is.na(birth_year)) |&gt;\n  group_by(birth_year) |&gt;\n  summarise(number_of_records = n()) |&gt;\n  ungroup() |&gt;\n  arrange(birth_year)\n\n# plot the resulting df\n\nfig_cnt_living_persons &lt;- plot_ly(\n  data = df3,\n  x = ~birth_year,\n  y = ~number_of_records,\n  type = \"bar\",\n  marker = list(color = \"cerulean\"),\n  width = 500,\n  height = 300\n)\n\nfig_cnt_living_persons &lt;- fig_cnt_living_persons |&gt;\n  layout(\n    title = \"Living Persons by Year of Birth\",\n    xaxis = list(title = \"Year of Birth\"),\n    yaxis = list(title = \"Count of Living Persons\")\n  )\n\nfig_cnt_living_persons\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\nSince we can’t manually verify verify hundreds of questionable records, we will have to use a rule-based approach to answer this question. The oldest verified person to have ever lived was 122 years and 164 days at the time of death so using this age as a threshold, we can filter out all individuals born after 1902, which leaves us with 65 individuals born in 1903.\n\n# list count of living persons by year of birth\n\nname_basics |&gt;\n  filter(is.na(death_year) & !is.na(birth_year) & birth_year &gt; 1902) |&gt;\n  group_by(birth_year) |&gt;\n  summarize(number_of_records = n()) |&gt;\n  ungroup() |&gt;\n  arrange(birth_year) |&gt;\n  head(5) |&gt;\n  gt()\n\n\n\n\n\n\n\nbirth_year\nnumber_of_records\n\n\n\n\n1903\n65\n\n\n1904\n77\n\n\n1905\n68\n\n\n1906\n83\n\n\n1907\n78\n\n\n\n\n\n\n\n\n# list living persons born in 1903\ndf4 &lt;- name_basics |&gt;\n  filter(birth_year == 1903 & is.na(death_year)) |&gt;\n  select(primaryName, birth_year, death_year) |&gt;\n  arrange(primaryName)\n\nsample_n(df4, 65) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n\nQ3. There is one TV Episode in this data set with a perfect 10/10 rating and 200,000 IMDb ratings. What is it? What series does it belong to?\nTo answer this question, we need to use 3 data sets, title ratings,title basics and title episodes.\n\n# create df with list of all TV episodes\n\nlist_tv_epis &lt;- title_basics |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  select(tconst, titleType, primaryTitle)\n\n# create df with list of all TV series\n\nlist_tv_series &lt;- title_basics |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  select(tconst, titleType, primaryTitle)\n\n# create df with records of tv episodes\n\ntv_ep_df1 &lt;- inner_join(list_tv_epis, title_episodes, by = \"tconst\")\n\n# join ratings data\n\ntv_ep_df2 &lt;- inner_join(tv_ep_df1, title_ratings, by = \"tconst\")\n\n# find a TV episode meeting criteria\n\ntv_ep_df3 &lt;- tv_ep_df2 |&gt;\n  filter((numVotes &gt;= 200000) & (averageRating == 10))\n\n# map tv series name\n\ntv_ep_ratings_df &lt;- inner_join(tv_ep_df3, list_tv_series, by = c(\"parentTconst\" = \"tconst\"))\n\n# rename columns in the resulting df\ntv_ep_ratings_df |&gt;\n  rename(\n    episode_id = tconst,\n    average_rating = averageRating,\n    number_of_ratings = numVotes,\n    title_type = titleType.x,\n    episode_title = primaryTitle.x,\n    series_id = parentTconst,\n    series_name = primaryTitle.y,\n    parent_title_type = titleType.y\n  ) |&gt;\n  gt()\n\n\n\n\n\n\n\nepisode_id\ntitle_type\nepisode_title\nseries_id\nseason_number\nepisode_number\naverage_rating\nnumber_of_ratings\nparent_title_type\nseries_name\n\n\n\n\ntt2301451\ntvEpisode\nOzymandias\ntt0903747\n5\n14\n10\n227589\ntvSeries\nBreaking Bad\n\n\n\n\n\n\n\nThe TV episode with the perfect 10/10 rating and over 200K reviews is Ozymandias ep.15 season 5 of the cult TV hit Breaking Bad.\n\n\nQ4. What four projects is the actor Mark Hamill most known for?\nTo answer this question, we will use name basics and title basics data sets.\n\n# get title records for mark hamill\n\nmh_df &lt;- name_basics |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  select(primaryName, knownForTitles) |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n# map titles names and types on the list of selected content IDs\n\nmh_df2 &lt;- inner_join(mh_df, title_basics, by = c(\"knownForTitles\" = \"tconst\"))\nmh_df2 |&gt;\n  select(knownForTitles, titleType, primaryTitle, start_year) |&gt;\n  rename(\n    title_id = knownForTitles,\n    content_type = titleType,\n    content_title = primaryTitle,\n    year = start_year\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Titles Mark Hamill is Known For\"\n  )\n\n\n\n\n\n\n\nTitles Mark Hamill is Known For\n\n\ntitle_id\ncontent_type\ncontent_title\nyear\n\n\n\n\ntt0076759\nmovie\nStar Wars: Episode IV - A New Hope\n1977\n\n\ntt2527336\nmovie\nStar Wars: Episode VIII - The Last Jedi\n2017\n\n\ntt0080684\nmovie\nStar Wars: Episode V - The Empire Strikes Back\n1980\n\n\ntt0086190\nmovie\nStar Wars: Episode VI - Return of the Jedi\n1983\n\n\n\n\n\n\n\nMark Hamill is known for his roles in the Star Wars movies, where he first starred in 1977 and most recently in 2017.\n\n\nQ5. What TV series, with more than 12 episodes, has the highest average rating?\nTo answer this question, we need 3 data sets - title_episodes, title ratings and title basics.\n\n# we already have a df with all TV series - we created it in a previous question - list_tv_series\n\n# create a df with records of tv series wirh all episodes\n\nep_filtered_series &lt;- inner_join(title_episodes, list_tv_series, by = c(\"parentTconst\" = \"tconst\"))\n\n# df with tv series with 12+ episodes\n\nseries_num_epis &lt;- ep_filtered_series |&gt;\n  group_by(parentTconst, primaryTitle, titleType) |&gt;\n  summarise(number_of_episodes = n()) |&gt;\n  ungroup() |&gt;\n  arrange(desc(number_of_episodes)) |&gt;\n  filter(number_of_episodes &gt;= 12)\n\ndatatable(series_num_epis)\n\n\n\n\n\nWe have over 20K TV series with 12 or more episodes.\n\n# join tv episodes and series data with ratings data\n\nep_filtered_series_ratings &lt;- inner_join(ep_filtered_series,\n  title_ratings,\n  by = \"tconst\"\n)\n\n# drop all tv series with fewer than 12 episodes\n\nep_filtered_series_ratings2 &lt;- inner_join(ep_filtered_series_ratings,\n  series_num_epis,\n  by = \"parentTconst\"\n)\n\n# calculate average ratings for tv series\n\nep_filtered_series_ratings2 |&gt;\n  group_by(parentTconst, primaryTitle.x) |&gt;\n  summarise(average_rating = mean(averageRating)) |&gt;\n  ungroup() |&gt;\n  rename(\n    tv_series_id = parentTconst,\n    tv_series_title = primaryTitle.x\n  ) |&gt;\n  arrange(desc(average_rating)) |&gt;\n  head(5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 TV Series by Average Rating\",\n    subtitle = \"TV series with 12 or more episodes only\"\n  )\n\n\n\n\n\n\n\nTop 5 TV Series by Average Rating\n\n\nTV series with 12 or more episodes only\n\n\ntv_series_id\ntv_series_title\naverage_rating\n\n\n\n\ntt0409579\nMade\n10.0\n\n\ntt11363282\nThe Real Housewives of Salt Lake City\n10.0\n\n\ntt21278628\nCowboys of Thunder\n10.0\n\n\ntt0060008\nThe Milton Berle Show\n9.9\n\n\ntt0168358\nParkinson\n9.9\n\n\n\n\n\n\n\nThere are 3 TV series that obtained the perfect 10/10 rating - ‘Made’,‘The Real Housewives of Salt Lake City’ and ‘Cowboys of Thunder’.\n\n\nQ6. Is it true that episodes from later seasons of Happy Days have lower average ratings than the early seasons?\nTo answer this question, we will use title basics,title episodes and title ratings data sets:\n\n# create df for TV series 'Happy Days'\n\nhd_df1 &lt;- title_basics |&gt;\n  filter(primaryTitle == \"Happy Days\" & titleType == \"tvSeries\")\n\n# join HD df with detailed TV episodes data\nhd_detail &lt;- inner_join(title_episodes, hd_df1, by = c(\"parentTconst\" = \"tconst\"))\n\n# join ratings data to detailed Happy Days records\n\nhd_detail_ratings &lt;- inner_join(hd_detail, title_ratings, by = \"tconst\")\n\ndatatable(hd_detail_ratings)\n\n\n\n\n\nNow that we have detailed records on all episodes of the Happy Days TV series, we can calculate the average rating for each season.\nIt appears that the earlier seasons of the series indeed had higher average ratings compared to the more recent seasons.\n\n# create df with average rating by season\n\navg_hd_detail_ratings &lt;- hd_detail_ratings |&gt;\n  group_by(season_number) |&gt;\n  summarise(avg_rating_season = mean(averageRating)) |&gt;\n  ungroup() |&gt;\n  arrange(season_number)\n\n# plot the resulting df\n\nfig_hd_seasons &lt;- plot_ly(\n  data = avg_hd_detail_ratings,\n  x = ~season_number,\n  y = ~avg_rating_season,\n  type = \"bar\",\n  marker = list(color = \"cerulean\"),\n  width = 500,\n  height = 300\n)\n\nfig_hd_seasons &lt;- fig_hd_seasons |&gt;\n  layout(\n    title = \"Happy Days - Average Rating by Season\",\n    xaxis = list(title = \"Season #\"),\n    yaxis = list(title = \"Average Rating\")\n  )\n\nfig_hd_seasons\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\n\n\n\nTask 3\n\nDesign a ‘success’ measure for IMDb entries, reflecting both quality and broad popular awareness.\n\nAs we found in Q1 in Task1, movies constitute the absolute majority of records in our data - 131.6K records vs 29.8K for TV series, the next largest category of content. We do not include TV episode in this analysis as TV episodes are not a standalone content. Given the obvious differences in production, marketing, and audience appeal, we will focus on movies for this part of the exercise.\n\n# plot number of records by content type from the earlier question\n\nfig_content_count_type\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\nLet’s start with creating a data frame with ratings data for movies.\n\n# create df with list of all movies\n\nlist_movies &lt;- title_basics |&gt;\n  filter(titleType == \"movie\") |&gt;\n  select(tconst, titleType, primaryTitle, start_year, genres, runtime_minutes, isAdult)\n\n# join with ratings data\n\nmovie_ratings_df &lt;- inner_join(list_movies, title_ratings, by = \"tconst\")\n\nmovie_ratings_df2 &lt;- movie_ratings_df |&gt;\n  rename(\n    title = primaryTitle,\n    title_id = tconst,\n    content_type = titleType,\n    year = start_year,\n    average_rating = averageRating,\n    number_of_votes = numVotes\n  )\n\n# sample movie df\n\nsample_n(movie_ratings_df2, 1000) |&gt;\n  DT::datatable()\n\n\n\n\n\nNext we will conduct an explanatory data analysis on our movies data set to better understand the two ratings metrics.\n\n# subset metrics\n\nmovie_ratings_df2_metrics &lt;- movie_ratings_df2 |&gt;\n  select(average_rating, number_of_votes)\n\n# describe metrics\n\nsummary(movie_ratings_df2_metrics)\n\n average_rating   number_of_votes  \n Min.   : 1.000   Min.   :    100  \n 1st Qu.: 5.200   1st Qu.:    195  \n Median : 6.100   Median :    459  \n Mean   : 5.923   Mean   :   8694  \n 3rd Qu.: 6.800   3rd Qu.:   1664  \n Max.   :10.000   Max.   :2942823  \n\n\n\n# histogram of average ratings\n\n# plot a histogram of number of ratings in plotly\navg_ratings_x &lt;- movie_ratings_df2$average_rating\n\nfig_hist_avg_ratings &lt;- plot_ly(\n  x = avg_ratings_x,\n  type = \"histogram\",\n  nbinsx = 100,\n  marker = list(color = \"cerulean\")\n) |&gt;\n  layout(\n    title = \"Distribution of Average Movie Ratings\",\n    xaxis = list(title = \"Average Rating\"),\n    yaxis = list(title = \"Frequency\")\n  )\n\nfig_hist_avg_ratings\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\n\n# histogram of average ratings\n\n# plot a histogram of number of ratings in plotly\n\nnum_ratings_x &lt;- movie_ratings_df2$number_of_votes\n\nfig_distr_number_ratings &lt;- plot_ly(\n  x = num_ratings_x,\n  type = \"histogram\",\n  nbinsx = 80,\n  marker = list(color = \"cerulean\")\n) |&gt;\n  layout(\n    title = \"Distribution of Movie Ratings\",\n    xaxis = list(title = \"Number of Ratings\"),\n    yaxis = list(type = \"log\", title = \"Frequency (Log-Scaled)\")\n  )\n\nfig_distr_number_ratings\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interaction options.)\nLooking at descriptive statistics and statistical plots, we can see that most titles have relatively high average ratings. 50% of all titles have a rating above 6.1, and top 25% of titles have a rating over 6.8. Distribution of number of ratings, on the other hand, has a right skew, meaning that we have only a handful of titles with a very high number of votes.\nSince we need to design a blended performance metric, we need to account for quality and popularity of a title simultaneously which can be done by an averaging of these two metrics. Before we proceed, we need to standardize the data to account for differences in magnitude and distribution of ratings and votes variables:\n\n# calculate mean and standard deviation for ratings and votes data\n\nmovie_ratings_df3 &lt;- movie_ratings_df2 |&gt;\n  mutate(\n    avg_ratings_movies = mean(average_rating),\n    avg_number_ratings = mean(number_of_votes),\n    sd_avg_ratings = sd(average_rating),\n    sd_number_ratings = sd(number_of_votes)\n  )\n\nsample_n(movie_ratings_df3, 1000) |&gt;\n  DT::datatable(options = list(\n    pageLength = 5\n  ))\n\n\n\n\n\nNow we can create standardized metrics for ratings and votes, as well as the blended performance index reflecting the quality of the movie (via average rating) and the popularity of the movie (via number of ratings), with equal weight given to each input.\n\n# create standardized metrics for votes and ratings\n\nmovie_ratings_df4 &lt;- movie_ratings_df3 |&gt;\n  mutate(\n    score_rating = round((average_rating - avg_ratings_movies) / sd_avg_ratings, 2),\n    score_votes = round((number_of_votes - avg_number_ratings) / sd_number_ratings, 2),\n    performance_index = round((score_rating + score_votes) / 2, 2)\n  )\n\nsample_n(movie_ratings_df4, 1000) |&gt;\n  DT::datatable(options = list(\n    pageLength = 5\n  ))\n\n\n\n\n\n\n# descriptive statistics for performance index\n\nmovie_ratings_df4_pi &lt;- movie_ratings_df4 |&gt;\n  select(performance_index)\n\nsummary(movie_ratings_df4_pi)\n\n performance_index  \n Min.   :-1.990000  \n 1st Qu.:-0.360000  \n Median : 0.030000  \n Mean   : 0.000264  \n 3rd Qu.: 0.310000  \n Max.   :27.490000  \n\n\n\n# histogram of performance index\n\npi_x2 &lt;- movie_ratings_df4$performance_index\n\nfig7 &lt;- plot_ly(\n  x = pi_x2,\n  type = \"histogram\",\n  nbinsx = 200,\n  marker = list(color = \"blue\")\n) |&gt;\n  layout(\n    title = \"Distribution of Movie Performance Indices\",\n    xaxis = list(title = \"Performance Index\"),\n    yaxis = list(title = \"Frequency\")\n  )\n\nfig7\n\n\n\n\n\n\n# % of titles with negative PI\n\nmovie_ratings_df4_pi |&gt;\n  summarise(\n    titles_with_negative_pi = sum(performance_index &lt; 0),\n    all_titles = n()\n  ) |&gt;\n  mutate(share_of_titles_with_negative_pi = round(titles_with_negative_pi / all_titles, 2)) |&gt;\n  gt()\n\n\n\n\n\n\n\ntitles_with_negative_pi\nall_titles\nshare_of_titles_with_negative_pi\n\n\n\n\n61673\n131662\n0.47\n\n\n\n\n\n\n\nPerformance index penalizes titles with subpar, i.e., below average, popularity and/or quality. 47% of movies in our data set have negative performance index.\n\nPerformance Index Validation\n\n1.Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\n\n\n# top 5 movies\n\nmrdf &lt;- movie_ratings_df4 |&gt;\n  select(title, year, genres, average_rating, number_of_votes, performance_index)\n\nmrdf |&gt;\n  arrange(performance_index) |&gt;\n  slice_max(performance_index, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 Movies by Peformance Index\"\n  )\n\n\n\n\n\n\n\nTop 5 Movies by Peformance Index\n\n\ntitle\nyear\ngenres\naverage_rating\nnumber_of_votes\nperformance_index\n\n\n\n\nThe Shawshank Redemption\n1994\nDrama\n9.3\n2942823\n27.49\n\n\nThe Dark Knight\n2008\nAction,Crime,Drama\n9.0\n2922922\n27.20\n\n\nInception\n2010\nAction,Adventure,Sci-Fi\n8.8\n2595555\n24.20\n\n\nFight Club\n1999\nDrama\n8.8\n2374722\n22.23\n\n\nForrest Gump\n1994\nDrama,Romance\n8.8\n2301630\n21.57\n\n\n\n\n\n\n\nAmong top 5 movies based on performance index, four (with the exception of The Shawshank Redemption) were commercial successes, and The Shawshank Redemption is still widely considered to be one of the beloved and most critically acclaimed movies of all times.\n\n\nChoose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\n\n\n\n# add this to top line to change plot size: , fig.width=4,fig.height=4}\n\n\n# plot ratings and votes data\n\ngfig &lt;- ggplot(data = movie_ratings_df2, aes(x = average_rating, y = number_of_votes)) +\n  geom_point(size = 1, color = \"blue\") +\n  labs(\n    title = \"Movie Quality (Average Rating) and Popularity (Number of Ratings)\",\n    x = \"Average Rating\",\n    y = \"Number of Ratings\"\n  ) +\n  theme_minimal() +\n  theme_bw() +\n  scale_x_log10(label = scales::comma) +\n  scale_y_log10(label = scales::comma)\n\n\ngfig\n\n\n\n\n\n\n\n\nAs seen on this chart, we should have a decent number of movies with average rating of 1-2 and 80K-100K number of ratings, so we will look up titles meeting these criteria:\n\nmovie_ratings_df4 |&gt;\n  filter(average_rating &lt; 3 & number_of_votes &gt;= 75000) |&gt;\n  arrange(desc(performance_index)) |&gt;\n  select(title, year, genres, average_rating, number_of_votes, performance_index) |&gt;\n  gt()\n\n\n\n\n\n\n\ntitle\nyear\ngenres\naverage_rating\nnumber_of_votes\nperformance_index\n\n\n\n\nRadhe\n2021\nAction,Crime,Thriller\n1.9\n180205\n-0.04\n\n\nAdipurush\n2023\nAction,Adventure,Drama\n2.7\n133981\n-0.13\n\n\nMeet the Spartans\n2008\nComedy,Fantasy\n2.8\n112199\n-0.29\n\n\nEpic Movie\n2007\nAdventure,Comedy,Fantasy\n2.4\n110222\n-0.47\n\n\nBattlefield Earth\n2000\nAction,Adventure,Sci-Fi\n2.5\n83786\n-0.66\n\n\nDragonball Evolution\n2009\nAction,Adventure,Fantasy\n2.5\n80118\n-0.70\n\n\nDisaster Movie\n2008\nComedy,Sci-Fi\n1.9\n95170\n-0.80\n\n\nJustin Bieber: Never Say Never\n2011\nDocumentary,Music\n1.7\n76466\n-1.04\n\n\nSadak 2\n2020\nAction,Drama\n1.2\n96825\n-1.06\n\n\n\n\n\n\n\nIndeed, these movies score very poorly on the performance index, and while they have a relatively large volume of ratings, they also have low average ratings.\n\n\nChoose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\n\n\nSteven Spielberg, one of the most famous and successful directors of our time, has 4 very successful projects with performance index of ranging from 4.65 to 14.54, which puts these titles in top 1% of all movies in our data set.\n\n# get title records for Steven Spielberg\n\nbp_df &lt;- name_basics |&gt;\n  filter(primaryName == \"Steven Spielberg\") |&gt;\n  select(primaryName, knownForTitles) |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n# map titles names and types on the list of selected content IDs\n\nbp_df2 &lt;- inner_join(bp_df, title_basics, by = c(\"knownForTitles\" = \"tconst\"))\nbp_df3 &lt;- bp_df2 |&gt;\n  select(primaryName, knownForTitles, titleType, primaryTitle) |&gt;\n  rename(\n    name = primaryName,\n    title_id = knownForTitles,\n    content_type = titleType,\n    content_title = primaryTitle\n  )\n\n# select performance index and title\nmovie_pi_df &lt;- movie_ratings_df4 |&gt;\n  select(title_id, average_rating, number_of_votes, performance_index)\n\n# join to SS records\nbp_df4 &lt;- inner_join(bp_df3, movie_pi_df, by = \"title_id\")\ndatatable(bp_df4)\n\n\n\n\n\n\n# percentiles for performance index\n\nquantile(movie_ratings_df4$performance_index, probs = c(0, 0.125, 0.375, 0.625, 0.875, 0.9, 0.95, 0.99, 1))\n\n   0% 12.5% 37.5% 62.5% 87.5%   90%   95%   99%  100% \n-1.99 -0.67 -0.16  0.16  0.51  0.58  0.78  2.00 27.49 \n\n\n\n\nPerform at least one other form of ‘spot check’ validation.\n\n\nAvatar, the highest-grossing movie of all times ($2.9B worldwide gross) has a performance index of 13.2, which puts it in top 1% of our data set.\n\n# select performance index and title\n\nmovie_ratings_df4 |&gt;\n  select(title, genres, year, average_rating, number_of_votes, performance_index) |&gt;\n  filter((title == \"Avatar\") & (year == 2009)) |&gt;\n  gt()\n\n\n\n\n\n\n\ntitle\ngenres\nyear\naverage_rating\nnumber_of_votes\nperformance_index\n\n\n\n\nAvatar\nAction,Adventure,Fantasy\n2009\n7.9\n1402915\n13.2\n\n\n\n\n\n\n\n\n\nCome up with a numerical threshold for a project to be a ‘success’; that is, determine a value such that movies above are all “solid” or better.\n\n\n\n# percentiles for performance index\n\nquantile(movie_ratings_df4$performance_index, probs = c(0, 0.10, 0.20, 0.40, 0.50, 0.60, 0.80, 0.95, 1))\n\n   0%   10%   20%   40%   50%   60%   80%   95%  100% \n-1.99 -0.74 -0.47 -0.12  0.03  0.12  0.38  0.78 27.49 \n\n\nWe will use 0.38 (top 20% score cutoff) as a threshold of success - titles with performance index of 0.38 or higher are high performers.\n\n\nTask 4: Trends in Success Over Time\nWe need to review our records in the context of distribution of titles by decade and genre.\nDue to a low volume of production and a stable share of successful productions over time, we can exclude data prior to 1970.\n\n# add new columns for decade and success\nmovie_ratings_df4 &lt;- movie_ratings_df4 |&gt;\n  mutate(\n    decade = floor(year / 10) * 10,\n    success_flag = case_when(\n      performance_index &gt; 0.38 ~ 1,\n      performance_index &lt;= 0.38 ~ 0\n    )\n  )\n\nmovie_ratings_df4_agg_decade &lt;- movie_ratings_df4 |&gt;\n  select(title_id, title, genres, decade, year, performance_index, success_flag) |&gt;\n  group_by(decade) |&gt;\n  summarise(\n    number_of_titles = n(),\n    number_of_successes = sum(success_flag == 1),\n    number_of_flops = sum(success_flag == 0)\n  ) |&gt;\n  ungroup()\n\n# plot the resulting df\n\nfig_decade &lt;- plot_ly(\n  data = movie_ratings_df4_agg_decade,\n  x = ~decade,\n  y = ~number_of_successes,\n  type = \"bar\",\n  name = \"number of successes\",\n  #  marker = list(color = \"blue\"),\n  width = 500,\n  height = 300\n) |&gt;\n  add_trace(y = ~number_of_flops, name = \"number of flops\")\n\n\nfig_decade &lt;- fig_decade |&gt;\n  layout(\n    title = \"Titles by Decade\",\n    xaxis = list(title = \"Decade\"),\n    yaxis = list(title = \"Count of Titles\"),\n    barmode = \"stack\"\n  )\n\nfig_decade\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interactive options.)\n\n# create a df for genres\n\nmovie_ratings_df4_agg_genres &lt;- movie_ratings_df4 |&gt;\n  select(title_id, title, genres, decade, year, performance_index, success_flag) |&gt;\n  group_by(genres) |&gt;\n  summarise(\n    number_of_titles = n(),\n    number_of_successes = sum(success_flag == 1),\n    number_of_flops = sum(success_flag == 0)\n  ) |&gt;\n  arrange(desc(number_of_titles)) |&gt;\n  ungroup()\n\n# plot the resulting df\n\nfig_genres &lt;- plot_ly(\n  data = movie_ratings_df4_agg_genres,\n  x = ~ reorder(genres, -number_of_titles),\n  y = ~number_of_titles,\n  type = \"bar\",\n  marker = list(color = \"blue\")\n)\n\nfig_genres &lt;- fig_genres |&gt;\n  layout(\n    title = \"Count of Titles by Genre\",\n    xaxis = list(\n      title = \"Genres\",\n      tickangle = -45,\n      tickfont = list(size = 8)\n    ),\n    yaxis = list(title = \"Count of Titles\")\n  )\n\nfig_genres\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interactive options.)\nWe have a very large number of genres with only a handful of titles, so we can exclude these records from our data set to ensure our analysis is as robust as possible.\n\n# top 20 genres by count of titles\n\nmovie_ratings_df4_top20_genres &lt;- movie_ratings_df4_agg_genres |&gt;\n  slice_max(number_of_titles, n = 20)\n\n# subset data by decade and aggregate count successes and flops\n\nmovie_ratings_df4_decade_genres &lt;- movie_ratings_df4 |&gt;\n  filter(year &gt;= 1970) |&gt;\n  select(title_id, title, genres, decade, year, success_flag) |&gt;\n  group_by(genres, decade) |&gt;\n  summarise(\n    number_of_titles = n(),\n    number_of_successes = sum(success_flag == 1),\n    number_of_flops = sum(success_flag == 0)\n  ) |&gt;\n  mutate(percent_of_success = round(number_of_successes / number_of_titles, 2)) |&gt;\n  ungroup()\n\nmovie_ratings_df4_decade_genres_filtered &lt;- inner_join(movie_ratings_df4_decade_genres,\n  movie_ratings_df4_top20_genres,\n  by = \"genres\"\n) |&gt;\n  select(genres, decade, number_of_titles.x, number_of_successes.x, number_of_flops.x, percent_of_success) |&gt;\n  rename(\n    number_of_titles = number_of_titles.x,\n    number_of_successes = number_of_successes.x,\n    number_of_flops = number_of_flops.x\n  )\n\ndatatable(movie_ratings_df4_decade_genres_filtered)\n\n\n\n\n\n\n1.What was the genre with the most “successes” in each decade?\n\nDrama produced more successes than other genres in 1970s (292 titles), 1980s (341 titles),and 1990s (334 titles). Starting in 2000s, Documentary took over with 747 successes in 2000s, 1290 successes in 2010, and 593 successful titles in 2020.\n\n# subset successes by genre and decade\n\nmovie_ratings_df4_decade_genres_filtered_successes_pw &lt;- pivot_wider(\n  movie_ratings_df4_decade_genres_filtered,\n  id_cols = genres,\n  names_from = decade,\n  values_from = number_of_successes\n)\n\n\n\ndatatable(movie_ratings_df4_decade_genres_filtered_successes_pw)\n\n\n\n\n\n\n\nWhat genre consistently has the most “successes”?\n\n\nDrama and documentary collectively produced more successes than other genres (2915 and 2868, respectively), with Documentary emerging as a leading genre in recent decades (2000- present).\n\ngp1 &lt;- ggplot(\n  movie_ratings_df4_decade_genres_filtered,\n  aes(x = decade, y = number_of_successes)\n) +\n  geom_col(fill = \"green4\") +\n  labs(\n    title = \"Successful Productions by Genre\",\n    x = \"Decade\",\n    y = \"Successful Productions\"\n  ) +\n  geom_text(aes(label = number_of_successes),\n    position = position_stack(vjust = 2), # Place labels outside the bars\n    size = 2\n  ) +\n  facet_grid(. ~ genres) +\n  facet_wrap(~genres, ncol = 4, strip.position = \"top\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(size = 8),\n    axis.text.y = element_text(size = 8),\n    axis.title.x = element_text(size = 10),\n    axis.title.y = element_text(size = 10),\n    plot.title = element_text(hjust = 0.5, size = 14)\n  )\n\ngp1\n\n\n\n\n\n\n\n\n\n\nWhat genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\n\n\n\n# create a custom color palette\npalette_genres3 &lt;- c(\n  \"dodgerblue2\", \"#E31A1C\", \"green4\", \"#6A3D9A\", \"#FF7F00\",\n  \"black\", \"yellow\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n  \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\",\n  \"orchid1\", \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\",\n  \"green1\", \"yellow4\", \"yellow3\", \"darkorange4\", \"brown\"\n)\n\n# create a chart\n\nfig_top20g &lt;- plot_ly(movie_ratings_df4_decade_genres_filtered,\n  x = ~decade, y = ~percent_of_success,\n  color = ~genres,\n  type = \"scatter\",\n  mode = \"lines\",\n  colors = palette_genres3\n) |&gt;\n  layout(\n    title = \"Top 20 Genres - Percent of Success by Decade\",\n    xaxis = list(title = \"\"),\n    yaxis = list(\n      title = \"Percent of Success\",\n      tickformat = \".0%\",\n      range = c(0, 1)\n    ),\n    legend = list(\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2, # Position below the plot\n      font = list(size = 8) # Smaller font size\n    )\n  )\n\n# Show the plot\n\nfig_top20g\n\n\n\n\n\n(Please note it’s an interactive chart - hover over it for interactive options.)\nDocumentary has produced most successful titles since 2010 (1883 titles) and it has the best success rate of all genres..\n\n4.What genre has become more popular in recent years?\n\nThere has been a spike in success rate for Action genre, going from 5% in 2010s to 21% in 2020s.\nBased on success rate, documentary is a clear standout and should be prioritized for investment opportunity.\n\n\nTask 5: Key Personnel\n\nIdentify (at least) two actors and one director who you will target as the key talent for your movie. Write a short “pitch” as to why they are likely to be successful. You should support your pitch with at least one graphic and one table.\n\nSince we are going to be developing a documentary title, we need to adjust this question a bit and identify a director-writer team as opposed to a director-actors team.\n\n# get a list of titles in documentary genre, made after 1970 , with sufficient level of awareness and high performance index and map director and writer info\n\ndoc_df1 &lt;- sqldf(\n  \"\n    with a as(\n    select title_id,\n    title,\n    decade,\n    performance_index,\n    average_rating,\n    number_of_votes,\n    success_flag\n    from movie_ratings_df4\n    where 1=1\n    and genres='Documentary'\n    and year&gt;=1970\n   and success_flag=1\nand number_of_votes&gt;=5000\n    )\n    select a.*,\n    t.directors,\n    t.writers,\n    n.primaryName as director_name,\n    n2.primaryName as writer_name,\n    tb.start_year\n    from a\n    inner join title_crew t\n    on a.title_id=t.tconst\n    inner join name_basics n\n    on directors=n.nconst\n    inner join name_basics n2\n    on t.writers=n2.nconst\n    inner join title_basics tb\n    on a.title_id=tb.tconst\n    order by a.performance_index desc, a.number_of_votes desc, a.average_rating desc\n\n    ;\n  \"\n)\n\n# calculate performance statistics for director-writer teams\n\ndoc_df2 &lt;- sqldf(\n  \"\nselect director_name,\nwriter_name,\ndirector_name||'-'||writer_name as movie_team,\ncount(title_id) as cnt_movies,\nround(avg(performance_index),2) as avg_performance_index,\nround(avg(average_rating),1) as avg_rating,\nround(avg(number_of_votes),0) as avg_number_of_ratings\nfrom doc_df1\ngroup by 1,2,3\nhaving count(title_id)&gt;1\norder by 4 desc\n    ;\n  \"\n)\n\ndatatable(doc_df2)\n\n\n\n\n\nLooking at the high-performing documentaries from 1970s - present, 3 film makers have produced multiple successful titles: Werner Herzog, Michael Moore and the director-writer duo of Sophie Fiennes and Slavoj Zizek. Since we need to identify a team for our next project, we propose to approach the Fiennes-Zizek duo as they have already demonstrated they can successfully work together, which might not be the case for established solo creators Moore and Herzog.\n\n# plot movie team data\n\n\nfig_movie_team &lt;- plot_ly(\n  data = doc_df2,\n  x = ~avg_rating,\n  y = ~avg_number_of_ratings,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(size = 10),\n  color = ~movie_team\n) |&gt;\n  layout(\n    title = \"Movie Team Performance Comparison\",\n    xaxis = list(title = \"Quality (Average Rating)\"),\n    yaxis = list(\n      title = \"Popularity (Number of Ratings)\"\n    ),\n    legend = list(\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2\n    )\n  )\n\nfig_movie_team\n\n\n\n\n\nTitles produced by Moore and Herzog appear to have a higher awareness among viewers but Fiennes-Zizek work is not far behind, and a more polarizing topic and a targeted marketing and PR campaign can help address this slight shortcoming.\n\n\nTask 6: Finding a Classic Movie to Remake\n\nFind a classic movie to remake with your key talent. The original should have a large number of IMDb ratings, a high average rating, and not have been remade in the past 25 years.\n\nWhen looking at the top documentary titles, Super Size Me is a definite outlier: Super Size Me premiered at the 2004 Sundance Film Festival, where Morgan Spurlock won the Grand Jury Prize for directing the film.The film opened in the US on May 7, 2004, and grossed a total of $11,536,423 worldwide, making it the 7th highest-grossing documentary film of all time.It was nominated for an Academy Award for Best Documentary Feature and won the award for Best Documentary Screenplay from the Writers Guild of America. (Source).\nA 2017 title Super Size Me 2: Holy Chicken! from the same director also performed reasonably well, even in the light of certain issues with with publicity and distribution. It’s important to note that this film was not a remake of a original title as it was focused on the process of opening a fast-food restaurant. (Source)\nGiven the success of the 2004 ‘Super Size Me’ and increasing popularity of the semaglutide drugs, we should consider making a documentary about a weight loss journey and impact of taking this medicine on one’s life, health and mind - a ‘Super Size Me’ journey in reverse. While this movie was released 20 years ago, cultural context, relevancy and timeliness play a huge role in documentary titles success, and for this topic the time is definitely now. Another reason to pursue this opportunity now is an unhappy one as Morgan Spurlock, the writer and director of both ‘Super Size Me’ titles, died in May of this year so re imagining his most famous work could serve as a tribute to Spurlock’s many talents and the impact his vision and creative genius left on our society. As a possible contributor to our project, we can consider Lee Fulkerson, who wrote and directed an award-winning and highly acclaimed documentary Forks Over Knives as he has already successfully explored the topic of self-improvement in his 2011 movie (performance index of 0.73).\n\n\nTask 7: Write and Deliver Your Pitch\nFrom Sophie Fiennes and Slavoj Zizek, the masters of philosophical and psychoanalytical exploration, and Lee Fulkerson, the visionary mind behind an inspiring story of human transformation, inspired by a critically acclaimed hit Super Size Me, comes the modern take on a timeless tale of metamorphosis, obsession and desire to be perfect at any cost. XXS Me: The Beginning coming to Netflix in December 2025."
  },
  {
    "objectID": "mp03_nodata_load.html",
    "href": "mp03_nodata_load.html",
    "title": "Mini-Project 03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "In this paper, we will investigate the claim that the US Electoral College systematically biases election results away from the popular vote.\n\nData Prep\nIn this section, we obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\nQ1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n## read in presidential elections data\n\npres&lt;-read.csv(\"president_1976_2020.csv\")\n#head(pres)\n\n# read in house election vote data \n\nhouse&lt;-read.csv(\"house_1976_2022.csv\")\n#head(house)\n\n\n## create a df with house results for 1976 and 2022\n\nhouse_1976_and_2022 &lt;- sqldf(\n  \"\n   with h76 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=1976\n   group by 1\n   )\n   ,\n   h22 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2022\n   group by 1\n   )\n   , \n   base as (\n   select h76.state,\n   h76.num_seats,\n   case\n   when h76.num_seats=0 then 1 \n   else h76.num_seats\n   end as number_of_seats_1976,\n   h22.num_seats,\n   case\n   when h22.num_seats=0 then 1 \n   else h22.num_seats\n   end as number_of_seats_2022\n   from h76\n   left join h22\n   on h76.state=h22.state\n   )\n   \n   select state,\n   number_of_seats_1976,\n   number_of_seats_2022,\n   number_of_seats_2022 - number_of_seats_1976 as delta \n   from base \n    ;\n  \"\n)\n\n\n# display states with largest gains in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n   slice_max(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Gains in House Seats\"\n  )\n\n\n\n\n\n\n\nTop 5 States with Largest Gains in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nTEXAS\n24\n38\n14\n\n\nFLORIDA\n15\n28\n13\n\n\nCALIFORNIA\n43\n52\n9\n\n\nARIZONA\n4\n9\n5\n\n\nGEORGIA\n10\n14\n4\n\n\n\n\n\n\n\n\n# display states with largest losses in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n   slice_min(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Losses in House Seats\"\n  )\n\n\n\n\n\n\n\nTop 5 States with Largest Losses in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nNEW YORK\n39\n26\n-13\n\n\nOHIO\n23\n15\n-8\n\n\nPENNSYLVANIA\n25\n17\n-8\n\n\nILLINOIS\n24\n17\n-7\n\n\nMICHIGAN\n19\n13\n-6\n\n\n\n\n\n\n\n\n\nQ2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nFirst, let’s find states, years, and districts where fusion system was in place\n\n#get a count of political parties candidates received votes from\nhouse_temp1 &lt;- house %&gt;%\n  group_by(state, year, district, candidate) |&gt;\n  summarize(distinct_party_count = n_distinct(party))\n\n#get a list of districts where candidates received votes from more than 1 party\nhouse_temp2&lt;-house_temp1 |&gt;\n  filter(distinct_party_count&gt;1)\n\n#get a list of all years, states and districts where fusion system was used\nhouse_temp3&lt;- house_temp2 |&gt;\n  group_by(state, year, district)\n\n# subset house df to only include results from states, years and districts meeting the criteria \n\nhouse_fusion1 &lt;- inner_join(house,house_temp3, by=c(\"state\"=\"state\",\"year\"=\"year\",\"district\"=\"district\"))\n\n# get elections totals for each candidate in states/years/districts meeting the criteria\nhouse_fusion1_actuals&lt;-house_fusion1 |&gt;\n  group_by(year,state,district,candidate.x) |&gt;\n  summarise(actual_total_votes=sum(candidatevotes)) |&gt;\n  mutate(max_votes=max(actual_total_votes)) |&gt;\n  mutate(is_actual_winner=case_when(actual_total_votes==max_votes ~1,\n                             TRUE ~0)) |&gt;\n  ungroup()\n\n# get candidate votes from their primary party only and determine a winner\n house_fusion1_primaryonly&lt;-house_fusion1 |&gt;\n  filter(party=='DEMOCRAT' | party=='REPUBLICAN') |&gt;\n  group_by(year,state,district,candidate.x) |&gt;\n  summarise(actual_primaryparty_votes=sum(candidatevotes)) |&gt;\n  mutate(max_primary_votes=max(actual_primaryparty_votes)) |&gt;\n  mutate(is_primaryvotesonly_winner=case_when(actual_primaryparty_votes==max_primary_votes ~1,\n                             TRUE ~0)) \n\n#merge 2 datasets\nhouse_fusion_merged&lt;-left_join(house_fusion1_actuals,\n                               house_fusion1_primaryonly,\n                               by=c(\"state\"=\"state\",\n                                    \"year\"=\"year\",\n                                    \"district\"=\"district\",\n                                    \"candidate.x\"=\"candidate.x\")\n                               )\n#filter for records where primary party is either D or R and determine if results would have been different\nhouse_fusion_merged_filtered&lt;-house_fusion_merged |&gt;\n  select(year,state,district,candidate.x,actual_total_votes,actual_primaryparty_votes,is_actual_winner,is_primaryvotesonly_winner) |&gt;\n  mutate(same_winner=case_when(is_actual_winner==is_primaryvotesonly_winner ~1,\n                             TRUE ~0)) |&gt;\n  filter((same_winner==0) & !is.na(actual_primaryparty_votes))\n  \n#display results\nhouse_fusion_merged_filtered |&gt;\n  rename(candidate=candidate.x) |&gt;\n   gt() |&gt;\n  tab_header(\n    title = \"Elections With  Different Results In The Absense of Fusion System\"\n  )\n\n\n\n\n\n\n\nElections With Different Results In The Absense of Fusion System\n\n\nyear\nstate\ndistrict\ncandidate\nactual_total_votes\nactual_primaryparty_votes\nis_actual_winner\nis_primaryvotesonly_winner\nsame_winner\n\n\n\n\n1976\nNEW YORK\n29\nEDWARD W PATTISON\n100663\n95361\n1\n0\n0\n\n\n1976\nNEW YORK\n29\nJOSEPH A MARTINO\n96476\n96476\n0\n1\n0\n\n\n1980\nNEW YORK\n3\nGREGORY W CARMAN\n175904\n149736\n1\n0\n0\n\n\n1980\nNEW YORK\n3\nJEROME A AMBRO JR\n166778\n150778\n0\n1\n0\n\n\n1980\nNEW YORK\n6\nJOHN LEBOUTILLIER\n179524\n143676\n1\n0\n0\n\n\n1980\nNEW YORK\n6\nLESTER L WOLFF\n160418\n148638\n0\n1\n0\n\n\n1984\nNEW YORK\n20\nJOSEPH J DIOGUARDI\n106958\n93518\n1\n0\n0\n\n\n1984\nNEW YORK\n20\nOREN J TEICHER\n102842\n102842\n0\n1\n0\n\n\n1986\nNEW YORK\n27\nGEORGE C WORTLEY\n166860\n154350\n1\n0\n0\n\n\n1986\nNEW YORK\n27\nROSEMARY S POOLER\n164982\n162266\n0\n1\n0\n\n\n1992\nCONNECTICUT\n2\nEDWARD W MUNSTER\n119416\n119416\n0\n1\n0\n\n\n1992\nCONNECTICUT\n2\nSAM GEJDENSON\n123291\n83197\n1\n0\n0\n\n\n1992\nNEW YORK\n3\nPETER T KING\n124727\n108574\n1\n0\n0\n\n\n1992\nNEW YORK\n3\nSTEVE A ORLINS\n116915\n116915\n0\n1\n0\n\n\n1994\nNEW YORK\n1\nGEORGE J HOCHBRUECKNER\n160292\n157384\n0\n1\n0\n\n\n1994\nNEW YORK\n1\nMICHAEL P FORBES\n180982\n144090\n1\n0\n0\n\n\n1996\nNEW YORK\n1\nMICHAEL P FORBES\n233240\n180002\n1\n0\n0\n\n\n1996\nNEW YORK\n1\nNORA L BREDES\n192992\n187632\n0\n1\n0\n\n\n1996\nNEW YORK\n30\nFRANCIS J PORDUM\n200080\n195372\n0\n1\n0\n\n\n1996\nNEW YORK\n30\nJACK QUINN\n242738\n194640\n1\n0\n0\n\n\n2000\nCONNECTICUT\n2\nROB SIMMONS\n114380\n110239\n1\n0\n0\n\n\n2000\nCONNECTICUT\n2\nSAM GEJDENSON\n111520\n111520\n0\n1\n0\n\n\n2006\nNEW YORK\n25\nDAN MAFFEI\n214216\n201210\n0\n1\n0\n\n\n2006\nNEW YORK\n25\nJAMES T WALSH\n221050\n182374\n1\n0\n0\n\n\n2006\nNEW YORK\n29\nERIC J MASSA\n200088\n189218\n0\n1\n0\n\n\n2006\nNEW YORK\n29\nJOHN R \"RANDY\" KUHL JR\n212154\n182766\n1\n0\n0\n\n\n2010\nNEW YORK\n13\nMICHAEL E MCMAHON\n60773\n60773\n0\n1\n0\n\n\n2010\nNEW YORK\n13\nMICHAEL G GRIMM\n65024\n55821\n1\n0\n0\n\n\n2010\nNEW YORK\n19\nJOHN J HALL\n98766\n98766\n0\n1\n0\n\n\n2010\nNEW YORK\n19\nNAN HAYMORTH\n109956\n88734\n1\n0\n0\n\n\n2010\nNEW YORK\n24\nMICHAEL A ARCURI\n89809\n89809\n0\n1\n0\n\n\n2010\nNEW YORK\n24\nRICHARD L HANNA\n101599\n85702\n1\n0\n0\n\n\n2010\nNEW YORK\n25\nANN MARIE BUERKLE\n104602\n81380\n1\n0\n0\n\n\n2010\nNEW YORK\n25\nDANIEL B MAFFEI\n103954\n103954\n0\n1\n0\n\n\n2012\nNEW YORK\n27\nCHRIS COLLINS\n322440\n274500\n1\n0\n0\n\n\n2012\nNEW YORK\n27\nKATHLEEN C HOCHUL\n312438\n280016\n0\n1\n0\n\n\n2018\nNEW YORK\n1\nLEE M ZELDIN\n278054\n243124\n1\n0\n0\n\n\n2018\nNEW YORK\n1\nPERRY GERSHON\n255982\n248426\n0\n1\n0\n\n\n2018\nNEW YORK\n24\nDANA BALTER\n246452\n231804\n0\n1\n0\n\n\n2018\nNEW YORK\n24\nJOHN M KATKO\n273840\n227076\n1\n0\n0\n\n\n2018\nNEW YORK\n27\nCHRIS COLLINS\n280292\n229012\n1\n0\n0\n\n\n2018\nNEW YORK\n27\nNATHAN D MCMURRAY\n278118\n256334\n0\n1\n0\n\n\n2022\nNEW YORK\n4\nANTHONY P D’ESPOSITO\n140622\n129353\n1\n0\n0\n\n\n2022\nNEW YORK\n4\nLAURA A GILLEN\n130871\n130871\n0\n1\n0\n\n\n2022\nNEW YORK\n17\nMICHAEL V LAWLER\n287100\n251476\n1\n0\n0\n\n\n2022\nNEW YORK\n17\nSEAN PATRICK MALONEY\n283460\n266914\n0\n1\n0\n\n\n2022\nNEW YORK\n22\nBRANDON M WILLIAMS\n135544\n116529\n1\n0\n0\n\n\n2022\nNEW YORK\n22\nFRANCIS CONOLE\n132913\n132913\n0\n1\n0\n\n\n\n\n\n\n\n\n#display a list of states, years and districts where election results would have been different\n\nhouse_fusion_merged_filtered |&gt;\n  select(year,state,district,is_actual_winner) |&gt;\n  group_by(year,state,district)|&gt;\n  summarise(num_elections=sum(is_actual_winner)) |&gt;\n  ungroup() |&gt;\n  select(-num_elections) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary - Elections With  Different Results In The Absense of Fusion System\"\n  )\n\n\n\n\n\n\n\nSummary - Elections With Different Results In The Absense of Fusion System\n\n\nyear\nstate\ndistrict\n\n\n\n\n1976\nNEW YORK\n29\n\n\n1980\nNEW YORK\n3\n\n\n1980\nNEW YORK\n6\n\n\n1984\nNEW YORK\n20\n\n\n1986\nNEW YORK\n27\n\n\n1992\nCONNECTICUT\n2\n\n\n1992\nNEW YORK\n3\n\n\n1994\nNEW YORK\n1\n\n\n1996\nNEW YORK\n1\n\n\n1996\nNEW YORK\n30\n\n\n2000\nCONNECTICUT\n2\n\n\n2006\nNEW YORK\n25\n\n\n2006\nNEW YORK\n29\n\n\n2010\nNEW YORK\n13\n\n\n2010\nNEW YORK\n19\n\n\n2010\nNEW YORK\n24\n\n\n2010\nNEW YORK\n25\n\n\n2012\nNEW YORK\n27\n\n\n2018\nNEW YORK\n1\n\n\n2018\nNEW YORK\n24\n\n\n2018\nNEW YORK\n27\n\n\n2022\nNEW YORK\n4\n\n\n2022\nNEW YORK\n17\n\n\n2022\nNEW YORK\n22\n\n\n\n\n\n\n\nBased on these results, we can ascertain that the use of fusion system in election barely has any discernible effect on the outcome as the election results would have been different only in a handful of cases.\n\n\nQ3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n# get a df with results of presidential elections by state. \n\npres_df1&lt;- sqldf(\n  \"\n  with a as(\n  select year,\n  state,\n  candidate,\n  party_simplified as party,\n  candidatevotes as votes\n  from pres\n  where 1=1\n  and PARTY_SIMPLIFIED in ('DEMOCRAT','REPUBLICAN')\n  GROUP BY 1,2,3,4\n  )\n  ,\n  b as (\n  select a.*,\n  row_number() over(partition by year,state order by votes desc) as is_winner\n  from a\n  )\n  \n  select year,\n  state,\n  candidate,\n  party,\n  votes,\n  case\n  when is_winner=1 then 1 else 0 end as is_pres_winner\n  from b\n  ;\n  \"\n)\n\n\n# get a df with records for house elections filtered to D and R only\n\nhouse_prim_only&lt;-house |&gt;\n  filter(party=='DEMOCRAT' | party=='REPUBLICAN') |&gt;\n  group_by(year,state,party) |&gt;\n  summarize(total_party_votes=sum(candidatevotes)) |&gt;\n  ungroup()\n\n#determine  max votes by year/state elections\nhouse_prim_only_agg &lt;- house_prim_only %&gt;%\n  group_by(year, state) %&gt;%\n  summarize(max_votes = max(total_party_votes, na.rm = TRUE)) |&gt;\n  ungroup()\n  \n#merge 2 dfs\n\nhouse_prim_only_merged&lt;-left_join(house_prim_only,\n                                  house_prim_only_agg,\n                                  by=c(\"state\"=\"state\",\"year\"=\"year\"))\n#identify a winning party\n\nhouse_prim_only_merged2&lt;-house_prim_only_merged |&gt;\n  mutate(house_party_winner=case_when(total_party_votes==max_votes ~1,\n                             TRUE ~0))\n\n\n#join dfs with house and presidential results \n\nhouse_pres_merged&lt;-inner_join(pres_df1,\n                              house_prim_only_merged2,\n                              by=c(\"year\"=\"year\",\n                                   \"state\"=\"state\",\n                                   \"party\"=\"party\"))\n#rename columns for clarity\n\nhouse_pres_merged_for_plot&lt;-house_pres_merged |&gt;\n  select(year,state,candidate,party,votes,is_pres_winner,total_party_votes,house_party_winner) |&gt;\n  rename(presidential_votes=votes,\n         is_party_winner=house_party_winner\n         ) |&gt;\n  mutate(president_more_votes=case_when(presidential_votes&gt;total_party_votes~1,\n                                        TRUE~0)) |&gt;\n  mutate(delta_votes=presidential_votes-total_party_votes)\n\n\n#create a df for D party\ndem_party&lt;-house_pres_merged_for_plot |&gt;\n  filter(party=='DEMOCRAT') |&gt;\n  group_by(state) |&gt;\n  summarize(D_president_more_popular_than_cogress=sum(president_more_votes),\n            D_pct_of_all_elections=D_president_more_popular_than_cogress/12,\n            D_avg_difference_votes=mean(delta_votes)\n            ) |&gt;\n  ungroup() |&gt;\n  mutate(D_avg_difference_rank=rank(-D_avg_difference_votes,ties.method='first'))\n  \n#create a df for R party\nrep_party&lt;-house_pres_merged_for_plot |&gt;\n  filter(party=='REPUBLICAN') |&gt;\n  group_by(state) |&gt;\n  summarize(R_president_more_popular_than_cogress=sum(president_more_votes),\n            R_pct_of_all_elections=R_president_more_popular_than_cogress/12,\n            R_avg_difference_votes=mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(R_avg_difference_rank=rank(-R_avg_difference_votes,ties.method='first'))\n            \n            \n\n#join 2 dfs\ndem_rep_df&lt;-inner_join(dem_party,rep_party,by=c(\"state\"=\"state\"))\n\n#display\ndem_rep_df |&gt;\n  DT::datatable(options = list(\n    pageLength = 51\n  ))\n\n\n\n\n\nLooking at Democratic party, we"
  },
  {
    "objectID": "no_data_load_mp03.html",
    "href": "no_data_load_mp03.html",
    "title": "Mini-Project 03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "In this mini-project, we will investigate the claim that the US Electoral College systematically biases election results away from the popular vote.\n\nData Prep\nFirst, we will obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\n\n\n\n\nTask 1.Download Congressional Shapefiles 1976-2012\n\n\nShow the code\ntd &lt;- tempdir()\n\nfor (i in 94:112) {\n  fname &lt;- paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n\n  if (!file.exists(fname)) {\n    url &lt;- paste0(\"https://cdmaps.polisci.ucla.edu/shp/\", fname)\n\n    download.file(url, destfile = fname)\n\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n\n    assign(paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\n\n\nTask 2.Download Congressional Shapefiles 2014-2022\n\n\nShow the code\nfor (i in 2014:2022) {\n  BASE_URL &lt;- \"https://www2.census.gov/geo/tiger/\"\n  if (i &gt;= 2018) {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd116.zip\")\n  } else if (i &gt; 2015) {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd115.zip\")\n  } else {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd114.zip\")\n  }\n  download_name &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \".zip\")\n\n  if (!file.exists(download_name)) {\n    FILE_URL &lt;- paste0(BASE_URL, file)\n    print(FILE_URL)\n    download.file(FILE_URL, destfile = download_name, mode = \"wb\")\n  }\n}\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\n\nQ1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\n\n\nShow the code\n## read in presidential elections data\n\npres &lt;- read.csv(\"president_1976_2020.csv\")\n# head(pres)\n\n# read in house election vote data\n\nhouse &lt;- read.csv(\"house_1976_2022.csv\")\n# head(house)\n\n## create a df with house results for 1976 and 2022\n\nhouse_1976_and_2022 &lt;- sqldf(\n  \"\n   with h76 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=1976\n   group by 1\n   )\n   ,\n   h22 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2022\n   group by 1\n   )\n   ,\n   base as (\n   select h76.state,\n   h76.num_seats,\n   case\n   when h76.num_seats=0 then 1\n   else h76.num_seats\n   end as number_of_seats_1976,\n   h22.num_seats,\n   case\n   when h22.num_seats=0 then 1\n   else h22.num_seats\n   end as number_of_seats_2022\n   from h76\n   left join h22\n   on h76.state=h22.state\n   )\n\n   select state,\n   number_of_seats_1976,\n   number_of_seats_2022,\n   number_of_seats_2022 - number_of_seats_1976 as delta\n   from base\n    ;\n  \"\n)\n\n\n# display states with largest gains in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n  slice_max(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Gains in House Seats\"\n  )\n\n\n\n\n\n\n\n\nTop 5 States with Largest Gains in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nTEXAS\n24\n38\n14\n\n\nFLORIDA\n15\n28\n13\n\n\nCALIFORNIA\n43\n52\n9\n\n\nARIZONA\n4\n9\n5\n\n\nGEORGIA\n10\n14\n4\n\n\n\n\n\n\n\nTexas, Florida, California, Arizona and Georgia had largest gains in house seats between 1976 and 2022.\n\n\nShow the code\n# display states with largest losses in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n  slice_min(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Losses in House Seats\"\n  )\n\n\n\n\n\n\n\n\nTop 5 States with Largest Losses in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nNEW YORK\n39\n26\n-13\n\n\nOHIO\n23\n15\n-8\n\n\nPENNSYLVANIA\n25\n17\n-8\n\n\nILLINOIS\n24\n17\n-7\n\n\nMICHIGAN\n19\n13\n-6\n\n\n\n\n\n\n\nNew York, Ohio, Pennsylvania, Illinois and Michigan had largest losses in house seats between 1976 and 2022.\n\nQ2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\nFirst, let’s find states, years, and districts where fusion system was used in elections.\n\n\nShow the code\n# get a count of political parties candidates received votes from\nhouse_temp1 &lt;- house %&gt;%\n  group_by(state, year, district, candidate) |&gt;\n  summarize(distinct_party_count = n_distinct(party))\n\n# get a list of districts where candidates received votes from more than 1 party\nhouse_temp2 &lt;- house_temp1 |&gt;\n  filter(distinct_party_count &gt; 1)\n\n# get a list of all years, states and districts where fusion system was used\nhouse_temp3 &lt;- house_temp2 |&gt;\n  group_by(state, year, district)\n\n# subset house df to only include results from states, years and districts meeting the criteria\n\nhouse_fusion1 &lt;- inner_join(house, house_temp3, by = c(\"state\" = \"state\", \"year\" = \"year\", \"district\" = \"district\"))\n\n# get elections totals for each candidate in states/years/districts meeting the criteria\nhouse_fusion1_actuals &lt;- house_fusion1 |&gt;\n  group_by(year, state, district, candidate.x) |&gt;\n  summarise(actual_total_votes = sum(candidatevotes)) |&gt;\n  mutate(max_votes = max(actual_total_votes)) |&gt;\n  mutate(is_actual_winner = case_when(\n    actual_total_votes == max_votes ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  ungroup()\n\n# get candidate votes from their primary party only and determine a winner\nhouse_fusion1_primaryonly &lt;- house_fusion1 |&gt;\n  filter(party == \"DEMOCRAT\" | party == \"REPUBLICAN\") |&gt;\n  group_by(year, state, district, candidate.x) |&gt;\n  summarise(actual_primaryparty_votes = sum(candidatevotes)) |&gt;\n  mutate(max_primary_votes = max(actual_primaryparty_votes)) |&gt;\n  mutate(is_primaryvotesonly_winner = case_when(\n    actual_primaryparty_votes == max_primary_votes ~ 1,\n    TRUE ~ 0\n  ))\n\n# merge 2 datasets\nhouse_fusion_merged &lt;- left_join(house_fusion1_actuals,\n  house_fusion1_primaryonly,\n  by = c(\n    \"state\" = \"state\",\n    \"year\" = \"year\",\n    \"district\" = \"district\",\n    \"candidate.x\" = \"candidate.x\"\n  )\n)\n# filter for records where primary party is either D or R and determine if results would have been different\nhouse_fusion_merged_filtered &lt;- house_fusion_merged |&gt;\n  select(year, state, district, candidate.x, actual_total_votes, actual_primaryparty_votes, is_actual_winner, is_primaryvotesonly_winner) |&gt;\n  mutate(same_winner = case_when(\n    is_actual_winner == is_primaryvotesonly_winner ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  filter((same_winner == 0) & !is.na(actual_primaryparty_votes))\n\n# display results\n# house_fusion_merged_filtered |&gt;\n#  rename(candidate=candidate.x) |&gt;\n#   gt() |&gt;\n#  tab_header(\n#    title = \"Elections With  Different Results In The Absense of Fusion System\"\n#  )\n\n\n# display a list of states, years and districts where election results would have been different\n\nhouse_fusion_merged_filtered |&gt;\n  select(year, state, district, is_actual_winner) |&gt;\n  group_by(year, state, district) |&gt;\n  summarise(num_elections = sum(is_actual_winner)) |&gt;\n  ungroup() |&gt;\n  select(-num_elections) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"List of Elections With  Different Results\"\n  )\n\n\n\n\n\n\n\n\nList of Elections With Different Results\n\n\nyear\nstate\ndistrict\n\n\n\n\n1976\nNEW YORK\n29\n\n\n1980\nNEW YORK\n3\n\n\n1980\nNEW YORK\n6\n\n\n1984\nNEW YORK\n20\n\n\n1986\nNEW YORK\n27\n\n\n1992\nCONNECTICUT\n2\n\n\n1992\nNEW YORK\n3\n\n\n1994\nNEW YORK\n1\n\n\n1996\nNEW YORK\n1\n\n\n1996\nNEW YORK\n30\n\n\n2000\nCONNECTICUT\n2\n\n\n2006\nNEW YORK\n25\n\n\n2006\nNEW YORK\n29\n\n\n2010\nNEW YORK\n13\n\n\n2010\nNEW YORK\n19\n\n\n2010\nNEW YORK\n24\n\n\n2010\nNEW YORK\n25\n\n\n2012\nNEW YORK\n27\n\n\n2018\nNEW YORK\n1\n\n\n2018\nNEW YORK\n24\n\n\n2018\nNEW YORK\n27\n\n\n2022\nNEW YORK\n4\n\n\n2022\nNEW YORK\n17\n\n\n2022\nNEW YORK\n22\n\n\n\n\n\n\n\n\n\nShow the code\ncnt_fusion_elections &lt;- sqldf(\n  \"\n  select 'all fusion elections' as elections,\n  count(distinct year||state||district) as count_elections\n  from house_fusion_merged\n  group by 1\n\n  union all\n\n  select 'fusion elections with different outcomes' as elections,\n  count(distinct year||state||district) as count_elections\n  from house_fusion_merged\n  where 1=1\n  and is_actual_winner!=is_primaryvotesonly_winner\n  group by 1\n  ;\n  \"\n)\n\ncnt_fusion_elections |&gt;\n  gt()\n\n\n\n\n\n\n\n\nelections\ncount_elections\n\n\n\n\nall fusion elections\n754\n\n\nfusion elections with different outcomes\n24\n\n\n\n\n\n\n\nBased on these results, we ascertain that the use of fusion system in elections has no discernible effect on the outcome as the results would have been different only in a handful of cases (24 out of 754 elections, or 3.2%).\n\nQ3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\nTo answer these questions, first we need to get results of Presidential and Congressional elections for every state for 12 years when Presidential elections were held. (You can look up results of individual states in the table below.)\n\n\nShow the code\n# get a df with results of presidential elections by state filtered to R and D candidates only\n\npres_df1 &lt;- sqldf(\n  \"\n  with a as(\n  select year,\n  state,\n  candidate,\n  party_simplified as party,\n  candidatevotes as votes\n  from pres\n  where 1=1\n  and PARTY_SIMPLIFIED in ('DEMOCRAT','REPUBLICAN')\n  GROUP BY 1,2,3,4\n  )\n  ,\n  b as (\n  select a.*,\n  row_number() over(partition by year,state order by votes desc) as is_winner\n  from a\n  )\n\n  select year,\n  state,\n  candidate,\n  party,\n  votes,\n  case\n  when is_winner=1 then 1 else 0 end as is_pres_winner\n  from b\n  ;\n  \"\n)\n\n\n# get a df with records for house elections filtered to D and R only\n\nhouse_prim_only &lt;- house |&gt;\n  filter(party == \"DEMOCRAT\" | party == \"REPUBLICAN\") |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_party_votes = sum(candidatevotes)) |&gt;\n  ungroup()\n\n# determine  max votes by year/state elections\nhouse_prim_only_agg &lt;- house_prim_only %&gt;%\n  group_by(year, state) %&gt;%\n  summarize(max_votes = max(total_party_votes, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# merge 2 dfs\n\nhouse_prim_only_merged &lt;- left_join(house_prim_only,\n  house_prim_only_agg,\n  by = c(\"state\" = \"state\", \"year\" = \"year\")\n)\n\n# identify a winning party in congressional election\n\nhouse_prim_only_merged2 &lt;- house_prim_only_merged |&gt;\n  mutate(house_party_winner = case_when(\n    total_party_votes == max_votes ~ 1,\n    TRUE ~ 0\n  ))\n\n\n# join dfs with house and presidential results\n\nhouse_pres_merged &lt;- inner_join(pres_df1,\n  house_prim_only_merged2,\n  by = c(\n    \"year\" = \"year\",\n    \"state\" = \"state\",\n    \"party\" = \"party\"\n  )\n)\n# rename columns for clarity\n\nhouse_pres_merged_for_plot &lt;- house_pres_merged |&gt;\n  select(year, state, candidate, party, votes, is_pres_winner, total_party_votes, house_party_winner) |&gt;\n  rename(\n    presidential_votes = votes,\n    is_party_winner = house_party_winner\n  ) |&gt;\n  mutate(president_more_votes = case_when(\n    presidential_votes &gt; total_party_votes ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  mutate(delta_votes = presidential_votes - total_party_votes)\n\n# create a df for D party\ndem_party &lt;- house_pres_merged_for_plot |&gt;\n  filter(party == \"DEMOCRAT\") |&gt;\n  group_by(state) |&gt;\n  summarize(\n    D_president_more_popular_than_cogress = sum(president_more_votes),\n    D_president_more_popular_share = D_president_more_popular_than_cogress / 12,\n    D_avg_difference_votes = mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(D_avg_delta_votes_ranked = rank(-D_avg_difference_votes, ties.method = \"first\"))\n\n# create a df for R party\nrep_party &lt;- house_pres_merged_for_plot |&gt;\n  filter(party == \"REPUBLICAN\") |&gt;\n  group_by(state) |&gt;\n  summarize(\n    R_president_more_popular_than_cogress = sum(president_more_votes),\n    R_president_more_popular_share = R_president_more_popular_than_cogress / 12,\n    R_avg_difference_votes = mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(R_avg_delta_votes_ranked = rank(-R_avg_difference_votes, ties.method = \"first\"))\n\n\n\n# join 2 dfs\ndem_rep_df &lt;- inner_join(dem_party, rep_party, by = c(\"state\" = \"state\"))\n\n# display the data\ndem_rep_df |&gt;\n  mutate(\n    R_president_more_popular_share = scales::percent(R_president_more_popular_share),\n    D_president_more_popular_share = scales::percent(D_president_more_popular_share),\n    R_avg_difference_votes = format(round(as.numeric(R_avg_difference_votes), 0), nsmall = 0, big.mark = \",\"),\n    D_avg_difference_votes = format(round(as.numeric(D_avg_difference_votes), 0), nsmall = 0, big.mark = \",\")\n  ) |&gt;\n  DT::datatable(\n    options = list(pageLength = 5),\n    filter = \"top\"\n  )\n\n\n\n\n\n\nNow that we have records with Presidential and Congress election results for all states, we can answer this question. Across all states in 12 elections, Democratic candidates had more votes than all Congressional candidates from Democratic party in 53% of all cases. For Republican candidates, this number was even higher at 65%.\n\n\nShow the code\ndem_rep_df %&gt;%\n  summarise(D_president_more_popular = mean(D_president_more_popular_share, na.rm = TRUE), R_president_more_popular = mean(R_president_more_popular_share, na.rm = TRUE)) %&gt;%\n  mutate(\n    D_president_more_popular = scales::percent(D_president_more_popular),\n    R_president_more_popular = scales::percent(R_president_more_popular)\n  ) |&gt;\n  ungroup()\n\n\n# A tibble: 1 × 2\n  D_president_more_popular R_president_more_popular\n  &lt;chr&gt;                    &lt;chr&gt;                   \n1 53%                      65%                     \n\n\n\n\nShow the code\n## display df by state\n\n\n# display the data\ndem_rep_df_plot &lt;- dem_rep_df |&gt;\n  select(state, R_president_more_popular_share, D_president_more_popular_share) |&gt;\n  mutate(\n    D_president_more_popular_share = scales::percent(D_president_more_popular_share),\n    R_president_more_popular_share = scales::percent(R_president_more_popular_share)\n  )\n\ndem_rep_df_plot |&gt;\n  gt()\n\n\n\n\n\n\n\n\nstate\nR_president_more_popular_share\nD_president_more_popular_share\n\n\n\n\nALABAMA\n91.7%\n58.3%\n\n\nALASKA\n25.0%\n58.3%\n\n\nARIZONA\n50.0%\n66.7%\n\n\nARKANSAS\n66.7%\n75.0%\n\n\nCALIFORNIA\n75.0%\n58.3%\n\n\nCOLORADO\n58.3%\n75.0%\n\n\nCONNECTICUT\n75.0%\n66.7%\n\n\nDELAWARE\n41.7%\n75.0%\n\n\nFLORIDA\n75.0%\n83.3%\n\n\nGEORGIA\n50.0%\n58.3%\n\n\nHAWAII\n83.3%\n33.3%\n\n\nIDAHO\n66.7%\n25.0%\n\n\nILLINOIS\n58.3%\n66.7%\n\n\nINDIANA\n75.0%\n16.7%\n\n\nIOWA\n41.7%\n66.7%\n\n\nKANSAS\n33.3%\n66.7%\n\n\nKENTUCKY\n66.7%\n66.7%\n\n\nLOUISIANA\n91.7%\n100.0%\n\n\nMAINE\n50.0%\n25.0%\n\n\nMARYLAND\n58.3%\n58.3%\n\n\nMASSACHUSETTS\n83.3%\n0.0%\n\n\nMICHIGAN\n75.0%\n50.0%\n\n\nMINNESOTA\n66.7%\n41.7%\n\n\nMISSISSIPPI\n83.3%\n50.0%\n\n\nMISSOURI\n75.0%\n41.7%\n\n\nMONTANA\n58.3%\n16.7%\n\n\nNEBRASKA\n0.0%\n83.3%\n\n\nNEVADA\n66.7%\n58.3%\n\n\nNEW HAMPSHIRE\n66.7%\n66.7%\n\n\nNEW JERSEY\n75.0%\n66.7%\n\n\nNEW MEXICO\n50.0%\n50.0%\n\n\nNEW YORK\n83.3%\n91.7%\n\n\nNORTH CAROLINA\n75.0%\n25.0%\n\n\nNORTH DAKOTA\n75.0%\n16.7%\n\n\nOHIO\n66.7%\n66.7%\n\n\nOKLAHOMA\n83.3%\n58.3%\n\n\nOREGON\n75.0%\n25.0%\n\n\nPENNSYLVANIA\n58.3%\n66.7%\n\n\nRHODE ISLAND\n58.3%\n41.7%\n\n\nSOUTH CAROLINA\n66.7%\n66.7%\n\n\nSOUTH DAKOTA\n58.3%\n25.0%\n\n\nTENNESSEE\n91.7%\n75.0%\n\n\nTEXAS\n66.7%\n50.0%\n\n\nUTAH\n66.7%\n8.3%\n\n\nVERMONT\n50.0%\n66.7%\n\n\nVIRGINIA\n58.3%\n91.7%\n\n\nWASHINGTON\n66.7%\n41.7%\n\n\nWEST VIRGINIA\n100.0%\n0.0%\n\n\nWISCONSIN\n58.3%\n66.7%\n\n\nWYOMING\n58.3%\n33.3%\n\n\n\n\n\n\n\n\n\nShow the code\n# plot states data\n\ndem_rep_plot1 &lt;- plot_ly(\n  data = dem_rep_df,\n  x = ~R_president_more_popular_share,\n  y = ~D_president_more_popular_share,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(size = 5),\n  color = ~state\n) |&gt;\n  layout(\n    title = \"\",\n    xaxis = list(\n      title = \"Popularity of R president vs Congress\",\n      font = 8,\n      tickfont = list(size = 8),\n      titlefont = list(size = 8)\n    ),\n    yaxis = list(\n      title = \"Popularity of D president vs Congress\",\n      font = 8,\n      tickfont = list(size = 8),\n      titlefont = list(size = 8)\n    ),\n    legend = list(\n      font = list(size = 8),\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2,\n      font = 8\n    )\n  )\n\ndem_rep_plot1\n\n\n\n\n\n\nConsistently with our previous findings, most states tend to favor presidential candidates more so than Congressional candidates, with this trend being more pronounced for Republican candidates. Notable exceptions are Nebraska, Alaska, Kansas, Delaware and Iowa, where Republican presidential candidates tend to receive fewer votes than Congressional hopefuls.\n\n\nShow the code\ndem_win_year &lt;- house_pres_merged_for_plot |&gt;\n  filter((party == \"DEMOCRAT\") & (president_more_votes == 1)) |&gt;\n  group_by(year, party) |&gt;\n  summarise(share_states_president_popular = n() / 51) |&gt;\n  ungroup()\n\nrep_win_year &lt;- house_pres_merged_for_plot |&gt;\n  filter((party == \"REPUBLICAN\") & (president_more_votes == 1)) |&gt;\n  group_by(year, party) |&gt;\n  summarise(share_states_president_popular = n() / 51) |&gt;\n  ungroup()\n\ndem_rep_win_for_plot &lt;- bind_rows(dem_win_year, rep_win_year)\n\ndem_rep_win_for_plot2 &lt;- dem_rep_win_for_plot |&gt;\n  ungroup()\n\n\n\n\nShow the code\n# create a chart\n\ndrplot2 &lt;- plot_ly(\n  data = dem_rep_win_for_plot2,\n  x = ~year, y = ~share_states_president_popular,\n  color = ~party,\n  type = \"scatter\",\n  mode = \"lines+markers\"\n) |&gt;\n  layout(\n    title = \"Share of States Favoring Presidential Candidates over Congressional\",\n    xaxis = list(title = \"\"),\n    yaxis = list(\n      title = \"Share of States\",\n      tickformat = \".0%\",\n      range = c(0, 1)\n    ),\n    legend = list(\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2, # Position below the plot\n      font = list(size = 8) # Smaller font size\n    )\n  )\n\ndrplot2\n\n\n\n\n\n\nAs we can see on this chart, in Democratic party, presidential candidates are slowly but surely becoming more popular than their Congressional peers over last 30 years, changes in preferences of Republican electorate are more drastic.\n\n\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\n\nUsing the data you downloaded earlier, create a chloropleth visualization of the electoral college results for the 2000 presidential election (Bush vs. Gore), coloring each state by the party that won the most votes in that state.\n\n\n\nShow the code\n# create a df with presidential election data\n\npres2000 &lt;- sqldf(\n  \"\n  with a as(\n  select year,state,state_po,\n  sum(case when party_simplified='DEMOCRAT' THEN candidatevotes else 0 end) as dem_votes,\n  sum(case when party_simplified='REPUBLICAN' THEN candidatevotes else 0 end) as rep_votes\n  from pres\n  where 1=1\n  and year=2000\n  and party_simplified in ('DEMOCRAT','REPUBLICAN')\n  group by 1,2,3\n  )\n  select year,\n  state_po,\n  dem_votes,\n  rep_votes,\n  case\n  when dem_votes&gt;rep_votes then 'Democrat' else 'Republican' end as party_won\n  from a\n  ;\n  \"\n)\n\n# create a df with ecv data\n\nhouse2000 &lt;- sqldf(\n  \"\n   with h20 as (\n   select\n   state_po,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2000\n   group by 1\n   )\n  ,\n  base as (\n   select\n   state_po,\n   case\n   when num_seats=0 then 1\n   else num_seats\n   end as number_of_seats2000\n   from h20\n  )\n  select\n  state_po,\n  number_of_seats2000+2 as ecv\n  from base\n    ;\n  \"\n)\n\n# join 2 dfs\necv20_df &lt;- inner_join(house2000, pres2000, by = c(\"state_po\" = \"state_po\"))\n\n## -get a shapefile of us states\n\ntd &lt;- tempdir()\nzip_contents_task5 &lt;- unzip(\"tl_2020_us_state.zip\",\n  exdir = td\n)\n\nfname_shp_task5 &lt;- zip_contents_task5[grepl(\"shp$\", zip_contents_task5)]\ncongress2020_sf &lt;- read_sf(fname_shp_task5)\n\n# subset data - we only need state abbreviations and coordinates\nstate_map1 &lt;- congress2020_sf |&gt;\n  select(STUSPS, geometry)\n\n# create a df with state boundaries and election data\nel20_for_map &lt;- inner_join(state_map1, ecv20_df, by = c(\"STUSPS\" = \"state_po\"))\n\n\n# create a plot for 2000 presidential election\nggplot(\n  el20_for_map,\n  aes(\n    geometry = geometry,\n    fill = party_won\n  )\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\n    \"Democrat\" = \"blue\",\n    \"Republican\" = \"red\"\n  ))\n\n\n\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results\n\nModify your previous code to make either a faceted version showing election results over time.\n\n\n\nShow the code\n# create a df with presidential election data\n\npres_elections_temp &lt;- pres |&gt;\n  filter(party_simplified == \"DEMOCRAT\" | party_simplified == \"REPUBLICAN\") |&gt;\n  group_by(year, state_po, party_simplified) |&gt;\n  summarize(votes = sum(candidatevotes))\n\n# head(pres_elections_temp)\n\npres_elections_temp2 &lt;- pivot_wider(pres_elections_temp,\n  # id_cols=(year,state_po),\n  names_from = party_simplified,\n  values_from = votes\n)\n\npres_elections_temp2 &lt;- pres_elections_temp2 |&gt;\n  mutate(party_won = case_when(DEMOCRAT &gt; REPUBLICAN ~ \"Democrat\", TRUE ~ \"Republican\"))\n\n\n# create a df with state boundaries and election data\nel20_for_map_overtime &lt;- inner_join(state_map1, pres_elections_temp2, by = c(\"STUSPS\" = \"state_po\"))\n\n\n\n# create a plot for 2000 presidential election\n\np1 &lt;- ggplot(\n  el20_for_map_overtime,\n  aes(\n    geometry = geometry,\n    fill = party_won\n  )\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\n    \"Democrat\" = \"blue\",\n    \"Republican\" = \"red\"\n  )) +\n  facet_wrap(~ factor(year), labeller = as_labeller(function(x) paste(\"year:\", x)), ncol = 1) +\n  theme(\n    strip.text = element_text(size = 24),\n    legend.text = element_text(size = 24), # Increase legend label text size\n    legend.title = element_text(size = 24), # Increase legend title text size\n    legend.key.size = unit(3, \"lines\")\n  )\n\np1\n\n\n\n\n\n\n\n\n\n\nTask 7: Evaluating Fairness of ECV Allocation Schemes\n\nWrite a fact check evaluating the fairness of the different ECV electoral allocation schemes. To do so, you should first determine which allocation scheme you consider “fairest”. You should then see which schemes give different results, if they ever do. To make your fact check more compelling, select one election where the ECV scheme had the largest impact–if one exists–and explain how the results would have been different under a different ECV scheme. As you perform your analysis, you may assume that the District of Columbia has three ECVs, which are allocated to the Democratic candidate under all schemes except possibly national popular vote."
  },
  {
    "objectID": "mp3.html",
    "href": "mp3.html",
    "title": "Mini-Project 03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "In this paper, we will investigate the claim that the US Electoral College systematically biases election results away from the popular vote.\n\nData Prep\nIn this section, we obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\nQ1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n## read in presidential elections data\n\npres&lt;-read.csv(\"president_1976_2020.csv\")\n#head(pres)\n\n# read in house election vote data \n\nhouse&lt;-read.csv(\"house_1976_2022.csv\")\n#head(house)\n\n\n## create a df with house results for 1976 and 2022\n\nhouse_1976_and_2022 &lt;- sqldf(\n  \"\n   with h76 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=1976\n   group by 1\n   )\n   ,\n   h22 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2022\n   group by 1\n   )\n   , \n   base as (\n   select h76.state,\n   h76.num_seats,\n   case\n   when h76.num_seats=0 then 1 \n   else h76.num_seats\n   end as number_of_seats_1976,\n   h22.num_seats,\n   case\n   when h22.num_seats=0 then 1 \n   else h22.num_seats\n   end as number_of_seats_2022\n   from h76\n   left join h22\n   on h76.state=h22.state\n   )\n   \n   select state,\n   number_of_seats_1976,\n   number_of_seats_2022,\n   number_of_seats_2022 - number_of_seats_1976 as delta \n   from base \n    ;\n  \"\n)\n\n\n# display states with largest gains in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n   slice_max(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Gains in House Seats\"\n  )\n\n\n\n\n\n\n\nTop 5 States with Largest Gains in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nTEXAS\n24\n38\n14\n\n\nFLORIDA\n15\n28\n13\n\n\nCALIFORNIA\n43\n52\n9\n\n\nARIZONA\n4\n9\n5\n\n\nGEORGIA\n10\n14\n4\n\n\n\n\n\n\n\n\n# display states with largest losses in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n   slice_min(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Losses in House Seats\"\n  )\n\n\n\n\n\n\n\nTop 5 States with Largest Losses in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nNEW YORK\n39\n26\n-13\n\n\nOHIO\n23\n15\n-8\n\n\nPENNSYLVANIA\n25\n17\n-8\n\n\nILLINOIS\n24\n17\n-7\n\n\nMICHIGAN\n19\n13\n-6\n\n\n\n\n\n\n\n\n\nQ2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\nFirst, let’s find states, years, and districts where fusion system was in place\n\n#get a count of political parties candidates received votes from\nhouse_temp1 &lt;- house %&gt;%\n  group_by(state, year, district, candidate) |&gt;\n  summarize(distinct_party_count = n_distinct(party))\n\n#get a list of districts where candidates received votes from more than 1 party\nhouse_temp2&lt;-house_temp1 |&gt;\n  filter(distinct_party_count&gt;1)\n\n#get a list of all years, states and districts where fusion system was used\nhouse_temp3&lt;- house_temp2 |&gt;\n  group_by(state, year, district)\n\n# subset house df to only include results from states, years and districts meeting the criteria \n\nhouse_fusion1 &lt;- inner_join(house,house_temp3, by=c(\"state\"=\"state\",\"year\"=\"year\",\"district\"=\"district\"))\n\n# get elections totals for each candidate in states/years/districts meeting the criteria\nhouse_fusion1_actuals&lt;-house_fusion1 |&gt;\n  group_by(year,state,district,candidate.x) |&gt;\n  summarise(actual_total_votes=sum(candidatevotes)) |&gt;\n  mutate(max_votes=max(actual_total_votes)) |&gt;\n  mutate(is_actual_winner=case_when(actual_total_votes==max_votes ~1,\n                             TRUE ~0)) |&gt;\n  ungroup()\n\n# get candidate votes from their primary party only and determine a winner\n house_fusion1_primaryonly&lt;-house_fusion1 |&gt;\n  filter(party=='DEMOCRAT' | party=='REPUBLICAN') |&gt;\n  group_by(year,state,district,candidate.x) |&gt;\n  summarise(actual_primaryparty_votes=sum(candidatevotes)) |&gt;\n  mutate(max_primary_votes=max(actual_primaryparty_votes)) |&gt;\n  mutate(is_primaryvotesonly_winner=case_when(actual_primaryparty_votes==max_primary_votes ~1,\n                             TRUE ~0)) \n\n#merge 2 datasets\nhouse_fusion_merged&lt;-left_join(house_fusion1_actuals,\n                               house_fusion1_primaryonly,\n                               by=c(\"state\"=\"state\",\n                                    \"year\"=\"year\",\n                                    \"district\"=\"district\",\n                                    \"candidate.x\"=\"candidate.x\")\n                               )\n#filter for records where primary party is either D or R and determine if results would have been different\nhouse_fusion_merged_filtered&lt;-house_fusion_merged |&gt;\n  select(year,state,district,candidate.x,actual_total_votes,actual_primaryparty_votes,is_actual_winner,is_primaryvotesonly_winner) |&gt;\n  mutate(same_winner=case_when(is_actual_winner==is_primaryvotesonly_winner ~1,\n                             TRUE ~0)) |&gt;\n  filter((same_winner==0) & !is.na(actual_primaryparty_votes))\n  \n#display results\nhouse_fusion_merged_filtered |&gt;\n  rename(candidate=candidate.x) |&gt;\n   gt() |&gt;\n  tab_header(\n    title = \"Elections With  Different Results In The Absense of Fusion System\"\n  )\n\n\n\n\n\n\n\nElections With Different Results In The Absense of Fusion System\n\n\nyear\nstate\ndistrict\ncandidate\nactual_total_votes\nactual_primaryparty_votes\nis_actual_winner\nis_primaryvotesonly_winner\nsame_winner\n\n\n\n\n1976\nNEW YORK\n29\nEDWARD W PATTISON\n100663\n95361\n1\n0\n0\n\n\n1976\nNEW YORK\n29\nJOSEPH A MARTINO\n96476\n96476\n0\n1\n0\n\n\n1980\nNEW YORK\n3\nGREGORY W CARMAN\n175904\n149736\n1\n0\n0\n\n\n1980\nNEW YORK\n3\nJEROME A AMBRO JR\n166778\n150778\n0\n1\n0\n\n\n1980\nNEW YORK\n6\nJOHN LEBOUTILLIER\n179524\n143676\n1\n0\n0\n\n\n1980\nNEW YORK\n6\nLESTER L WOLFF\n160418\n148638\n0\n1\n0\n\n\n1984\nNEW YORK\n20\nJOSEPH J DIOGUARDI\n106958\n93518\n1\n0\n0\n\n\n1984\nNEW YORK\n20\nOREN J TEICHER\n102842\n102842\n0\n1\n0\n\n\n1986\nNEW YORK\n27\nGEORGE C WORTLEY\n166860\n154350\n1\n0\n0\n\n\n1986\nNEW YORK\n27\nROSEMARY S POOLER\n164982\n162266\n0\n1\n0\n\n\n1992\nCONNECTICUT\n2\nEDWARD W MUNSTER\n119416\n119416\n0\n1\n0\n\n\n1992\nCONNECTICUT\n2\nSAM GEJDENSON\n123291\n83197\n1\n0\n0\n\n\n1992\nNEW YORK\n3\nPETER T KING\n124727\n108574\n1\n0\n0\n\n\n1992\nNEW YORK\n3\nSTEVE A ORLINS\n116915\n116915\n0\n1\n0\n\n\n1994\nNEW YORK\n1\nGEORGE J HOCHBRUECKNER\n160292\n157384\n0\n1\n0\n\n\n1994\nNEW YORK\n1\nMICHAEL P FORBES\n180982\n144090\n1\n0\n0\n\n\n1996\nNEW YORK\n1\nMICHAEL P FORBES\n233240\n180002\n1\n0\n0\n\n\n1996\nNEW YORK\n1\nNORA L BREDES\n192992\n187632\n0\n1\n0\n\n\n1996\nNEW YORK\n30\nFRANCIS J PORDUM\n200080\n195372\n0\n1\n0\n\n\n1996\nNEW YORK\n30\nJACK QUINN\n242738\n194640\n1\n0\n0\n\n\n2000\nCONNECTICUT\n2\nROB SIMMONS\n114380\n110239\n1\n0\n0\n\n\n2000\nCONNECTICUT\n2\nSAM GEJDENSON\n111520\n111520\n0\n1\n0\n\n\n2006\nNEW YORK\n25\nDAN MAFFEI\n214216\n201210\n0\n1\n0\n\n\n2006\nNEW YORK\n25\nJAMES T WALSH\n221050\n182374\n1\n0\n0\n\n\n2006\nNEW YORK\n29\nERIC J MASSA\n200088\n189218\n0\n1\n0\n\n\n2006\nNEW YORK\n29\nJOHN R \"RANDY\" KUHL JR\n212154\n182766\n1\n0\n0\n\n\n2010\nNEW YORK\n13\nMICHAEL E MCMAHON\n60773\n60773\n0\n1\n0\n\n\n2010\nNEW YORK\n13\nMICHAEL G GRIMM\n65024\n55821\n1\n0\n0\n\n\n2010\nNEW YORK\n19\nJOHN J HALL\n98766\n98766\n0\n1\n0\n\n\n2010\nNEW YORK\n19\nNAN HAYMORTH\n109956\n88734\n1\n0\n0\n\n\n2010\nNEW YORK\n24\nMICHAEL A ARCURI\n89809\n89809\n0\n1\n0\n\n\n2010\nNEW YORK\n24\nRICHARD L HANNA\n101599\n85702\n1\n0\n0\n\n\n2010\nNEW YORK\n25\nANN MARIE BUERKLE\n104602\n81380\n1\n0\n0\n\n\n2010\nNEW YORK\n25\nDANIEL B MAFFEI\n103954\n103954\n0\n1\n0\n\n\n2012\nNEW YORK\n27\nCHRIS COLLINS\n322440\n274500\n1\n0\n0\n\n\n2012\nNEW YORK\n27\nKATHLEEN C HOCHUL\n312438\n280016\n0\n1\n0\n\n\n2018\nNEW YORK\n1\nLEE M ZELDIN\n278054\n243124\n1\n0\n0\n\n\n2018\nNEW YORK\n1\nPERRY GERSHON\n255982\n248426\n0\n1\n0\n\n\n2018\nNEW YORK\n24\nDANA BALTER\n246452\n231804\n0\n1\n0\n\n\n2018\nNEW YORK\n24\nJOHN M KATKO\n273840\n227076\n1\n0\n0\n\n\n2018\nNEW YORK\n27\nCHRIS COLLINS\n280292\n229012\n1\n0\n0\n\n\n2018\nNEW YORK\n27\nNATHAN D MCMURRAY\n278118\n256334\n0\n1\n0\n\n\n2022\nNEW YORK\n4\nANTHONY P D’ESPOSITO\n140622\n129353\n1\n0\n0\n\n\n2022\nNEW YORK\n4\nLAURA A GILLEN\n130871\n130871\n0\n1\n0\n\n\n2022\nNEW YORK\n17\nMICHAEL V LAWLER\n287100\n251476\n1\n0\n0\n\n\n2022\nNEW YORK\n17\nSEAN PATRICK MALONEY\n283460\n266914\n0\n1\n0\n\n\n2022\nNEW YORK\n22\nBRANDON M WILLIAMS\n135544\n116529\n1\n0\n0\n\n\n2022\nNEW YORK\n22\nFRANCIS CONOLE\n132913\n132913\n0\n1\n0\n\n\n\n\n\n\n\n\n#display a list of states, years and districts where election results would have been different\n\nhouse_fusion_merged_filtered |&gt;\n  select(year,state,district,is_actual_winner) |&gt;\n  group_by(year,state,district)|&gt;\n  summarise(num_elections=sum(is_actual_winner)) |&gt;\n  ungroup() |&gt;\n  select(-num_elections) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Summary - Elections With  Different Results In The Absense of Fusion System\"\n  )\n\n\n\n\n\n\n\nSummary - Elections With Different Results In The Absense of Fusion System\n\n\nyear\nstate\ndistrict\n\n\n\n\n1976\nNEW YORK\n29\n\n\n1980\nNEW YORK\n3\n\n\n1980\nNEW YORK\n6\n\n\n1984\nNEW YORK\n20\n\n\n1986\nNEW YORK\n27\n\n\n1992\nCONNECTICUT\n2\n\n\n1992\nNEW YORK\n3\n\n\n1994\nNEW YORK\n1\n\n\n1996\nNEW YORK\n1\n\n\n1996\nNEW YORK\n30\n\n\n2000\nCONNECTICUT\n2\n\n\n2006\nNEW YORK\n25\n\n\n2006\nNEW YORK\n29\n\n\n2010\nNEW YORK\n13\n\n\n2010\nNEW YORK\n19\n\n\n2010\nNEW YORK\n24\n\n\n2010\nNEW YORK\n25\n\n\n2012\nNEW YORK\n27\n\n\n2018\nNEW YORK\n1\n\n\n2018\nNEW YORK\n24\n\n\n2018\nNEW YORK\n27\n\n\n2022\nNEW YORK\n4\n\n\n2022\nNEW YORK\n17\n\n\n2022\nNEW YORK\n22\n\n\n\n\n\n\n\nBased on these results, we can ascertain that the use of fusion system in election barely has any discernible effect on the outcome as the election results would have been different only in a handful of cases.\n\n\nQ3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\n# get a df with results of presidential elections by state. \n\npres_df1&lt;- sqldf(\n  \"\n  with a as(\n  select year,\n  state,\n  candidate,\n  party_simplified as party,\n  candidatevotes as votes\n  from pres\n  where 1=1\n  and PARTY_SIMPLIFIED in ('DEMOCRAT','REPUBLICAN')\n  GROUP BY 1,2,3,4\n  )\n  ,\n  b as (\n  select a.*,\n  row_number() over(partition by year,state order by votes desc) as is_winner\n  from a\n  )\n  \n  select year,\n  state,\n  candidate,\n  party,\n  votes,\n  case\n  when is_winner=1 then 1 else 0 end as is_pres_winner\n  from b\n  ;\n  \"\n)\n\n\n# get a df with records for house elections filtered to D and R only\n\nhouse_prim_only&lt;-house |&gt;\n  filter(party=='DEMOCRAT' | party=='REPUBLICAN') |&gt;\n  group_by(year,state,party) |&gt;\n  summarize(total_party_votes=sum(candidatevotes)) |&gt;\n  ungroup()\n\n#determine  max votes by year/state elections\nhouse_prim_only_agg &lt;- house_prim_only %&gt;%\n  group_by(year, state) %&gt;%\n  summarize(max_votes = max(total_party_votes, na.rm = TRUE)) |&gt;\n  ungroup()\n  \n#merge 2 dfs\n\nhouse_prim_only_merged&lt;-left_join(house_prim_only,\n                                  house_prim_only_agg,\n                                  by=c(\"state\"=\"state\",\"year\"=\"year\"))\n#identify a winning party\n\nhouse_prim_only_merged2&lt;-house_prim_only_merged |&gt;\n  mutate(house_party_winner=case_when(total_party_votes==max_votes ~1,\n                             TRUE ~0))\n\n\n#join dfs with house and presidential results \n\nhouse_pres_merged&lt;-inner_join(pres_df1,\n                              house_prim_only_merged2,\n                              by=c(\"year\"=\"year\",\n                                   \"state\"=\"state\",\n                                   \"party\"=\"party\"))\n#rename columns for clarity\n\nhouse_pres_merged_for_plot&lt;-house_pres_merged |&gt;\n  select(year,state,candidate,party,votes,is_pres_winner,total_party_votes,house_party_winner) |&gt;\n  rename(presidential_votes=votes,\n         is_party_winner=house_party_winner\n         ) |&gt;\n  mutate(president_more_votes=case_when(presidential_votes&gt;total_party_votes~1,\n                                        TRUE~0)) |&gt;\n  mutate(delta_votes=presidential_votes-total_party_votes)\n\n\n#create a df for D party\ndem_party&lt;-house_pres_merged_for_plot |&gt;\n  filter(party=='DEMOCRAT') |&gt;\n  group_by(state) |&gt;\n  summarize(D_president_more_popular_than_cogress=sum(president_more_votes),\n            D_pct_of_all_elections=D_president_more_popular_than_cogress/12,\n            D_avg_difference_votes=mean(delta_votes)\n            ) |&gt;\n  ungroup() |&gt;\n  mutate(D_avg_difference_rank=rank(-D_avg_difference_votes,ties.method='first'))\n  \n#create a df for R party\nrep_party&lt;-house_pres_merged_for_plot |&gt;\n  filter(party=='REPUBLICAN') |&gt;\n  group_by(state) |&gt;\n  summarize(R_president_more_popular_than_cogress=sum(president_more_votes),\n            R_pct_of_all_elections=R_president_more_popular_than_cogress/12,\n            R_avg_difference_votes=mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(R_avg_difference_rank=rank(-R_avg_difference_votes,ties.method='first'))\n            \n            \n\n#join 2 dfs\ndem_rep_df&lt;-inner_join(dem_party,rep_party,by=c(\"state\"=\"state\"))\n\n#display\ndem_rep_df |&gt;\n  DT::datatable(options = list(\n    pageLength = 51\n  ))\n\n\n\n\n\nLooking at Democratic party, we"
  },
  {
    "objectID": "mp_course_project.html",
    "href": "mp_course_project.html",
    "title": "Unemployment in New York",
    "section": "",
    "text": "In this section, we obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tidycensus\")) install.packages(\"tidycensus\")\nlibrary(tidycensus)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"readr\")) install.packages(\"readr\")\nlibrary(readr)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project 03: Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "In this mini-project, we will investigate the claim that the US Electoral College systematically biases election results away from the popular vote.\n\nData Prep\nFirst, we will obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\n\n\n\n\nTask 1.Download Congressional Shapefiles 1976-2012\n\nDownload congressional shapefiles from Lewis et al. for all US Congresses5 from 1976 to 2012. Your download code should: 1)Be fully automated (no “hand-downloading”); 2)Download files with a systematic and interpretable naming convention 3)Only download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nThe code below will download files to a local folder per instructions.\n\n\nShow the code\ntd &lt;- tempdir()\n\nfor (i in 94:112) {\n  fname &lt;- paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \".zip\")\n\n  if (!file.exists(fname)) {\n    url &lt;- paste0(\"https://cdmaps.polisci.ucla.edu/shp/\", fname)\n\n    download.file(url, destfile = fname)\n\n    zip_contents &lt;- unzip(fname, exdir = td)\n    shp_file &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n    sf_data &lt;- read_sf(shp_file)\n\n    assign(paste0(\"districts\", formatC(i, width = 3, format = \"d\", flag = \"0\"), \"_sf\"), sf_data)\n  }\n}\n\n\n\n\nTask 2.Download Congressional Shapefiles 2014-2022\n\nDownload congressional shapefiles from the US Census Bureau for all US Congresses from 2014 to 2022. Your download code should: 1)Be fully automated (no “hand-downloading”); 2)Download files with a systematic and interpretable naming convention 3)Only download files as needed out of courtesy for the data provider’s web sever. That is, if you already have a copy of the file, do not re-download it repeatedly.\n\nThe code below will download files to a local folder per instructions.\n\n\nShow the code\nfor (i in 2014:2022) {\n  BASE_URL &lt;- \"https://www2.census.gov/geo/tiger/\"\n  if (i &gt;= 2018) {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd116.zip\")\n  } else if (i &gt; 2015) {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd115.zip\")\n  } else {\n    file &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \"/CD/tl_\", sprintf(\"%d\", i), \"_us_cd114.zip\")\n  }\n  download_name &lt;- paste0(\"TIGER\", sprintf(\"%d\", i), \".zip\")\n\n  if (!file.exists(download_name)) {\n    FILE_URL &lt;- paste0(BASE_URL, file)\n    print(FILE_URL)\n    download.file(FILE_URL, destfile = download_name, mode = \"wb\")\n  }\n}\n\n\n\n\nTask 3: Exploration of Vote Count Data\n\n\nQ1. Which states have gained and lost the most seats in the US House of Representatives between 1976 and 2022?\n\n\nBefore we proceed to analyzing data, it’s worth noting that states lose and gain seats in the U.S. House of Representatives based on changes in their population as measured by the U.S. Census. The number of House seats is fixed at 435, so after each Census, which is conducted every ten years, seats are reapportioned among the states to reflect population shifts. The population used for this calculation includes all residents, regardless of age, citizenship, or legal status. (Source)\n\n\nShow the code\n## read in presidential elections data\n\npres &lt;- read.csv(\"president_1976_2020.csv\")\n# head(pres)\n\n# read in house election vote data\n\nhouse &lt;- read.csv(\"house_1976_2022.csv\")\n# head(house)\n\n## create a df with house results for 1976 and 2022\n\nhouse_1976_and_2022 &lt;- sqldf(\n  \"\n   with h76 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=1976\n   group by 1\n   )\n   ,\n   h22 as (\n   select state,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2022\n   group by 1\n   )\n   ,\n   base as (\n   select h76.state,\n   h76.num_seats,\n   case\n   when h76.num_seats=0 then 1\n   else h76.num_seats\n   end as number_of_seats_1976,\n   h22.num_seats,\n   case\n   when h22.num_seats=0 then 1\n   else h22.num_seats\n   end as number_of_seats_2022\n   from h76\n   left join h22\n   on h76.state=h22.state\n   )\n\n   select state,\n   number_of_seats_1976,\n   number_of_seats_2022,\n   number_of_seats_2022 - number_of_seats_1976 as delta\n   from base\n    ;\n  \"\n)\n\n\n# display states with largest gains in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n  slice_max(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Gains in House Seats\"\n  )\n\n\n\n\n\n\n\n\nTop 5 States with Largest Gains in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nTEXAS\n24\n38\n14\n\n\nFLORIDA\n15\n28\n13\n\n\nCALIFORNIA\n43\n52\n9\n\n\nARIZONA\n4\n9\n5\n\n\nGEORGIA\n10\n14\n4\n\n\n\n\n\n\n\nTexas, Florida, California, Arizona and Georgia had largest gains in house seats between 1976 and 2022.\n\n\nShow the code\n# display states with largest losses in number of house seats from 1976 to 2022\n\nhouse_1976_and_2022 |&gt;\n  slice_min(delta, n = 5) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Top 5 States with Largest Losses in House Seats\"\n  )\n\n\n\n\n\n\n\n\nTop 5 States with Largest Losses in House Seats\n\n\nstate\nnumber_of_seats_1976\nnumber_of_seats_2022\ndelta\n\n\n\n\nNEW YORK\n39\n26\n-13\n\n\nOHIO\n23\n15\n-8\n\n\nPENNSYLVANIA\n25\n17\n-8\n\n\nILLINOIS\n24\n17\n-7\n\n\nMICHIGAN\n19\n13\n-6\n\n\n\n\n\n\n\nNew York, Ohio, Pennsylvania, Illinois and Michigan had largest losses in house seats between 1976 and 2022.\n\nQ2. New York State has a unique “fusion” voting system where one candidate can appear on multiple “lines” on the ballot and their vote counts are totaled. For instance, in 2022, Jerrold Nadler appeared on both the Democrat and Working Families party lines for NYS’ 12th Congressional District. He received 200,890 votes total (184,872 as a Democrat and 16,018 as WFP), easily defeating Michael Zumbluskas, who received 44,173 votes across three party lines (Republican, Conservative, and Parent). Are there any elections in our data where the election would have had a different outcome if the “fusion” system was not used and candidates only received the votes their received from their “major party line” (Democrat or Republican) and not their total number of votes across all lines?\n\nTo answer this question, first we are going to find states, years, and districts where fusion system was used in elections. We can do that by identifying instances where candidates received votes from multiple parties. Then we will recalculate the results of these elections by excluding votes from candidates non-primary parties. Lastly, we will compare historic performance to results in our “what-if” scenario and determine whether winners get to keep their win. Here’s a list of all elections where results would have been different in the absence of fusion system:\n\n\nShow the code\n# get a count of political parties candidates received votes from\nhouse_temp1 &lt;- house %&gt;%\n  group_by(state, year, district, candidate) |&gt;\n  summarize(distinct_party_count = n_distinct(party))\n\n# get a list of districts where candidates received votes from more than 1 party\nhouse_temp2 &lt;- house_temp1 |&gt;\n  filter(distinct_party_count &gt; 1)\n\n# get a list of all years, states and districts where fusion system was used\nhouse_temp3 &lt;- house_temp2 |&gt;\n  group_by(state, year, district)\n\n# subset house df to only include results from states, years and districts meeting the criteria\n\nhouse_fusion1 &lt;- inner_join(house, house_temp3, by = c(\"state\" = \"state\", \"year\" = \"year\", \"district\" = \"district\"))\n\n# get elections totals for each candidate in states/years/districts meeting the criteria\nhouse_fusion1_actuals &lt;- house_fusion1 |&gt;\n  group_by(year, state, district, candidate.x) |&gt;\n  summarise(actual_total_votes = sum(candidatevotes)) |&gt;\n  mutate(max_votes = max(actual_total_votes)) |&gt;\n  mutate(is_actual_winner = case_when(\n    actual_total_votes == max_votes ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  ungroup()\n\n# get candidate votes from their primary party only and determine a winner\nhouse_fusion1_primaryonly &lt;- house_fusion1 |&gt;\n  filter(party == \"DEMOCRAT\" | party == \"REPUBLICAN\") |&gt;\n  group_by(year, state, district, candidate.x) |&gt;\n  summarise(actual_primaryparty_votes = sum(candidatevotes)) |&gt;\n  mutate(max_primary_votes = max(actual_primaryparty_votes)) |&gt;\n  mutate(is_primaryvotesonly_winner = case_when(\n    actual_primaryparty_votes == max_primary_votes ~ 1,\n    TRUE ~ 0\n  ))\n\n# merge 2 datasets\nhouse_fusion_merged &lt;- left_join(house_fusion1_actuals,\n  house_fusion1_primaryonly,\n  by = c(\n    \"state\" = \"state\",\n    \"year\" = \"year\",\n    \"district\" = \"district\",\n    \"candidate.x\" = \"candidate.x\"\n  )\n)\n# filter for records where primary party is either D or R and determine if results would have been different\nhouse_fusion_merged_filtered &lt;- house_fusion_merged |&gt;\n  select(year, state, district, candidate.x, actual_total_votes, actual_primaryparty_votes, is_actual_winner, is_primaryvotesonly_winner) |&gt;\n  mutate(same_winner = case_when(\n    is_actual_winner == is_primaryvotesonly_winner ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  filter((same_winner == 0) & !is.na(actual_primaryparty_votes))\n\n# display a list of states, years and districts where election results would have been different\n\nhouse_fusion_merged_filtered |&gt;\n  select(year, state, district, is_actual_winner) |&gt;\n  group_by(year, state, district) |&gt;\n  summarise(num_elections = sum(is_actual_winner)) |&gt;\n  ungroup() |&gt;\n  select(-num_elections) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"List of Elections With  Different Results\"\n  )\n\n\n\n\n\n\n\n\nList of Elections With Different Results\n\n\nyear\nstate\ndistrict\n\n\n\n\n1976\nNEW YORK\n29\n\n\n1980\nNEW YORK\n3\n\n\n1980\nNEW YORK\n6\n\n\n1984\nNEW YORK\n20\n\n\n1986\nNEW YORK\n27\n\n\n1992\nCONNECTICUT\n2\n\n\n1992\nNEW YORK\n3\n\n\n1994\nNEW YORK\n1\n\n\n1996\nNEW YORK\n1\n\n\n1996\nNEW YORK\n30\n\n\n2000\nCONNECTICUT\n2\n\n\n2006\nNEW YORK\n25\n\n\n2006\nNEW YORK\n29\n\n\n2010\nNEW YORK\n13\n\n\n2010\nNEW YORK\n19\n\n\n2010\nNEW YORK\n24\n\n\n2010\nNEW YORK\n25\n\n\n2012\nNEW YORK\n27\n\n\n2018\nNEW YORK\n1\n\n\n2018\nNEW YORK\n24\n\n\n2018\nNEW YORK\n27\n\n\n2022\nNEW YORK\n4\n\n\n2022\nNEW YORK\n17\n\n\n2022\nNEW YORK\n22\n\n\n\n\n\n\n\n\n\nShow the code\ncnt_fusion_elections &lt;- sqldf(\n  \"\n  select 'all fusion elections' as elections,\n  count(distinct year||state||district) as count_elections\n  from house_fusion_merged\n  group by 1\n\n  union all\n\n  select 'fusion elections with different outcomes' as elections,\n  count(distinct year||state||district) as count_elections\n  from house_fusion_merged\n  where 1=1\n  and is_actual_winner!=is_primaryvotesonly_winner\n  group by 1\n  ;\n  \"\n)\n\ncnt_fusion_elections |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Fusion Elections - Summary Results\"\n  )\n\n\n\n\n\n\n\n\nFusion Elections - Summary Results\n\n\nelections\ncount_elections\n\n\n\n\nall fusion elections\n754\n\n\nfusion elections with different outcomes\n24\n\n\n\n\n\n\n\nBased on these results, we ascertain that the use of fusion system in elections has no discernible effect on the outcome as the results would have been different only in a handful of cases (24 out of 754 elections, or 3.2%).\n\nQ3. Do presidential candidates tend to run ahead of or run behind congressional candidates in the same state? That is, does a Democratic candidate for president tend to get more votes in a given state than all Democratic congressional candidates in the same state? Does this trend differ over time? Does it differ across states or across parties? Are any presidents particularly more or less popular than their co-partisans?\n\nTo answer these questions, first we need to get results of Presidential and Congressional elections for every state for 12 years when Presidential elections were held. (You can look up results of individual states in the table below.)\n\n\nShow the code\n# get a df with results of presidential elections by state filtered to R and D candidates only\n\npres_df1 &lt;- sqldf(\n  \"\n  with a as(\n  select year,\n  state,\n  candidate,\n  party_simplified as party,\n  candidatevotes as votes\n  from pres\n  where 1=1\n  and PARTY_SIMPLIFIED in ('DEMOCRAT','REPUBLICAN')\n  GROUP BY 1,2,3,4\n  )\n  ,\n  b as (\n  select a.*,\n  row_number() over(partition by year,state order by votes desc) as is_winner\n  from a\n  )\n\n  select year,\n  state,\n  candidate,\n  party,\n  votes,\n  case\n  when is_winner=1 then 1 else 0 end as is_pres_winner\n  from b\n  ;\n  \"\n)\n\n\n# get a df with records for house elections filtered to D and R only\n\nhouse_prim_only &lt;- house |&gt;\n  filter(party == \"DEMOCRAT\" | party == \"REPUBLICAN\") |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_party_votes = sum(candidatevotes)) |&gt;\n  ungroup()\n\n# determine  max votes by year/state elections\nhouse_prim_only_agg &lt;- house_prim_only %&gt;%\n  group_by(year, state) %&gt;%\n  summarize(max_votes = max(total_party_votes, na.rm = TRUE)) |&gt;\n  ungroup()\n\n# merge 2 dfs\n\nhouse_prim_only_merged &lt;- left_join(house_prim_only,\n  house_prim_only_agg,\n  by = c(\"state\" = \"state\", \"year\" = \"year\")\n)\n\n# identify a winning party in congressional election\n\nhouse_prim_only_merged2 &lt;- house_prim_only_merged |&gt;\n  mutate(house_party_winner = case_when(\n    total_party_votes == max_votes ~ 1,\n    TRUE ~ 0\n  ))\n\n\n# join dfs with house and presidential results\n\nhouse_pres_merged &lt;- inner_join(pres_df1,\n  house_prim_only_merged2,\n  by = c(\n    \"year\" = \"year\",\n    \"state\" = \"state\",\n    \"party\" = \"party\"\n  )\n)\n# rename columns for clarity\n\nhouse_pres_merged_for_plot &lt;- house_pres_merged |&gt;\n  select(year, state, candidate, party, votes, is_pres_winner, total_party_votes, house_party_winner) |&gt;\n  rename(\n    presidential_votes = votes,\n    is_party_winner = house_party_winner\n  ) |&gt;\n  mutate(president_more_votes = case_when(\n    presidential_votes &gt; total_party_votes ~ 1,\n    TRUE ~ 0\n  )) |&gt;\n  mutate(delta_votes = presidential_votes - total_party_votes)\n\n# create a df for D party\ndem_party &lt;- house_pres_merged_for_plot |&gt;\n  filter(party == \"DEMOCRAT\") |&gt;\n  group_by(state) |&gt;\n  summarize(\n    D_president_more_popular_than_cogress = sum(president_more_votes),\n    D_president_more_popular_share = D_president_more_popular_than_cogress / 12,\n    D_avg_difference_votes = mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(D_avg_delta_votes_ranked = rank(-D_avg_difference_votes, ties.method = \"first\"))\n\n# create a df for R party\nrep_party &lt;- house_pres_merged_for_plot |&gt;\n  filter(party == \"REPUBLICAN\") |&gt;\n  group_by(state) |&gt;\n  summarize(\n    R_president_more_popular_than_cogress = sum(president_more_votes),\n    R_president_more_popular_share = R_president_more_popular_than_cogress / 12,\n    R_avg_difference_votes = mean(delta_votes)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(R_avg_delta_votes_ranked = rank(-R_avg_difference_votes, ties.method = \"first\"))\n\n\n\n# join 2 dfs\ndem_rep_df &lt;- inner_join(dem_party, rep_party, by = c(\"state\" = \"state\"))\n\n# display the data\ndem_rep_df |&gt;\n  mutate(\n    R_president_more_popular_share = scales::percent(R_president_more_popular_share),\n    D_president_more_popular_share = scales::percent(D_president_more_popular_share),\n    R_avg_difference_votes = format(round(as.numeric(R_avg_difference_votes), 0), nsmall = 0, big.mark = \",\"),\n    D_avg_difference_votes = format(round(as.numeric(D_avg_difference_votes), 0), nsmall = 0, big.mark = \",\")\n  ) |&gt;\n  DT::datatable(\n    options = list(pageLength = 5),\n    filter = \"top\"\n  )\n\n\n\n\n\n\nNow that we have records with Presidential and Congress election results for all states, we can answer this question. Across all states in 12 elections, Democratic candidates had more votes than all Congressional candidates from Democratic party in 53% of all cases. For Republican candidates, this number was even higher at 65%.\n\n\nShow the code\ndem_rep_df %&gt;%\n  summarise(D_president_more_popular = mean(D_president_more_popular_share, na.rm = TRUE), R_president_more_popular = mean(R_president_more_popular_share, na.rm = TRUE)) %&gt;%\n  mutate(\n    D_president_more_popular = scales::percent(D_president_more_popular),\n    R_president_more_popular = scales::percent(R_president_more_popular)\n  ) |&gt;\n  ungroup()\n\n\n# A tibble: 1 × 2\n  D_president_more_popular R_president_more_popular\n  &lt;chr&gt;                    &lt;chr&gt;                   \n1 53%                      65%                     \n\n\n\n\nShow the code\n## display df by state\n\n\n# display the data\ndem_rep_df_plot &lt;- dem_rep_df |&gt;\n  select(state, R_president_more_popular_share, D_president_more_popular_share) |&gt;\n  mutate(\n    D_president_more_popular_share = scales::percent(D_president_more_popular_share),\n    R_president_more_popular_share = scales::percent(R_president_more_popular_share)\n  )\n\ndem_rep_df_plot |&gt;\n  gt()\n\n\n\n\n\n\n\n\nstate\nR_president_more_popular_share\nD_president_more_popular_share\n\n\n\n\nALABAMA\n91.7%\n58.3%\n\n\nALASKA\n25.0%\n58.3%\n\n\nARIZONA\n50.0%\n66.7%\n\n\nARKANSAS\n66.7%\n75.0%\n\n\nCALIFORNIA\n75.0%\n58.3%\n\n\nCOLORADO\n58.3%\n75.0%\n\n\nCONNECTICUT\n75.0%\n66.7%\n\n\nDELAWARE\n41.7%\n75.0%\n\n\nFLORIDA\n75.0%\n83.3%\n\n\nGEORGIA\n50.0%\n58.3%\n\n\nHAWAII\n83.3%\n33.3%\n\n\nIDAHO\n66.7%\n25.0%\n\n\nILLINOIS\n58.3%\n66.7%\n\n\nINDIANA\n75.0%\n16.7%\n\n\nIOWA\n41.7%\n66.7%\n\n\nKANSAS\n33.3%\n66.7%\n\n\nKENTUCKY\n66.7%\n66.7%\n\n\nLOUISIANA\n91.7%\n100.0%\n\n\nMAINE\n50.0%\n25.0%\n\n\nMARYLAND\n58.3%\n58.3%\n\n\nMASSACHUSETTS\n83.3%\n0.0%\n\n\nMICHIGAN\n75.0%\n50.0%\n\n\nMINNESOTA\n66.7%\n41.7%\n\n\nMISSISSIPPI\n83.3%\n50.0%\n\n\nMISSOURI\n75.0%\n41.7%\n\n\nMONTANA\n58.3%\n16.7%\n\n\nNEBRASKA\n0.0%\n83.3%\n\n\nNEVADA\n66.7%\n58.3%\n\n\nNEW HAMPSHIRE\n66.7%\n66.7%\n\n\nNEW JERSEY\n75.0%\n66.7%\n\n\nNEW MEXICO\n50.0%\n50.0%\n\n\nNEW YORK\n83.3%\n91.7%\n\n\nNORTH CAROLINA\n75.0%\n25.0%\n\n\nNORTH DAKOTA\n75.0%\n16.7%\n\n\nOHIO\n66.7%\n66.7%\n\n\nOKLAHOMA\n83.3%\n58.3%\n\n\nOREGON\n75.0%\n25.0%\n\n\nPENNSYLVANIA\n58.3%\n66.7%\n\n\nRHODE ISLAND\n58.3%\n41.7%\n\n\nSOUTH CAROLINA\n66.7%\n66.7%\n\n\nSOUTH DAKOTA\n58.3%\n25.0%\n\n\nTENNESSEE\n91.7%\n75.0%\n\n\nTEXAS\n66.7%\n50.0%\n\n\nUTAH\n66.7%\n8.3%\n\n\nVERMONT\n50.0%\n66.7%\n\n\nVIRGINIA\n58.3%\n91.7%\n\n\nWASHINGTON\n66.7%\n41.7%\n\n\nWEST VIRGINIA\n100.0%\n0.0%\n\n\nWISCONSIN\n58.3%\n66.7%\n\n\nWYOMING\n58.3%\n33.3%\n\n\n\n\n\n\n\n\n\nShow the code\n# plot states data\n\ndem_rep_plot1 &lt;- plot_ly(\n  data = dem_rep_df,\n  x = ~R_president_more_popular_share,\n  y = ~D_president_more_popular_share,\n  type = \"scatter\",\n  mode = \"markers\",\n  marker = list(size = 5),\n  color = ~state\n) |&gt;\n  layout(\n    title = \"\",\n    xaxis = list(\n      title = \"Popularity of R president vs Congress\",\n      font = 8,\n      tickfont = list(size = 8),\n      titlefont = list(size = 8)\n    ),\n    yaxis = list(\n      title = \"Popularity of D president vs Congress\",\n      font = 8,\n      tickfont = list(size = 8),\n      titlefont = list(size = 8)\n    ),\n    legend = list(\n      font = list(size = 8),\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2,\n      font = 8\n    )\n  )\n\ndem_rep_plot1\n\n\n\n\n\n\nConsistently with our previous findings, most states tend to favor presidential candidates more so than Congressional candidates, with this trend being more pronounced for Republican candidates. Notable exceptions are Nebraska, Alaska, Kansas, Delaware and Iowa, where Republican presidential candidates tend to receive fewer votes than Congressional hopefuls.\n\n\nShow the code\ndem_win_year &lt;- house_pres_merged_for_plot |&gt;\n  filter((party == \"DEMOCRAT\") & (president_more_votes == 1)) |&gt;\n  group_by(year, party) |&gt;\n  summarise(share_states_president_popular = n() / 51) |&gt;\n  ungroup()\n\nrep_win_year &lt;- house_pres_merged_for_plot |&gt;\n  filter((party == \"REPUBLICAN\") & (president_more_votes == 1)) |&gt;\n  group_by(year, party) |&gt;\n  summarise(share_states_president_popular = n() / 51) |&gt;\n  ungroup()\n\ndem_rep_win_for_plot &lt;- bind_rows(dem_win_year, rep_win_year)\n\ndem_rep_win_for_plot2 &lt;- dem_rep_win_for_plot |&gt;\n  ungroup()\n\n\n\n\nShow the code\n# create a chart\n\ndrplot2 &lt;- plot_ly(\n  data = dem_rep_win_for_plot2,\n  x = ~year, y = ~share_states_president_popular,\n  color = ~party,\n  type = \"scatter\",\n  mode = \"lines+markers\"\n) |&gt;\n  layout(\n    title = \"Share of States Favoring Presidential Candidates over Congressional\",\n    xaxis = list(title = \"\"),\n    yaxis = list(\n      title = \"Share of States\",\n      tickformat = \".0%\",\n      range = c(0, 1)\n    ),\n    legend = list(\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2, # Position below the plot\n      font = list(size = 8) # Smaller font size\n    )\n  )\n\ndrplot2\n\n\n\n\n\n\nAs we can see on this chart, in Democratic party, presidential candidates are slowly but surely becoming more popular than their Congressional peers over last 30 years. At the same time, changes in preferences of Republican electorate appear more drastic.\n\n\nTask 4: Automate Zip File Extraction\n\nMake a function read_shp_from_zip() which takes in a file name, pulls out the .shp file contained there in, and reads it into R using read_sf().\n\nHere’s the code for the function:\n\n# create a function\nread_shp_from_zip &lt;- function(filename) {\n  dest_dir &lt;- tempdir()\n  file &lt;- unzip(zipfile = filename, exdir = dest_dir)\n  shp_file &lt;- file[grepl(\"\\\\.shp$\", file)]\n  sf &lt;- read_sf(shp_file)\n  return(sf)\n}\n\n# test a function\ntest_sf &lt;- read_shp_from_zip(\"districts095.zip\")\nhead(test_sf)\n\nSimple feature collection with 6 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.5993 ymin: 30.62397 xmax: -73.7243 ymax: 43.09876\nGeodetic CRS:  NAD83\n# A tibble: 6 × 16\n  STATENAME ID    DISTRICT STARTCONG ENDCONG DISTRICTSI COUNTY PAGE  LAW   NOTE \n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Californ… 0060… 27       94        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n2 Georgia   0130… 2        93        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n3 New York  0360… 10       94        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n4 New York  0360… 11       94        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n5 New York  0360… 37       94        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n6 New York  0360… 38       94        97      &lt;NA&gt;       &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  \"{\\\"…\n# ℹ 6 more variables: BESTDEC &lt;chr&gt;, FINALNOTE &lt;chr&gt;, RNOTE &lt;chr&gt;,\n#   LASTCHANGE &lt;chr&gt;, FROMCOUNTY &lt;chr&gt;, geometry &lt;MULTIPOLYGON [°]&gt;\n\n\n\n\nTask 5: Chloropleth Visualization of the 2000 Presidential Election Electoral College Results\n\nUsing the data you downloaded earlier, create a chloropleth visualization of the electoral college results for the 2000 presidential election (Bush vs. Gore), coloring each state by the party that won the most votes in that state.\n\nTo create this visualization, first we will determine the winning candidate/party for every state and we will use it to assign color values on the chrolopleth map.\n\n\nShow the code\n# create a df with presidential election data\n\npres2000 &lt;- sqldf(\n  \"\n  with a as(\n  select year,state,state_po,\n  sum(case when party_simplified='DEMOCRAT' THEN candidatevotes else 0 end) as dem_votes,\n  sum(case when party_simplified='REPUBLICAN' THEN candidatevotes else 0 end) as rep_votes\n  from pres\n  where 1=1\n  and year=2000\n  and party_simplified in ('DEMOCRAT','REPUBLICAN')\n  group by 1,2,3\n  )\n  select year,\n  state_po,\n  dem_votes,\n  rep_votes,\n  case\n  when dem_votes&gt;rep_votes then 'Democrat' else 'Republican' end as party_won\n  from a\n  ;\n  \"\n)\n\n# create a df with ecv data\n\nhouse2000 &lt;- sqldf(\n  \"\n   with h20 as (\n   select\n   state_po,\n   max(district) as num_seats\n   from house\n   where 1=1\n   and year=2000\n   group by 1\n   )\n  ,\n  base as (\n   select\n   state_po,\n   case\n   when num_seats=0 then 1\n   else num_seats\n   end as number_of_seats2000\n   from h20\n  )\n  select\n  state_po,\n  number_of_seats2000+2 as ecv\n  from base\n    ;\n  \"\n)\n\n# join 2 dfs\necv20_df &lt;- inner_join(house2000, pres2000, by = c(\"state_po\" = \"state_po\"))\n\n###### -get a shapefile for map\nmap_sf &lt;- read_shp_from_zip(\"tl_2020_us_state.zip\")\n\n# subset data - we only need state abbreviations and coordinates\nstate_map1 &lt;- map_sf |&gt;\n  select(STUSPS, geometry)\n\n# create a df with state boundaries and election data\nel20_for_map &lt;- inner_join(state_map1, ecv20_df, by = c(\"STUSPS\" = \"state_po\"))\n\n\n\n# create a plot for 2000 presidential election\nggplot(\n  el20_for_map,\n  aes(\n    geometry = geometry,\n    fill = party_won\n  )\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\n    \"Democrat\" = \"blue\",\n    \"Republican\" = \"red\"\n  )) +\n  ggtitle(\"2020 Election Results by State\")\n\n\n\n\n\n\n\n\n\n\nTask 6: Advanced Chloropleth Visualization of Electoral College Results\n\nModify your previous code to make either a faceted version showing election results over time.\n\n\n\nShow the code\n# create a df with presidential election data\n\npres_elections_temp &lt;- pres |&gt;\n  filter(party_simplified == \"DEMOCRAT\" | party_simplified == \"REPUBLICAN\") |&gt;\n  group_by(year, state_po, party_simplified) |&gt;\n  summarize(votes = sum(candidatevotes))\n\n# pivot for plot\n\npres_elections_temp2 &lt;- pivot_wider(pres_elections_temp,\n  # id_cols=(year,state_po),\n  names_from = party_simplified,\n  values_from = votes\n)\n\n# add new column with winning party\npres_elections_temp2 &lt;- pres_elections_temp2 |&gt;\n  mutate(party_won = case_when(DEMOCRAT &gt; REPUBLICAN ~ \"Democrat\", TRUE ~ \"Republican\"))\n\n\n# create a df with state boundaries and election data\nel20_for_map_overtime &lt;- inner_join(state_map1, pres_elections_temp2, by = c(\"STUSPS\" = \"state_po\"))\n\n\n\n# create a plot for 2000 presidential election\n\np1 &lt;- ggplot(\n  el20_for_map_overtime,\n  aes(\n    geometry = geometry,\n    fill = party_won\n  )\n) +\n  geom_sf() +\n  scale_fill_manual(values = c(\n    \"Democrat\" = \"blue\",\n    \"Republican\" = \"red\"\n  )) +\n  facet_wrap(~ factor(year), labeller = as_labeller(function(x) paste(\"year:\", x)), ncol = 1) +\n  ggtitle(\"US Presidential Election Results Over Time\") +\n  theme(\n    strip.text = element_text(size = 24),\n    legend.text = element_text(size = 24), # Increase legend label text size\n    legend.title = element_text(size = 24), # Increase legend title text size\n    legend.key.size = unit(3, \"lines\"),\n    plot.title = element_text(hjust = 0.5, size = 24)\n  )\n\np1\n\n\n\n\n\n\n\n\nAs we can see on this plot, the changes in the U.S. electoral map reflect significant shifts in political alignments, resulting in a very polarized and competitive landscape, especially lately."
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project 04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "In this mini-project, we will help a new CUNY employee make an important personal financial decision by analyzing performance of two available retirement plans, TRS and ORP.\n\nData Prep\nFirst, we will install and load libraries.\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\nif (!require(\"alphavantager\")) install.packages(\"alphavantager\")\nlibrary(alphavantager)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"purrr\")) install.packages(\"purrr\")\nlibrary(purrr)\nif (!require(\"tibble\")) install.packages(\"tibble\")\nlibrary(tibble)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"infer\")) install.packages(\"infer\")\nlibrary(infer)\nif (!require(\"reshape2\")) install.packages(\"reshape2\")\nlibrary(reshape2)\nif (!require(\"forecast\")) install.packages(\"forecast\")\nlibrary(forecast)\n\n\n\n\nTask 1.Register for AlphaVantage API Key.\nI registered for my API key at the website and stored it in a text file - we will read it into R using the readLines function.\n\n# Load the AV API key from a local file\nav_api_key &lt;- readLines(\"alpha_advantage.txt\")\n\n\n\nTask 2.Register for FRED API Key.\nI registered for my API key at the website and stored in a text file - we will read it into R using the readLines function.\n\n# Load the FRED API key from a local file\nfred_api_key &lt;- readLines(\"fred.txt\")\n\n\n\nTask 3.Data Acquisition.\n\nIdentify and download historical data series for each of the above inputs to your Monte Carlo analysis.\n\n\nFRED data acquisition\nWe will download inflation and wage growth data from FRED:\n1. Sticky Price Consumer Price Index\nThe Sticky Price Consumer Price Index (CPI) is calculated from a subset of goods and services included in the CPI that change price relatively infrequently. Because these goods and services change price relatively infrequently, they are thought to incorporate expectations about future inflation to a greater degree than prices that change on a more frequent basis. Source\n\n\nShow the code\n# Base URL for the API request\nbase_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Construct the request for cpi\nresponse &lt;- request(base_url) %&gt;%\n  req_url_query(\n    series_id = \"STICKCPIM157SFRBATL\",\n    api_key = fred_api_key,\n    file_type = \"json\" #\n  ) %&gt;%\n  req_perform()\n\n# Print the response as a JSON object\ndata_cpi &lt;- response %&gt;%\n  resp_body_json()\n\n# View the data\n# str(data_cpi)\n\n# Extract the observations into a data frame\nif (!is.null(data_cpi$observations)) {\n  # Create a tibble from the observations list\n  df_cpi &lt;- map_df(data_cpi$observations, ~ tibble(\n    date = .x$date,\n    value = as.numeric(.x$value) # Convert value to numeric\n  ))\n}\n\n\n\nWage growth\nWe will use median of the year-over-year percent change in hourly wage rates computed at the individual level using linked wage records.Source\n\n\n\nShow the code\n# Construct the request for wage growth\nresponse &lt;- request(base_url) %&gt;%\n  req_url_query(\n    series_id = \"FRBATLWGTUMHWGO\",\n    api_key = fred_api_key,\n    file_type = \"json\" #\n  ) %&gt;%\n  req_perform()\n\n# Print the response as a JSON object\ndata_wg &lt;- response %&gt;%\n  resp_body_json()\n\n# Extract the observations into a data frame\nif (!is.null(data_wg$observations)) {\n  # Create a tibble from the observations list\n  df_wg &lt;- map_df(data_wg$observations, ~ tibble(\n    date = .x$date,\n    value = as.numeric(.x$value) # Convert value to numeric\n  ))\n}\n\n\n\n\nAlpha Vantage data acquisition:\n\nSPY ETF. The fund seeks to provide investment results that, before expenses, correspond generally to the price and yield performance of the S&P 500® Index. We will use it as a proxy for investment in US equities.\nAGG ETF. This fund seeks to track the investment results of an index composed of the total U.S. investment-grade bond market. We will use it as a proxy for investment in bonds.\nVEU ETF. This fund seeks to track the performance of the FTSE All-World ex US Index, providing a convenient way to get broad exposure across developed and emerging non-U.S. equity markets around the world. We will use it as a proxy for investment in international equities.\nSHV ETF. The fund seeks to track the investment results of an index composed of U.S. Treasury bonds with remaining maturities one year or less. We will use it as a proxy for investment in short-term debt. Please note that Exchange-Traded Funds (ETFs) focusing on short-term debt became more prevalent in the early 2000s. For instance, the iShares Short Treasury Bond ETF (SHV), which invests in short-term U.S. Treasury securities, was launched on January 5, 2007.\n\n\n\nShow the code\n# Define the base URL\nbase_url &lt;- \"https://www.alphavantage.co/query\"\n\n# List of symbols to query\nsymbols &lt;- c(\"SPY\", \"AGG\", \"VEU\", \"SHV\")\n\n# Function to fetch data for a single symbol\nfetch_data_for_symbol &lt;- function(symbol) {\n  response &lt;- request(base_url) %&gt;%\n    req_url_query(\n      `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n      symbol = symbol,\n      apikey = av_api_key\n    ) %&gt;%\n    req_perform()\n\n  data &lt;- response %&gt;% resp_body_json()\n\n  # Extract the time series data into a data frame if it exists\n  if (!is.null(data$`Monthly Adjusted Time Series`)) {\n    df &lt;- as.data.frame(do.call(rbind, data$`Monthly Adjusted Time Series`), stringsAsFactors = FALSE)\n    df$date &lt;- rownames(df)\n    rownames(df) &lt;- NULL\n\n    # Add the symbol to identify data\n    df &lt;- mutate(df, symbol = symbol)\n\n    # Convert numeric columns\n    df &lt;- transform(\n      df,\n      `1. open` = as.numeric(`1. open`),\n      `2. high` = as.numeric(`2. high`),\n      `3. low` = as.numeric(`3. low`),\n      `4. close` = as.numeric(`4. close`),\n      `5. adjusted close` = as.numeric(`5. adjusted close`),\n      `6. volume` = as.numeric(`6. volume`),\n      `7. dividend amount` = as.numeric(`7. dividend amount`)\n    )\n\n    return(df)\n  } else {\n    return(NULL) # Return NULL if no data is available\n  }\n}\n\n# Fetch data for all symbols and combine into one data frame\nall_data &lt;- map_dfr(symbols, fetch_data_for_symbol)\n\n# Print the combined data frame\n# print(all_data)\n\n# save AV df to a local file - we will work with it to avoid exceeding the API rate limit\nwrite_csv(all_data2, \"all_data.csv\")\n\n\n\n\nData prep\nNow we will clean, consolidate and prepare data for further analysis.\n\n\nShow the code\n# read the df with 4 etfs\n\ntemp_av_df &lt;- read_csv(\"all_data2.csv\")\n# head(temp_av_df)\n\nav_df &lt;- temp_av_df |&gt;\n  select(date, symbol, `5. adjusted close`) |&gt;\n  rename(adj_close = `5. adjusted close`) |&gt;\n  mutate(date = as.Date(date, format = \"%m/%d/%Y\")) |&gt;\n  mutate(cal_month = floor_date(date, unit = \"month\")) |&gt;\n  mutate(symbol_upd = case_when(\n    symbol == \"SPY\" ~ \"us_equities\",\n    symbol == \"AGG\" ~ \"bonds\",\n    symbol == \"SHV\" ~ \"short_term_debt\",\n    symbol == \"VEU\" ~ \"international_equities\",\n    TRUE ~ \"Unknown\"\n  )) |&gt;\n  select(-date)\n\n# check data avail for all etfs to determine lookback for historical data\n# av_df |&gt;\n#  group_by(symbol) |&gt;\n#  summarize(\n#    min_ds = min(cal_month, na.rm = TRUE),\n#    max_ds = max(cal_month, na.rm = TRUE)\n#  ) |&gt;\n#  ungroup()\n\n# subset etf data to exclude everything prior to 2007-03-01 as we only have data available from then for one of the ETFs\n\nav_df2 &lt;- av_df |&gt;\n  filter(cal_month &gt;= \"2007-03-01\")\n\n# transpose df with av data - we will use it to join with fred data\n\nav_df_wide &lt;- pivot_wider(av_df2,\n  id_cols = cal_month,\n  names_from = symbol_upd,\n  values_from = adj_close\n)\n# update cpi df\n\ndf_cpi2 &lt;- df_cpi |&gt;\n  mutate(date = as.Date(date)) |&gt;\n  mutate(cal_month = floor_date(date, unit = \"month\")) |&gt;\n  rename(cpi = value) |&gt;\n  select(-date)\n\n# update wage growth df\n\ndf_wg2 &lt;- df_wg |&gt;\n  mutate(date = as.Date(date)) |&gt;\n  rename(wage_growth = value) |&gt;\n  mutate(\n    cal_month = floor_date(date, unit = \"month\"),\n    wage_growth_pct = wage_growth / 100\n  ) |&gt;\n  select(-date, -wage_growth)\n\n# join 2 fred dfs\nfred_df &lt;- inner_join(df_cpi2,\n  df_wg2,\n  by = c(\"cal_month\" = \"cal_month\")\n)\n\n# join AV and Fred data\n\nmerged_df_wide &lt;- inner_join(av_df_wide,\n  fred_df,\n  by = c(\"cal_month\" = \"cal_month\")\n)\n\n# exclude data before 09/2007 and after 09/2023 to enable inflation adjustment calculation as well calculation of long-term averages\n\nmerged_df_wide &lt;- merged_df_wide |&gt;\n  filter(between(cal_month, as.Date(\"2007-09-01\"), as.Date(\"2023-09-01\")))\n\n# calculate annual and monthly returns for etfs\nmerged_df_wide2 &lt;- sqldf(\n  \"\n  with ao as(\n  select a.*,\n  lead(us_equities,12) over(order by cal_month desc) as us_prev,\nlead(bonds,12) over(order by cal_month desc) as bonds_prev,\nlead(international_equities,12) over(order by cal_month desc) as int_prev,\nlead(short_term_debt,12) over(order by cal_month desc) as short_prev ,\n\nlead(us_equities,1) over(order by cal_month desc) as us_prev_mo,\nlead(bonds,1) over(order by cal_month desc) as bonds_prev_mo,\nlead(international_equities,1) over(order by cal_month desc) as int_prev_mo,\nlead(short_term_debt,1) over(order by cal_month desc) as short_prev_mo\n\n  from merged_df_wide a\n  )\n  select ao.*,\n  us_equities/us_prev-1 as us_equities_return_an,\n  bonds/bonds_prev-1 as bonds_return_an,\n  international_equities/int_prev-1 as international_market_return_an,\n  short_term_debt/short_prev-1 as short_term_debt_return_an,\n\n  us_equities/us_prev_mo-1 as us_equities_return_mo,\n  bonds/bonds_prev_mo-1 as bonds_return_mo,\n  international_equities/int_prev_mo-1 as international_market_return_mo,\n  short_term_debt/short_prev_mo-1 as short_term_debt_return_mo\n\n  from ao\n    ;\n  \"\n)\n\n# drop columns we no longer need\nmerged_df_wide_use &lt;- merged_df_wide2 |&gt;\n  select(\n    cal_month,\n    cpi,\n    wage_growth_pct,\n    us_equities,\n    bonds,\n    short_term_debt,\n    international_equities,\n    us_equities_return_an,\n    bonds_return_an,\n    short_term_debt_return_an,\n    international_market_return_an,\n    us_equities_return_mo,\n    bonds_return_mo,\n    short_term_debt_return_mo,\n    international_market_return_mo,\n  )\n\n# create a different version of merged df - long data\nmerged_df_long_use &lt;- merged_df_wide_use |&gt;\n  pivot_longer(\n    cols = -cal_month,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(merged_df_long_use)\n\n\n# A tibble: 6 × 3\n  cal_month  metric                   value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;dbl&gt;\n1 2023-09-01 cpi                      0.414\n2 2023-09-01 wage_growth_pct          0.051\n3 2023-09-01 us_equities            422.   \n4 2023-09-01 bonds                   90.2  \n5 2023-09-01 short_term_debt        110.   \n6 2023-09-01 international_equities  51.9  \n\n\n\n\n\nTask 4.Initial Analysis.\n\nPerform some basic exploratory data analysis to identify key properties of your data.\n\nLet’s start with some descriptive statistics.\n\n\nShow the code\n# Get descriptive statistics\nstats &lt;- psych::describe(merged_df_wide_use)\n\n# convert to dataframe\nstats_df &lt;- as.data.frame(stats)\n\n# Convert row names to a column named 'metric'\nstats_df$metric &lt;- rownames(stats_df)\n\n# Remove row names\nrownames(stats_df) &lt;- NULL\n\n# Reorder columns\nstats_df &lt;- stats_df[, c(\"metric\", setdiff(names(stats_df), \"metric\"))]\n\n\nstats_df2 &lt;- stats_df |&gt;\n  select(-trimmed, -mad, -skew, -kurtosis, -se, -range, -vars) |&gt;\n  filter(metric != \"cal_month\") |&gt;\n  mutate(\n    mean = round(mean, 2),\n    sd = round(sd, 2),\n    median = round(median, 2),\n    min = round(min, 2),\n    max = round(max, 2)\n  )\n\nstats_df3 &lt;- stats_df2 |&gt;\n  gt() |&gt;\n  tab_options(\n    table.font.size = 12,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    metric = \"Metric\",\n    n = \"Count\",\n    mean = \"Mean\",\n    sd = \"St. Dev.\",\n    median = \"Median\",\n    min = \"Min\",\n    max = \"Max\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"lightgrey\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = seq(1, nrow(stats_df2), by = 2)\n    )\n  )\n\nstats_df3\n\n\n\n\n\n\n\n\nMetric\nCount\nMean\nSt. Dev.\nMedian\nMin\nMax\n\n\n\n\ncpi\n193\n0.22\n0.13\n0.20\n-0.14\n0.66\n\n\nwage_growth_pct\n193\n0.03\n0.01\n0.03\n0.01\n0.07\n\n\nus_equities\n193\n205.06\n113.27\n176.34\n54.98\n456.00\n\n\nbonds\n193\n85.53\n12.12\n86.75\n60.80\n106.86\n\n\nshort_term_debt\n193\n110.33\n0.18\n110.28\n109.60\n111.02\n\n\ninternational_equities\n193\n48.60\n6.93\n49.03\n25.53\n64.21\n\n\nus_equities_return_an\n181\n0.11\n0.17\n0.14\n-0.43\n0.56\n\n\nbonds_return_an\n181\n0.03\n0.05\n0.03\n-0.16\n0.14\n\n\nshort_term_debt_return_an\n181\n0.00\n0.00\n0.00\n0.00\n0.01\n\n\ninternational_market_return_an\n181\n0.02\n0.19\n0.02\n-0.53\n0.63\n\n\nus_equities_return_mo\n192\n0.01\n0.05\n0.01\n-0.17\n0.13\n\n\nbonds_return_mo\n192\n0.00\n0.01\n0.00\n-0.04\n0.07\n\n\nshort_term_debt_return_mo\n192\n0.00\n0.00\n0.00\n0.00\n0.01\n\n\ninternational_market_return_mo\n192\n0.00\n0.05\n0.00\n-0.23\n0.15\n\n\n\n\n\n\n\nWe have records of 193 months (~16 years) for 4 ETFs, which serve as proxies for different types of investment, as well as 2 economic indicators measuring CPI and wage growth. We also have monthly and annual returns for different types of investment (~15 years).\n\n\nShow the code\n# create a df with monthly returns on investment\netf_long_mo &lt;- merged_df_long_use |&gt;\n  filter(metric == \"us_equities_return_mo\" | metric == \"bonds_return_mo\" | metric == \"short_term_debt_return_mo\" | metric == \"international_market_return_mo\")\n\n\nggplot(etf_long_mo, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Monthly Returns on Investment\",\n    x = \"Month\",\n    y = \"Monthy Return (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nMonthly returns are very volatile so we will instead examine the plot of annual returns for trends in the data.\n\n\nShow the code\n# create a df with annual returns on investment\netf_long_an &lt;- merged_df_long_use |&gt;\n  filter(metric == \"us_equities_return_an\" | metric == \"bonds_return_an\" | metric == \"short_term_debt_return_an\" | metric == \"international_market_return_an\")\n\n\nggplot(etf_long_an, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Annual Returns on Investment\",\n    x = \"Month\",\n    y = \"Annual Return (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nStock market, either US or International, is the most volatile form of investment whereas short-term debt, while the least risky form of the investment, offers very little, if any, return.\n\n\nShow the code\n# create a df with econ indicators only\nfred_long &lt;- merged_df_long_use |&gt;\n  filter(metric == \"cpi\" | metric == \"wage_growth_pct\")\n\nggplot(fred_long, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Economic Indicators\",\n    x = \"Month\",\n    y = \"Change Rate From the Prior Year (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nIt’s a little disheartening to see how wage growth is lagging behind inflation.\n\n\nShow the code\n# subset the data for correlation matrix - we can drop monthly returns and ETFs values\nmerged_df_wide_use_numeric &lt;- merged_df_wide_use |&gt;\n  select(\n    -cal_month,\n    -us_equities,\n    -bonds,\n    -short_term_debt,\n    -international_equities,\n    -us_equities_return_mo,\n    -bonds_return_mo,\n    -short_term_debt_return_mo,\n    -international_market_return_mo\n  ) |&gt;\n  drop_na()\n\n# corr matrix\ncorr_matrix &lt;- round(cor(merged_df_wide_use_numeric), 2)\n\n# Get upper triangle of the correlation matrix\nget_upper_tri &lt;- function(corr_matrix) {\n  corr_matrix[lower.tri(corr_matrix)] &lt;- NA\n  return(corr_matrix)\n}\n\nupper_tri &lt;- get_upper_tri(corr_matrix)\n\nmelted_corr_matrix &lt;- melt(upper_tri, na.rm = TRUE)\n\nggheatmap &lt;- ggplot(data = melted_corr_matrix, aes(Var2, Var1, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"red\", high = \"green\", mid = \"white\",\n    midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n    name = \"Pearson\\nCorrelation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(\n    angle = 45, vjust = 1,\n    size = 8, hjust = 1\n  )) +\n  coord_fixed()\n\nggheatmap +\n  geom_text(aes(Var2, Var1, label = value), color = \"black\", size = 2) +\n  labs(title = \"Correlation Matrix\")\n\n\n\n\n\n\n\n\n\nWage growth rate is highly correlated with CPI and US stock market moves in the same direction as the international market.\n\n\nTask 5.Historical Comparison.\n\nImplement the TRS and ORP formulas above and compare the value of each of them for the first month of retirement. To do this, you may assume that your hypothetical employee joined CUNY in the first month of the historical data and retired from CUNY at the end of the final month of data.\n\n\nTRS\nUnder the TRS plan, employees pay a fixed percentage of their paycheck into the pension fund, and after retirement, the employer (CUNY) continues to pay employees a fraction of their salary until death. For CUNY employees contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nHowever, the retirement benefit itself is calculated based on the final average salary of the employee, specifically, on the average salary from the final 3 years of employment as well as the overall tenure:\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\\((35\\% + 2\\% * (N-20)) * \\text{FAS}\\) if \\(N \\geq 20\\)\n\nFirst, we will create salary records. We will increase the salary every year based on corresponding wage growth data to determine final annual salary for the last 3 years of employment, which in turn, will be used to calculate the benefit available for distribution when retirement starts.\nFor a starting salary, we will use $50,000.\n\n# Initialize the starting salary\ninitial_salary &lt;- 50000\n# print(paste(\"starting salary: \", initial_salary))\n\n\n\nShow the code\n# subset data\nmerged_df_wide_use_selected &lt;- merged_df_wide_use |&gt;\n  select(\n    cal_month,\n    wage_growth_pct,\n    cpi,\n    us_equities_return_mo,\n    international_market_return_mo,\n    short_term_debt_return_mo,\n    bonds_return_mo\n  ) |&gt;\n  arrange(cal_month)\n\n\n# create a column with salary\nmerged_df_wide_use_selected$salary_adjusted &lt;- initial_salary\n\n# Loop through the dataset to adjust salary only every September after the 1st year of employment\nfor (i in 2:nrow(merged_df_wide_use_selected)) {\n  # Check if the current month is September\n  if (format(merged_df_wide_use_selected$cal_month[i], \"%m\") == \"09\") {\n    merged_df_wide_use_selected$salary_adjusted[i] &lt;- merged_df_wide_use_selected$salary_adjusted[i - 1] * (1 + merged_df_wide_use_selected$wage_growth_pct[i])\n  } else {\n    # Carry forward the previous salary for non-September months\n    merged_df_wide_use_selected$salary_adjusted[i] &lt;- merged_df_wide_use_selected$salary_adjusted[i - 1]\n  }\n}\n\n# determine contribution rate\n\nmerged_df_wide_use_selected$trs_contribution_rate &lt;- ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 45000, 0.03,\n  ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 55000, 0.035,\n    ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 75000, 0.045,\n      ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 100000, 0.0575, 0.06)\n    )\n  )\n)\n\n# calculate monthly contribution\nmerged_df_wide_use_selected$monthly_salary &lt;- merged_df_wide_use_selected$salary_adjusted / 12\n\nmerged_df_wide_use_selected$trs_monthly_contribution &lt;- merged_df_wide_use_selected$monthly_salary * merged_df_wide_use_selected$trs_contribution_rate\n\n# merged_df_wide_use_selected |&gt;\n#  summarize(total_trs_contributions = sum(trs_monthly_contribution))\n\n\nAs stated earlier, the starting retirement benefit value is driven by tenure and final salary (FAS), so this is what we will calculate next.\n\n\nShow the code\n# add a year column\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected |&gt;\n  mutate(cal_year = floor_date(cal_month, unit = \"year\"))\n\n# get number of years worked (tenure)\ntenure &lt;- merged_df_wide_use_selected |&gt;\n  summarize(tenure = n_distinct(cal_year)) |&gt;\n  pull(tenure)\n\n## calculate FAS value\n\n# extract the most recent 3 years\nlast_3_years &lt;- max(as.numeric(format(merged_df_wide_use_selected$cal_year, \"%Y\"))) - 2\n\n# filter data for the last 3 years\nfiltered_data &lt;- merged_df_wide_use_selected %&gt;%\n  filter(as.numeric(format(cal_year, \"%Y\")) &gt;= last_3_years)\n\n# get the maximum salary_adjusted for each year\nmax_salary_per_year &lt;- filtered_data %&gt;%\n  group_by(format(cal_year, \"%Y\")) %&gt;%\n  summarize(max_salary = max(salary_adjusted, na.rm = TRUE))\n\n# calculate the average of the maximum salary_adjusted values\nfas_value &lt;- mean(max_salary_per_year$max_salary, na.rm = TRUE)\n\n# create the function to calculate TRS benefit_value\ncalculate_trs_value &lt;- function(tenure, fas_value) {\n  if (tenure &lt; 20) {\n    trs_value &lt;- 0.0167 * fas_value * tenure\n  } else if (tenure == 20) {\n    trs_value &lt;- 0.0175 * fas_value * tenure\n  } else if (tenure &gt; 20) {\n    trs_value &lt;- (0.35 + 0.02 * (tenure - 20)) * fas_value\n  }\n  return(trs_value)\n}\n\ntrs_starting_benefit &lt;- calculate_trs_value(tenure, fas_value)\n\nprint(paste(\"starting salary: \", initial_salary))\n\n\n[1] \"starting salary:  50000\"\n\n\nShow the code\nprint(paste(\"tenure: \", tenure))\n\n\n[1] \"tenure:  17\"\n\n\nShow the code\nprint(paste(\"FAS: \", round(fas_value, 0)))\n\n\n[1] \"FAS:  81567\"\n\n\nShow the code\nprint(paste(\"TRS value in Y1: \", round(trs_starting_benefit, 0)))\n\n\n[1] \"TRS value in Y1:  23157\"\n\n\n\n\nORP\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. At retirement, the employee has access to those funds and can choose to withdraw them at any rate desired. Under the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nContributions are immediately invested according to the asset allocations:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds -`6% Short-Term Debt\n\n\nAs contributions allocations are subject to the participant’s age, we need to define and add the age of the employee upon joining CUNY into the model for OPR. Let’s use 35 as the age upon hire.\n\nage_input &lt;- 35\n# print(paste(\"age when hired at CUNY: \", age_input))\n\nWe will also need to define withdrawal rate to calculate the retirement distribution in the first year. We will use 4%.\n\nwi_rate &lt;- 0.04\n\nNow we calculate contributions under the ORP system and compare the 2 plans.\n\n\nShow the code\n# fill in missing data - since we do not have monthly investment returns for our first month of data, 9/2007,  we will use the following month's values instead.\n\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected %&gt;%\n  mutate(across(everything(), ~ ifelse(is.na(.), lead(.), .))) |&gt;\n  mutate(cal_month = as.Date(cal_month))\n\n# get min cal_month for calculation\nmin_ds &lt;- merged_df_wide_use_selected |&gt;\n  summarize(min_ds = min(cal_month)) |&gt;\n  mutate(min_ds = as.Date(min_ds)) |&gt;\n  pull(min_ds)\n\n# Calculate a proxy for date of birth\ndob &lt;- min_ds - (age_input * 365.25) # Account for leap years\ndob &lt;- format(dob, \"%Y-%m-%d\") # Format as YYYY-MM-DD\n\n# add a column with DOB to df\nmerged_df_wide_use_selected$dob &lt;- dob\n\n# calculate age over time\nmerged_df_wide_use_selected$age_actual &lt;- round(as.numeric(interval(merged_df_wide_use_selected$dob, merged_df_wide_use_selected$cal_month) / years(1)), 0)\n\n#### calculate allocation amounts by employee and CUNY\n\n# create monthly opr_contributions and rates by employee - it's the same as in TRS so we can just copy them over\nmerged_df_wide_use_selected$opr_contribution_rate &lt;- merged_df_wide_use_selected$trs_contribution_rate\nmerged_df_wide_use_selected$opr_contribution_employee &lt;- merged_df_wide_use_selected$trs_monthly_contribution\n\n### calculate contribution rates and amounts by employer/CUNY\n\n# calculate number of years employed for every record we have\n\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected %&gt;%\n  mutate(years_employed0 = ceiling(as.numeric(difftime(cal_month, min(cal_month), units = \"days\")) / 365.25))\n\nmerged_df_wide_use_selected$years_employed &lt;- ifelse(merged_df_wide_use_selected$years_employed0 == 0, 1, merged_df_wide_use_selected$years_employed0)\n\n# calculate employer contribution %\n\nmerged_df_wide_use_selected$opr_contribution_rate_cuny &lt;- ifelse(merged_df_wide_use_selected$years_employed0 &lt;= 7, 0.08, 0.1)\n\n# calculate employer monthly contributions\n\nmerged_df_wide_use_selected$opr_contribution_cuny &lt;- merged_df_wide_use_selected$monthly_salary * merged_df_wide_use_selected$opr_contribution_rate_cuny\n\n\n### define contribution allocations\n\n# define allocation for US equity %\n\nmerged_df_wide_use_selected$us_equity_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.54,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.47,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.34,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.19, 0.54)\n    )\n  )\n)\n\n# define allocation for international equity %\n\nmerged_df_wide_use_selected$int_equity_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.36,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.32,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.23,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.13, 0.36)\n    )\n  )\n)\n\n# define allocation for bonds %\n\nmerged_df_wide_use_selected$bonds_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.1,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.21,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.43,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.62, 0.1)\n    )\n  )\n)\n\n# define allocation for short term debt  %\n\nmerged_df_wide_use_selected$short_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.0,\n  ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.06, 0.06)\n)\n\n\n#### calculate total contribution amounts\n\nmerged_df_wide_use_selected$opr_total_contribution &lt;- merged_df_wide_use_selected$opr_contribution_employee + merged_df_wide_use_selected$opr_contribution_cuny\n\n\n#### calculate investment allocation\n\n# us equity\nmerged_df_wide_use_selected$opr_us_stocks &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$us_equity_pct\n\n# int equity\nmerged_df_wide_use_selected$opr_int_stocks &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$int_equity_pct\n\n# bonds\nmerged_df_wide_use_selected$opr_bond &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$bonds_pct\n\n# short\nmerged_df_wide_use_selected$opr_short &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$short_pct\n\n# calculate future value of each type of invested contributions\n\nopr_returns &lt;- merged_df_wide_use_selected |&gt;\n  select(\n    cal_month,\n    us_equities_return_mo,\n    international_market_return_mo,\n    short_term_debt_return_mo,\n    bonds_return_mo,\n    opr_us_stocks,\n    opr_int_stocks,\n    opr_bond,\n    opr_short\n  ) |&gt;\n  mutate(net_total_return_us_stocks = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(us_equities_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_int_stocks = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(international_market_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_bonds = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(bonds_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_short_debt = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(short_term_debt_return_mo, default = 0))\n  ))\n\n\nopr &lt;- opr_returns |&gt;\n  summarize(\n    future_value_us_stocks = sum(opr_us_stocks * net_total_return_us_stocks),\n    future_value_int_stocks = sum(opr_int_stocks * net_total_return_int_stocks),\n    future_value_bonds = sum(opr_bond * net_total_return_bonds),\n    future_value_short_term = sum(opr_short * net_total_return_short_debt)\n  )\n\nopr$future_value_total_opr &lt;- rowSums(opr[, c(\n  \"future_value_us_stocks\",\n  \"future_value_int_stocks\",\n  \"future_value_bonds\",\n  \"future_value_short_term\"\n)])\n\n\n#  ORP withdrawal 1st year\nopr$opr_benefit_1year &lt;- opr$future_value_total_opr * wi_rate\n\n\n\n\nShow the code\n# trs_starting_benefit\norp_1y &lt;- opr$opr_benefit_1year\n\ndf_plans &lt;- data.frame(\n  metric = c(\"orp_1y\", \"trs_1y\"),\n  values = c(orp_1y, trs_starting_benefit)\n)\n\ndf_plans2 &lt;- as.data.frame(\n  pivot_wider(df_plans,\n    names_from = metric,\n    values_from = values\n  )\n)\n\n\ndf_plans2 &lt;- df_plans2 |&gt;\n  mutate(\n    orp_1mo = orp_1y / 12,\n    trs_1mo = trs_1y / 12,\n    delta = ((trs_1mo - orp_1mo) / orp_1mo)\n  )\n\n\ndf_plans2 |&gt;\n  mutate(\n    orp_1y = scales::dollar(orp_1y, accuracy = 1),\n    trs_1y = scales::dollar(trs_1y, accuracy = 1),\n    orp_1mo = scales::dollar(orp_1mo, accuracy = 1),\n    trs_1mo = scales::dollar(trs_1mo, accuracy = 1),\n    delta = scales::percent(delta, accuracy = 0.1)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"TRS vs ORP Comparison\",\n    subtitle = \"Retirement Benefit in Year 1 and Month 1 of Retirement\"\n  ) |&gt;\n  tab_options(\n    table.font.size = 12,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    orp_1y = \"ORP Benefit Year 1\",\n    trs_1y = \"TRS Benefit Year 1\",\n    orp_1mo = \"ORP Benefit Month 1\",\n    trs_1mo = \"TRS Benefit Month 1\",\n    delta = \"Delta\"\n  ) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"lightgrey\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = seq(1, nrow(opr), by = 2)\n    )\n  )\n\n\n\n\n\n\n\n\nTRS vs ORP Comparison\n\n\nRetirement Benefit in Year 1 and Month 1 of Retirement\n\n\nORP Benefit Year 1\nTRS Benefit Year 1\nORP Benefit Month 1\nTRS Benefit Month 1\nDelta\n\n\n\n\n$10,040\n$23,157\n$837\n$1,930\n130.7%\n\n\n\n\n\n\n\nUnder the 4% withdrawal rule, the TRS plan would provide 2.3X times more benefit than the ORP plan (under 4% withdrawal rule) when retirement starts, which emphasizes the importance of having additional income sources for a given ORP participant. However, even the larger TRS benefit is likely not enough to cover the living expenses in retirement if it’s the only source of income.\n\n\n\nTask 6. Fixed-Rate Analysis\n\nModify your simulation from the previous section to project an employee’s pension benefit (TRS) or withdrawal amount (ORP) from retirement until death. (You will need to select an estimated death age.) In order to implement cost-of-living-adjustments (TRS) and future market returns (ORP), you can use the long-run averages you computed previously.\n\nTRS plan\nThe benefit is increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent: e.g., a CPI of 2.9% gives an inflation adjustment of 1.5%. The benefit is capped below at 1% and above at 3%, so a CPI of 10% leads to a 3% inflation adjustment while a CPI of 0% leads to a 1% inflation adjustment.The inflation adjustment is effective each September and the CPI used is the aggregate monthly CPI of the previous 12 months.\nTo recalculate the TRS benefit to be paid out after the 1st year of retirement, we need to calculate annual inflation adjustment for each year in retirement. The average life expectancy in US as of 2022 was 77.43 years so we use this number, rounding it up to 75, to determine the number of years we will need to project the retirement benefit for. We will use the long-term average CPI for inflation adjustment of the retirement benefit annually.\n\n# life expectancy\nlife_exp &lt;- 75\n\n# create a variable for age at retirement\nage_retire &lt;- max(merged_df_wide_use_selected$age_actual)\n\nprint(paste(\"Age at Retirement: \", age_retire))\n\n[1] \"Age at Retirement:  51\"\n\n\n\n# calculate number of years in retirement\nyears_in_retirement &lt;- life_exp - age_retire\n\n\nprint(paste(\"Years in Retirement: \", years_in_retirement))\n\n[1] \"Years in Retirement:  24\"\n\n\n\n# long term avg cpi from task 4\nstats_df4 &lt;- as.data.frame(stats_df3)\nlong_term_avg_cpi &lt;- as.numeric(stats_df4$mean[1])\n\nprint(paste(\"Long-term CPI change: \", round(long_term_avg_cpi, 2)))\n\n[1] \"Long-term CPI change:  0.22\"\n\n\nNow that we defined all inputs, we can calculate the TRS benefit for each year in retirement.\n\n\nShow the code\n# create a df for inflation adjustment of TRS benefit\n\ntrs_var &lt;- as.numeric(df_plans2$trs_1y[1])\n\ntrs_df1 &lt;- data.frame(\n  cpi_reported = long_term_avg_cpi, # annual inflation adjustment\n  cpi_adj_rate = round(long_term_avg_cpi / 2, 1), # annual change\n  annual_trs_benefit = trs_var, # annual TRS benefit to be adjusted\n  years_in_retirement = 1:years_in_retirement # payments in retirement\n  #    trs_benefit_adj_an= trs_var*long_term_avg_cpi, #adjusted annual trs benefit\n  #    trs_benefit_adj_mo= trs_var*long_term_avg_cpi/12 #adjusted monthly trs benefit\n)\n\n\n# determine the size of the annual TRS adjustment\n\ntrs_df1$trs_adjustment_rate &lt;- ifelse(trs_df1$cpi_adj_rate &lt; 0.01, 0.01,\n  ifelse(merged_df_wide_use_selected$age_actual &gt; 0.03, 0.03, trs_df1$cpi_adj_rate)\n)\n\n# calculate net annual increases and adjusted annual benefit\ntrs_ann_adjustment_df &lt;- trs_df1 |&gt;\n  select(years_in_retirement, annual_trs_benefit, trs_adjustment_rate) |&gt;\n  mutate(net_total_return = order_by(\n    desc(years_in_retirement),\n    cumprod(1 + lead(trs_adjustment_rate, default = 0))\n  )) |&gt;\n  mutate(net_total_return_adj = rev(net_total_return)) |&gt;\n  mutate(trs_annual_benefit_adj = net_total_return_adj * annual_trs_benefit)\n\n\n\n# trs_plot_df1&lt;-trs_ann_adjustment_df |&gt;\n#  select(years_in_retirement,trs_annual_benefit_adj) |&gt;\n#  filter(years_in_retirement==1|years_in_retirement==max(years_in_retirement))\n\n# select vars for plotting\n\ntrs_plot1 &lt;- plot_ly(trs_ann_adjustment_df,\n  x = ~years_in_retirement, y = ~trs_annual_benefit_adj,\n  type = \"scatter\",\n  mode = \"lines+markers+text\",\n  text = ~ paste0(\"$\", round(trs_annual_benefit_adj, 0)), \n  textfont = list(size = 8),\n  textposition = \"top center\"\n) |&gt;\n  layout(\n    title = \"TRS Annual Benefit Adjusted for Inflation\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(\n      title = \"Annual Benefit ($)\",\n      tickformat = \"$,.0f\"\n    ),\n    legend = list(\n      orientation = \"h\", \n      x = 0.5, \n      xanchor = \"center\", \n      y = -0.2, \n      font = list(size = 8) \n    )\n  )\n\n\ntrs_plot1\n\n\n\n\n\n\nORP plan\nTo project the value of the ORP benefit distribution, we need to know the following:\n\nThe total value of ORP investment in the first year of retirement. We will use this to calculate amounts allocated to different types of investment going forward\nLong-run average return rates on different types of investment. We will use this to simulate investment portfolio growth over time in retirement.\nThe age at which the employee retired from CUNY. We need this to determine the allocation between different types of investment in 1st year of retirement\nAge of the retiree in each year of retirement. We need this to determine the allocation between different types of investment going forward.\nNumber of years in retirement. We need this to determine the number of projected periods.\nAnnual withdrawal rate. We need this to adjust ORP amount available every next year; here, we will use 4%.\n\nWe will start with consolidating records for the 1st year of retirement as all of these variables have already been calculated.\n\nyears_in_retirement\n\n[1] 24\n\n\n\nage_retire\n\n[1] 51\n\n\n\n\nShow the code\n# create an initial df\nopr_ret_df &lt;- data.frame(\n  years_in_retirement = 1:years_in_retirement,\n  age_when_retired = age_retire,\n  orp_total_y1 = opr$future_value_total_opr[1],\n  wrate = wi_rate\n)\n\n# add actual age calc\nopr_ret_df$age_actual &lt;- opr_ret_df$age_when_retired + opr_ret_df$years_in_retirement - 1\n\n# add us equity allocation%\nopr_ret_df$us_equity_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.54,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.47,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.34,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.19, 0.54)\n    )\n  )\n)\n\n# define allocation for international equity %\n\nopr_ret_df$int_equity_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.36,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.32,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.23,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.13, 0.36)\n    )\n  )\n)\n\n# define allocation for bonds %\n\nopr_ret_df$bonds_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.1,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.21,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.43,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.62, 0.1)\n    )\n  )\n)\n\n# define allocation for short term debt  %\n\nopr_ret_df$short_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 74, 0.0,\n  ifelse(opr_ret_df$age_actual &gt;= 75, 0.06, 0.06)\n)\n\n## create lon-run avg variables\n\nus_return &lt;- stats_df2 |&gt;\n  filter(metric == \"us_equities_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nint_return &lt;- stats_df2 |&gt;\n  filter(metric == \"international_market_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nbonds_return &lt;- stats_df2 |&gt;\n  filter(metric == \"bonds_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nshort_return &lt;- stats_df2 |&gt;\n  filter(metric == \"short_term_debt_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\n## add long-run rates to df\nopr_ret_df$us_return &lt;- us_return\nopr_ret_df$int_return &lt;- int_return\nopr_ret_df$bonds_return &lt;- bonds_return\nopr_ret_df$short_return &lt;- short_return\n\n\n\n## initialize columns\nopr_ret_df$ann_payment &lt;- NA\nopr_ret_df$orp_total_after_deduction &lt;- NA\nopr_ret_df$us_inv &lt;- NA\nopr_ret_df$int_inv &lt;- NA\nopr_ret_df$bonds_inv &lt;- NA\nopr_ret_df$short_inv &lt;- NA\nopr_ret_df$us_inv_growth &lt;- NA\nopr_ret_df$bonds_inv_growth &lt;- NA\nopr_ret_df$short_inv_growth &lt;- NA\nopr_ret_df$opr_total_eoy &lt;- NA\n\n# calculate annual growth after distributions\n\nfor (i in 1:nrow(opr_ret_df)) {\n  if (opr_ret_df$years_in_retirement[i] == 1) {\n    # first year\n    opr_ret_df$ann_payment[i] &lt;- opr_ret_df$wrate[i] * opr_ret_df$orp_total_y1[i]\n    opr_ret_df$orp_total_after_deduction[i] &lt;- opr_ret_df$orp_total_y1[i] - opr_ret_df$ann_payment[i]\n    opr_ret_df$us_inv[i] &lt;- opr_ret_df$us_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$int_inv[i] &lt;- opr_ret_df$int_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$bonds_inv[i] &lt;- opr_ret_df$bonds_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$short_inv[i] &lt;- opr_ret_df$short_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$us_inv_growth[i] &lt;- (1 + opr_ret_df$us_return[i]) * opr_ret_df$us_inv[i]\n    opr_ret_df$int_inv_growth[i] &lt;- (1 + opr_ret_df$int_return[i]) * opr_ret_df$int_inv[i]\n    opr_ret_df$bonds_inv_growth[i] &lt;- (1 + opr_ret_df$bonds_return[i]) * opr_ret_df$bonds_inv[i]\n    opr_ret_df$short_inv_growth[i] &lt;- (1 + opr_ret_df$short_return[i]) * opr_ret_df$short_inv[i]\n    opr_ret_df$opr_total_eoy[i] &lt;- opr_ret_df$us_inv_growth[i] + opr_ret_df$int_inv_growth[i] + opr_ret_df$bonds_inv_growth[i] + opr_ret_df$short_inv_growth[i]\n  } else {\n    # years 2 and after\n    opr_ret_df$ann_payment[i] &lt;- opr_ret_df$wrate[i] * opr_ret_df$opr_total_eoy[i - 1]\n    opr_ret_df$orp_total_after_deduction[i] &lt;- opr_ret_df$opr_total_eoy[i - 1] - opr_ret_df$ann_payment[i]\n    opr_ret_df$us_inv[i] &lt;- opr_ret_df$us_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$int_inv[i] &lt;- opr_ret_df$int_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$bonds_inv[i] &lt;- opr_ret_df$bonds_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$short_inv[i] &lt;- opr_ret_df$short_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$us_inv_growth[i] &lt;- (1 + opr_ret_df$us_return[i]) * opr_ret_df$us_inv[i]\n    opr_ret_df$int_inv_growth[i] &lt;- (1 + opr_ret_df$int_return[i]) * opr_ret_df$int_inv[i]\n    opr_ret_df$bonds_inv_growth[i] &lt;- (1 + opr_ret_df$bonds_return[i]) * opr_ret_df$bonds_inv[i]\n    opr_ret_df$short_inv_growth[i] &lt;- (1 + opr_ret_df$short_return[i]) * opr_ret_df$short_inv[i]\n    opr_ret_df$opr_total_eoy[i] &lt;- opr_ret_df$us_inv_growth[i] + opr_ret_df$int_inv_growth[i] + opr_ret_df$bonds_inv_growth[i] + opr_ret_df$short_inv_growth[i]\n  }\n}\n\n## subset relevant records from ORP projections\nopr_comp_selected &lt;- opr_ret_df |&gt;\n  select(years_in_retirement, opr_total_eoy, ann_payment)\n## join with TRS forecast\ntrs_comp_selected &lt;- trs_ann_adjustment_df |&gt;\n  select(years_in_retirement, trs_annual_benefit_adj)\n\nret_plans_comp &lt;- inner_join(opr_comp_selected,\n  trs_comp_selected,\n  by = c(\"years_in_retirement\" = \"years_in_retirement\")\n)\n\n\nret_plans_comp &lt;- ret_plans_comp |&gt;\n  rename(\n    orp_available_investment = opr_total_eoy,\n    orp_annual_benefit = ann_payment,\n    trs_annual_benefit = trs_annual_benefit_adj\n  ) |&gt;\n  mutate(\n    trs_annual_benefit = round(trs_annual_benefit, 0),\n    orp_annual_benefit = round(orp_annual_benefit, 0),\n    orp_available_investment = round(orp_available_investment, 0)\n  ) |&gt;\n  mutate(\n    trs_monthly_benefit = round(trs_annual_benefit / 12, 0),\n    orp_monthly_benefit = round(orp_annual_benefit / 12, 0),\n    delta = round((trs_monthly_benefit - orp_monthly_benefit) / orp_monthly_benefit, 2)\n  )\n\n\n\n# Create the plot\nfig1 &lt;- plot_ly(ret_plans_comp) %&gt;%\n  # ORP Available Investment\n  add_bars(\n    x = ~years_in_retirement,\n    y = ~orp_available_investment,\n    name = \"ORP Available Investment\",\n    marker = list(color = \"lightgrey\", opacity = 0.5)\n  ) %&gt;%\n  # ORP Annual Benefit\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~orp_annual_benefit,\n    name = \"ORP Annual Benefit\",\n    line = list(color = \"darkblue\")\n  ) %&gt;%\n  # TRS Annual Benefit\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~trs_annual_benefit,\n    name = \"TRS Annual Benefit\",\n    line = list(color = \"red\")\n  ) %&gt;%\n  layout(\n    title = \"Comparison of ORP and TRS Annual Benefits\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(title = \"Available Funds($)\"),\n    barmode = \"overlay\", # Bars and lines on the same plot\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.2)\n  )\n\n# Display the plot\nfig1\n\n\n\n\n\n\n\n\nShow the code\nret_plans_comp &lt;- ret_plans_comp %&gt;%\n  mutate(delta_percent = delta * 100)\n\n# Create the plot\nfig2 &lt;- plot_ly(ret_plans_comp) %&gt;%\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~delta_percent,\n    name = \"Delta b/w ORP and TRS Benefit\",\n    line = list(color = \"black\", dash = \"dot\"),\n    text = ~ paste0(round(delta_percent, 2), \"%\"),\n    textposition = \"top center\",\n    mode = \"lines+markers+text\"\n  ) %&gt;%\n  layout(\n    title = \"Delta between ORP and TRS Benefits\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(title = \"Percentage Difference(%)\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.2)\n  )\n\n# Display the plot\n#fig2\n\n\nUnder the existing assumptions, the TRS plan participant will receive a larger annual retirement benefit but the ORP plan participant will have a substantial amount of money left after death.\n\n\nTask 7.\n\nUsing your historical data, generate several (at least 200) “bootstrap histories” suitable for a Monte Carlo analysis. Use bootstrap sampling, i.e. sampling with replacement, to generate values for both the “while working” and “while retired” periods of the model; you do not need to assume constant long-term average values for the retirement predictions any more.\n\nTo check the validity of assumptions made in terms of long-run averages, we can resample the historical data and build 95% confidence intervals for the true mean of each distribution.\nWage Growth\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = wage_growth_pct) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the Code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0316   0.0348\n\n\nWe are 95% confident that the mean wage growth % is between 0.031 and 0.035, which means we were a bit too conservative with our assumption of long-run average of 0.03 based on historical data.\nCPI\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = cpi) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.202    0.234\n\n\nWe are 95% confident that the mean annual inflation is between 0.19 and 0.23, which means we our assumption of long-run average of 0.22 was reasonable.\nUS Stocks Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = us_equities_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0830    0.139\n\n\nWe are 95% confident that the mean US stock market return is between 0.08 and 0.13, which means we our assumption of long-run average of 0.11 was reasonable.\nIntl Stocks Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = international_market_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1  -0.0103   0.0411\n\n\nWe are 95% confident that the mean Intl stock market return is between -0.01 and 0.05, which means we our assumption of long-run average of 0.03 was reasonable.\nBonds Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = bonds_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0202   0.0346\n\n\nWe are 95% confident that the mean bonds investment return is between 0.019 and 0.033, which means we our assumption of long-run average of 0.03 was reasonable.\nShort-Term Debt Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = short_term_debt_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n    lower_ci upper_ci\n       &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.0000798 0.000373\n\n\nWe are 95% confident that the mean short-term debt investment return is between -0.0001 and 0.0003, which means we our assumption of long-run average of 0.00 was reasonable."
  },
  {
    "objectID": "mp_course_project.html#get-unrate-by-zip-code",
    "href": "mp_course_project.html#get-unrate-by-zip-code",
    "title": "Unemployment in New York",
    "section": "get unrate by zip code",
    "text": "get unrate by zip code\n\n# get a list of all variables in 5 year estimates\nvariables &lt;- load_variables(year = 2022, dataset = \"acs5\", cache = TRUE)\nvariables\n\n# A tibble: 28,152 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_001 Estimate!!Total:                         Sex by Age (W… tract    \n 2 B01001A_002 Estimate!!Total:!!Male:                  Sex by Age (W… tract    \n 3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years    Sex by Age (W… tract    \n 5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years  Sex by Age (W… tract    \n 6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years  Sex by Age (W… tract    \n 7 B01001A_007 Estimate!!Total:!!Male:!!18 and 19 years Sex by Age (W… tract    \n 8 B01001A_008 Estimate!!Total:!!Male:!!20 to 24 years  Sex by Age (W… tract    \n 9 B01001A_009 Estimate!!Total:!!Male:!!25 to 29 years  Sex by Age (W… tract    \n10 B01001A_010 Estimate!!Total:!!Male:!!30 to 34 years  Sex by Age (W… tract    \n# ℹ 28,142 more rows\n\n\n\n# test data for 1 year    \n\nunemployment_data &lt;- get_acs(\n    geography = \"zcta\",  # ZIP Code Tabulation Areas\n    variables = c(\n      unemployed = \"B23025_005E\",  # civilian labor force: Unemployed\n      labor_force = \"B23025_003E\",  # total civilian labor force\n      total_population_16plus=\"B23025_001E\", # total population 16+\n      employed = \"B23025_004E\"  # civilian labor force: employed\n       ),\n    year = 2011,  # Specify the ACS year\n    survey = \"acs5\"  # 5-year estimates\n  )\n\nGetting data from the 2007-2011 5-year ACS\n\n\n\n## loop for all years - available until 2022\nyears &lt;- lst(2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022)\n\n#list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes$ZipCode)) \n\n# Initialize an empty list to store results\nresults  &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)  # Print progress\n  \n  tryCatch({\n    # Fetch ACS data for the current year\n    data &lt;- get_acs(\n      geography = \"zcta\",  # ZIP Code Tabulation Areas\n      variables = c(\n        unemployed = \"B23025_005E\",  # Civilian labor force: Unemployed\n        labor_force = \"B23025_003E\",  # Total civilian labor force\n        total_population_16plus = \"B23025_001E\",  # Total population 16+\n        employed = \"B23025_004E\"  # Civilian labor force: Employed\n      ),\n      year = year,\n      survey = \"acs5\"  # 5-year estimates\n    )\n    \n    # Store the data in the results list\n    results[[as.character(year)]] &lt;- data\n  }, error = function(e) {\n    message(\"Error fetching data for year: \", year, \": \", e$message)\n  })\n}\n\nFetching data for year: 2011\n\n\nGetting data from the 2007-2011 5-year ACS\n\n\nFetching data for year: 2012\n\n\nGetting data from the 2008-2012 5-year ACS\n\n\nFetching data for year: 2013\n\n\nGetting data from the 2009-2013 5-year ACS\n\n\nFetching data for year: 2014\n\n\nGetting data from the 2010-2014 5-year ACS\n\n\nFetching data for year: 2015\n\n\nGetting data from the 2011-2015 5-year ACS\n\n\nFetching data for year: 2016\n\n\nGetting data from the 2012-2016 5-year ACS\n\n\nFetching data for year: 2017\n\n\nGetting data from the 2013-2017 5-year ACS\n\n\nFetching data for year: 2018\n\n\nGetting data from the 2014-2018 5-year ACS\n\n\nFetching data for year: 2019\n\n\nGetting data from the 2015-2019 5-year ACS\n\n\nFetching data for year: 2020\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nFetching data for year: 2021\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nFetching data for year: 2022\n\n\nGetting data from the 2018-2022 5-year ACS\n\n# Combine all results into a single dataframe\ncombined_data &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data$zcta&lt;-gsub(\"ZCTA5 \",\"\",combined_data$NAME)\n#head(combined_data)\n\nnyc_zip_codes&lt;-nyc_zip_codes |&gt;\n  mutate(zcta=as.character(ZipCode))\n\ncombined_data_nyc&lt;-inner_join(combined_data,nyc_zip_codes, by=c(\"zcta\"=\"zcta\"))\nhead(combined_data_nyc)\n\n# A tibble: 6 × 10\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… B23025_…     1515   277 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… B23025_…      379   117 10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… B23025_…     3010   470 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… B23025_…     1698   363 10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… B23025_…      749   214 10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… B23025_…      757   194 10471 Bronx   Kingsbridge…   10471\n\n\n\n#vars_df&lt;-as.data.frame(variables)\n#head(vars_df)\n#\n#\ncombined_data_nyc &lt;- combined_data_nyc %&gt;%\n      mutate(variable = recode(variable,\n                               \"B23025_005\" = \"unemployed\",\n                               \"B23025_003\" = \"labor_force\",\n                               \"B23025_001\" = \"total_population_16plus\",\n                               \"B23025_004\" = \"employed\"))\nhead(combined_data_nyc)\n\n# A tibble: 6 × 10\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… unemplo…     1515   277 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… unemplo…      379   117 10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… unemplo…     3010   470 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… unemplo…     1698   363 10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… unemplo…      749   214 10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… unemplo…      757   194 10471 Bronx   Kingsbridge…   10471\n\n\n\nget zhape files for zip codes\n\nif(!file.exists(\"zipcodes_maps.zip\")){\n    download.file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\", \n              destfile=\"zipcodes_maps.zip\",\n              method=\"curl\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"zipcodes_maps.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nzip_sf &lt;- read_sf(fname_shp)\n\n\nzip_sf2 &lt;- zip_sf |&gt;\n  semi_join(\n    nyc_zip_codes,\n    join_by(GEOID10 == zcta)\n  )\n\nhead(zip_sf2)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.98814 ymin: 40.64933 xmax: -73.83094 ymax: 40.86634\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  ZCTA5CE10 AFFGEOID10     GEOID10 ALAND10 AWATER10                     geometry\n  &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;MULTIPOLYGON [°]&gt;\n1 10040     8600000US10040 10040    988493        0 (((-73.93519 40.86027, -73.…\n2 11207     8600000US11207 11207   6920385        0 (((-73.90885 40.6934, -73.9…\n3 10456     8600000US10456 10456   2635671        0 (((-73.91903 40.83309, -73.…\n4 11417     8600000US11417 11417   2898418        0 (((-73.86235 40.67916, -73.…\n5 11104     8600000US11104 11104   1005110        0 (((-73.92763 40.7366, -73.9…\n6 10024     8600000US10024 10024   2222641        0 (((-73.98814 40.78141, -73.…\n\n\n\n## create a df with unemployed totals only\nunemployed_df&lt;-combined_data_nyc |&gt;\n  filter(variable=='unemployed') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(unemployed=estimate)\n\n## create a df with unemployed totals only\nlaborforce_df&lt;-combined_data_nyc |&gt;\n  filter(variable=='labor_force') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(laborforce=estimate)\n\n\nnyc_unrate_zip_df&lt;-inner_join(unemployed_df,\n                              laborforce_df,\n                              by=c(\"year\"=\"year\",\"zcta\"=\"zcta\"))\n\nnyc_unrate_zip_df&lt;-nyc_unrate_zip_df |&gt;\n  mutate(unrate=round(unemployed/laborforce,2))\n\n\n#pivot wide for maps\n\nnyc_unrate_zip_df_wide&lt;-pivot_wider(nyc_unrate_zip_df,\n                                    id_cols=zcta,\n                                    names_from=year,\n                                    values_from=unrate)\n\n\nzip_map2&lt;-zip_sf2 |&gt;\n  select(GEOID10,geometry)\n\nzip_map_merged&lt;-inner_join(zip_map2,\n                           nyc_unrate_zip_df_wide,\n                           by=c(\"GEOID10\"=\"zcta\"))\n\n\n## check plot\n## \n## ##check plot\n  \nggplot(zip_map_merged, \n       aes(geometry=geometry, \n           fill = `2011`)) + \n    geom_sf()"
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Mini-Project 04: Monte Carlo-Informed Selection of CUNY Retirement Plans",
    "section": "",
    "text": "In this mini-project, we will help a new CUNY employee make an important personal financial decision by analyzing performance of two available retirement plans, TRS and ORP.\n\nData Prep\nFirst, we will install and load libraries.\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\nif (!require(\"alphavantager\")) install.packages(\"alphavantager\")\nlibrary(alphavantager)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"purrr\")) install.packages(\"purrr\")\nlibrary(purrr)\nif (!require(\"tibble\")) install.packages(\"tibble\")\nlibrary(tibble)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"infer\")) install.packages(\"infer\")\nlibrary(infer)\nif (!require(\"reshape2\")) install.packages(\"reshape2\")\nlibrary(reshape2)\nif (!require(\"forecast\")) install.packages(\"forecast\")\nlibrary(forecast)\n\n\n\n\nTask 1.Register for AlphaVantage API Key.\nI registered for my API key at the website and stored it in a text file - we will read it into R using the readLines function.\n\n# Load the AV API key from a local file\nav_api_key &lt;- readLines(\"alpha_advantage.txt\")\n\n\n\nTask 2.Register for FRED API Key.\nI registered for my API key at the website and stored in a text file - we will read it into R using the readLines function.\n\n# Load the FRED API key from a local file\nfred_api_key &lt;- readLines(\"fred.txt\")\n\n\n\nTask 3.Data Acquisition.\n\nIdentify and download historical data series for each of the above inputs to your Monte Carlo analysis.\n\n\nFRED data acquisition\nWe will download inflation and wage growth data from FRED:\n1. Sticky Price Consumer Price Index\nThe Sticky Price Consumer Price Index (CPI) is calculated from a subset of goods and services included in the CPI that change price relatively infrequently. Because these goods and services change price relatively infrequently, they are thought to incorporate expectations about future inflation to a greater degree than prices that change on a more frequent basis. Source\n\n\nShow the code\n# Base URL for the API request\nbase_url &lt;- \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Construct the request for cpi\nresponse &lt;- request(base_url) %&gt;%\n  req_url_query(\n    series_id = \"STICKCPIM157SFRBATL\",\n    api_key = fred_api_key,\n    file_type = \"json\" #\n  ) %&gt;%\n  req_perform()\n\n# Print the response as a JSON object\ndata_cpi &lt;- response %&gt;%\n  resp_body_json()\n\n# View the data\n# str(data_cpi)\n\n# Extract the observations into a data frame\nif (!is.null(data_cpi$observations)) {\n  # Create a tibble from the observations list\n  df_cpi &lt;- map_df(data_cpi$observations, ~ tibble(\n    date = .x$date,\n    value = as.numeric(.x$value) # Convert value to numeric\n  ))\n}\n\n\n\nWage growth\nWe will use median of the year-over-year percent change in hourly wage rates computed at the individual level using linked wage records.Source\n\n\n\nShow the code\n# Construct the request for wage growth\nresponse &lt;- request(base_url) %&gt;%\n  req_url_query(\n    series_id = \"FRBATLWGTUMHWGO\",\n    api_key = fred_api_key,\n    file_type = \"json\" #\n  ) %&gt;%\n  req_perform()\n\n# Print the response as a JSON object\ndata_wg &lt;- response %&gt;%\n  resp_body_json()\n\n# Extract the observations into a data frame\nif (!is.null(data_wg$observations)) {\n  # Create a tibble from the observations list\n  df_wg &lt;- map_df(data_wg$observations, ~ tibble(\n    date = .x$date,\n    value = as.numeric(.x$value) # Convert value to numeric\n  ))\n}\n\n\n\n\nAlpha Vantage data acquisition:\n\nSPY ETF. The fund seeks to provide investment results that, before expenses, correspond generally to the price and yield performance of the S&P 500® Index. We will use it as a proxy for investment in US equities.\nAGG ETF. This fund seeks to track the investment results of an index composed of the total U.S. investment-grade bond market. We will use it as a proxy for investment in bonds.\nVEU ETF. This fund seeks to track the performance of the FTSE All-World ex US Index, providing a convenient way to get broad exposure across developed and emerging non-U.S. equity markets around the world. We will use it as a proxy for investment in international equities.\nSHV ETF. The fund seeks to track the investment results of an index composed of U.S. Treasury bonds with remaining maturities one year or less. We will use it as a proxy for investment in short-term debt. Please note that Exchange-Traded Funds (ETFs) focusing on short-term debt became more prevalent in the early 2000s. For instance, the iShares Short Treasury Bond ETF (SHV), which invests in short-term U.S. Treasury securities, was launched on January 5, 2007.\n\n\n\nShow the code\n# Define the base URL\nbase_url &lt;- \"https://www.alphavantage.co/query\"\n\n# List of symbols to query\nsymbols &lt;- c(\"SPY\", \"AGG\", \"VEU\", \"SHV\")\n\n# Function to fetch data for a single symbol\nfetch_data_for_symbol &lt;- function(symbol) {\n  response &lt;- request(base_url) %&gt;%\n    req_url_query(\n      `function` = \"TIME_SERIES_MONTHLY_ADJUSTED\",\n      symbol = symbol,\n      apikey = av_api_key\n    ) %&gt;%\n    req_perform()\n\n  data &lt;- response %&gt;% resp_body_json()\n\n  # Extract the time series data into a data frame if it exists\n  if (!is.null(data$`Monthly Adjusted Time Series`)) {\n    df &lt;- as.data.frame(do.call(rbind, data$`Monthly Adjusted Time Series`), stringsAsFactors = FALSE)\n    df$date &lt;- rownames(df)\n    rownames(df) &lt;- NULL\n\n    # Add the symbol to identify data\n    df &lt;- mutate(df, symbol = symbol)\n\n    # Convert numeric columns\n    df &lt;- transform(\n      df,\n      `1. open` = as.numeric(`1. open`),\n      `2. high` = as.numeric(`2. high`),\n      `3. low` = as.numeric(`3. low`),\n      `4. close` = as.numeric(`4. close`),\n      `5. adjusted close` = as.numeric(`5. adjusted close`),\n      `6. volume` = as.numeric(`6. volume`),\n      `7. dividend amount` = as.numeric(`7. dividend amount`)\n    )\n\n    return(df)\n  } else {\n    return(NULL) # Return NULL if no data is available\n  }\n}\n\n# Fetch data for all symbols and combine into one data frame\nall_data &lt;- map_dfr(symbols, fetch_data_for_symbol)\n\n# Print the combined data frame\n# print(all_data)\n\n# save AV df to a local file - we will work with it to avoid exceeding the API rate limit\nwrite_csv(all_data2, \"all_data.csv\")\n\n\n\n\nData prep\nNow we will clean, consolidate and prepare data for further analysis.\n\n\nShow the code\n# read the df with 4 etfs\n\ntemp_av_df &lt;- read_csv(\"all_data2.csv\")\n# head(temp_av_df)\n\nav_df &lt;- temp_av_df |&gt;\n  select(date, symbol, `5. adjusted close`) |&gt;\n  rename(adj_close = `5. adjusted close`) |&gt;\n  mutate(date = as.Date(date, format = \"%m/%d/%Y\")) |&gt;\n  mutate(cal_month = floor_date(date, unit = \"month\")) |&gt;\n  mutate(symbol_upd = case_when(\n    symbol == \"SPY\" ~ \"us_equities\",\n    symbol == \"AGG\" ~ \"bonds\",\n    symbol == \"SHV\" ~ \"short_term_debt\",\n    symbol == \"VEU\" ~ \"international_equities\",\n    TRUE ~ \"Unknown\"\n  )) |&gt;\n  select(-date)\n\n# check data avail for all etfs to determine lookback for historical data\n# av_df |&gt;\n#  group_by(symbol) |&gt;\n#  summarize(\n#    min_ds = min(cal_month, na.rm = TRUE),\n#    max_ds = max(cal_month, na.rm = TRUE)\n#  ) |&gt;\n#  ungroup()\n\n# subset etf data to exclude everything prior to 2007-03-01 as we only have data available from then for one of the ETFs\n\nav_df2 &lt;- av_df |&gt;\n  filter(cal_month &gt;= \"2007-03-01\")\n\n# transpose df with av data - we will use it to join with fred data\n\nav_df_wide &lt;- pivot_wider(av_df2,\n  id_cols = cal_month,\n  names_from = symbol_upd,\n  values_from = adj_close\n)\n# update cpi df\n\ndf_cpi2 &lt;- df_cpi |&gt;\n  mutate(date = as.Date(date)) |&gt;\n  mutate(cal_month = floor_date(date, unit = \"month\")) |&gt;\n  rename(cpi = value) |&gt;\n  select(-date)\n\n# update wage growth df\n\ndf_wg2 &lt;- df_wg |&gt;\n  mutate(date = as.Date(date)) |&gt;\n  rename(wage_growth = value) |&gt;\n  mutate(\n    cal_month = floor_date(date, unit = \"month\"),\n    wage_growth_pct = wage_growth / 100\n  ) |&gt;\n  select(-date, -wage_growth)\n\n# join 2 fred dfs\nfred_df &lt;- inner_join(df_cpi2,\n  df_wg2,\n  by = c(\"cal_month\" = \"cal_month\")\n)\n\n# join AV and Fred data\n\nmerged_df_wide &lt;- inner_join(av_df_wide,\n  fred_df,\n  by = c(\"cal_month\" = \"cal_month\")\n)\n\n# exclude data before 09/2007 and after 09/2023 to enable inflation adjustment calculation as well calculation of long-term averages\n\nmerged_df_wide &lt;- merged_df_wide |&gt;\n  filter(between(cal_month, as.Date(\"2007-09-01\"), as.Date(\"2023-09-01\")))\n\n# calculate annual and monthly returns for etfs\nmerged_df_wide2 &lt;- sqldf(\n  \"\n  with ao as(\n  select a.*,\n  lead(us_equities,12) over(order by cal_month desc) as us_prev,\nlead(bonds,12) over(order by cal_month desc) as bonds_prev,\nlead(international_equities,12) over(order by cal_month desc) as int_prev,\nlead(short_term_debt,12) over(order by cal_month desc) as short_prev ,\n\nlead(us_equities,1) over(order by cal_month desc) as us_prev_mo,\nlead(bonds,1) over(order by cal_month desc) as bonds_prev_mo,\nlead(international_equities,1) over(order by cal_month desc) as int_prev_mo,\nlead(short_term_debt,1) over(order by cal_month desc) as short_prev_mo\n\n  from merged_df_wide a\n  )\n  select ao.*,\n  us_equities/us_prev-1 as us_equities_return_an,\n  bonds/bonds_prev-1 as bonds_return_an,\n  international_equities/int_prev-1 as international_market_return_an,\n  short_term_debt/short_prev-1 as short_term_debt_return_an,\n\n  us_equities/us_prev_mo-1 as us_equities_return_mo,\n  bonds/bonds_prev_mo-1 as bonds_return_mo,\n  international_equities/int_prev_mo-1 as international_market_return_mo,\n  short_term_debt/short_prev_mo-1 as short_term_debt_return_mo\n\n  from ao\n    ;\n  \"\n)\n\n# drop columns we no longer need\nmerged_df_wide_use &lt;- merged_df_wide2 |&gt;\n  select(\n    cal_month,\n    cpi,\n    wage_growth_pct,\n    us_equities,\n    bonds,\n    short_term_debt,\n    international_equities,\n    us_equities_return_an,\n    bonds_return_an,\n    short_term_debt_return_an,\n    international_market_return_an,\n    us_equities_return_mo,\n    bonds_return_mo,\n    short_term_debt_return_mo,\n    international_market_return_mo,\n  )\n\n# create a different version of merged df - long data\nmerged_df_long_use &lt;- merged_df_wide_use |&gt;\n  pivot_longer(\n    cols = -cal_month,\n    names_to = \"metric\",\n    values_to = \"value\"\n  )\n\nhead(merged_df_long_use)\n\n\n# A tibble: 6 × 3\n  cal_month  metric                   value\n  &lt;date&gt;     &lt;chr&gt;                    &lt;dbl&gt;\n1 2023-09-01 cpi                      0.414\n2 2023-09-01 wage_growth_pct          0.051\n3 2023-09-01 us_equities            422.   \n4 2023-09-01 bonds                   90.2  \n5 2023-09-01 short_term_debt        110.   \n6 2023-09-01 international_equities  51.9  \n\n\n\n\n\nTask 4.Initial Analysis.\n\nPerform some basic exploratory data analysis to identify key properties of your data.\n\nLet’s start with some descriptive statistics.\n\n\nShow the code\n# Get descriptive statistics\nstats &lt;- psych::describe(merged_df_wide_use)\n\n# convert to dataframe\nstats_df &lt;- as.data.frame(stats)\n\n# Convert row names to a column named 'metric'\nstats_df$metric &lt;- rownames(stats_df)\n\n# Remove row names\nrownames(stats_df) &lt;- NULL\n\n# Reorder columns\nstats_df &lt;- stats_df[, c(\"metric\", setdiff(names(stats_df), \"metric\"))]\n\n\nstats_df2 &lt;- stats_df |&gt;\n  select(-trimmed, -mad, -skew, -kurtosis, -se, -range, -vars) |&gt;\n  filter(metric != \"cal_month\") |&gt;\n  mutate(\n    mean = round(mean, 2),\n    sd = round(sd, 2),\n    median = round(median, 2),\n    min = round(min, 2),\n    max = round(max, 2)\n  )\n\nstats_df3 &lt;- stats_df2 |&gt;\n  gt() |&gt;\n  tab_options(\n    table.font.size = 12,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    metric = \"Metric\",\n    n = \"Count\",\n    mean = \"Mean\",\n    sd = \"St. Dev.\",\n    median = \"Median\",\n    min = \"Min\",\n    max = \"Max\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"lightgrey\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = seq(1, nrow(stats_df2), by = 2)\n    )\n  )\n\nstats_df3\n\n\n\n\n\n\n\n\nMetric\nCount\nMean\nSt. Dev.\nMedian\nMin\nMax\n\n\n\n\ncpi\n193\n0.22\n0.13\n0.20\n-0.14\n0.66\n\n\nwage_growth_pct\n193\n0.03\n0.01\n0.03\n0.01\n0.07\n\n\nus_equities\n193\n205.06\n113.27\n176.34\n54.98\n456.00\n\n\nbonds\n193\n85.53\n12.12\n86.75\n60.80\n106.86\n\n\nshort_term_debt\n193\n110.33\n0.18\n110.28\n109.60\n111.02\n\n\ninternational_equities\n193\n48.60\n6.93\n49.03\n25.53\n64.21\n\n\nus_equities_return_an\n181\n0.11\n0.17\n0.14\n-0.43\n0.56\n\n\nbonds_return_an\n181\n0.03\n0.05\n0.03\n-0.16\n0.14\n\n\nshort_term_debt_return_an\n181\n0.00\n0.00\n0.00\n0.00\n0.01\n\n\ninternational_market_return_an\n181\n0.02\n0.19\n0.02\n-0.53\n0.63\n\n\nus_equities_return_mo\n192\n0.01\n0.05\n0.01\n-0.17\n0.13\n\n\nbonds_return_mo\n192\n0.00\n0.01\n0.00\n-0.04\n0.07\n\n\nshort_term_debt_return_mo\n192\n0.00\n0.00\n0.00\n0.00\n0.01\n\n\ninternational_market_return_mo\n192\n0.00\n0.05\n0.00\n-0.23\n0.15\n\n\n\n\n\n\n\nWe have records of 193 months (~16 years) for 4 ETFs, which serve as proxies for different types of investment, as well as 2 economic indicators measuring CPI and wage growth. We also have monthly and annual returns for different types of investment (~15 years).\n\n\nShow the code\n# create a df with monthly returns on investment\netf_long_mo &lt;- merged_df_long_use |&gt;\n  filter(metric == \"us_equities_return_mo\" | metric == \"bonds_return_mo\" | metric == \"short_term_debt_return_mo\" | metric == \"international_market_return_mo\")\n\n\nggplot(etf_long_mo, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Monthly Returns on Investment\",\n    x = \"Month\",\n    y = \"Monthy Return (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nMonthly returns are very volatile so we will instead examine the plot of annual returns for trends in the data.\n\n\nShow the code\n# create a df with annual returns on investment\netf_long_an &lt;- merged_df_long_use |&gt;\n  filter(metric == \"us_equities_return_an\" | metric == \"bonds_return_an\" | metric == \"short_term_debt_return_an\" | metric == \"international_market_return_an\")\n\n\nggplot(etf_long_an, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Annual Returns on Investment\",\n    x = \"Month\",\n    y = \"Annual Return (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nStock market, either US or International, is the most volatile form of investment whereas short-term debt, while the least risky form of the investment, offers very little, if any, return.\n\n\nShow the code\n# create a df with econ indicators only\nfred_long &lt;- merged_df_long_use |&gt;\n  filter(metric == \"cpi\" | metric == \"wage_growth_pct\")\n\nggplot(fred_long, aes(\n  x = cal_month,\n  y = value,\n  color = metric\n)) +\n  geom_line() +\n  labs(\n    title = \"Economic Indicators\",\n    x = \"Month\",\n    y = \"Change Rate From the Prior Year (Percent)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nIt’s a little disheartening to see how wage growth is lagging behind inflation.\n\n\nShow the code\n# subset the data for correlation matrix - we can drop monthly returns and ETFs values\nmerged_df_wide_use_numeric &lt;- merged_df_wide_use |&gt;\n  select(\n    -cal_month,\n    -us_equities,\n    -bonds,\n    -short_term_debt,\n    -international_equities,\n    -us_equities_return_mo,\n    -bonds_return_mo,\n    -short_term_debt_return_mo,\n    -international_market_return_mo\n  ) |&gt;\n  drop_na()\n\n# corr matrix\ncorr_matrix &lt;- round(cor(merged_df_wide_use_numeric), 2)\n\n# Get upper triangle of the correlation matrix\nget_upper_tri &lt;- function(corr_matrix) {\n  corr_matrix[lower.tri(corr_matrix)] &lt;- NA\n  return(corr_matrix)\n}\n\nupper_tri &lt;- get_upper_tri(corr_matrix)\n\nmelted_corr_matrix &lt;- melt(upper_tri, na.rm = TRUE)\n\nggheatmap &lt;- ggplot(data = melted_corr_matrix, aes(Var2, Var1, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(\n    low = \"red\", high = \"green\", mid = \"white\",\n    midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n    name = \"Pearson\\nCorrelation\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(\n    angle = 45, vjust = 1,\n    size = 8, hjust = 1\n  )) +\n  coord_fixed()\n\nggheatmap +\n  geom_text(aes(Var2, Var1, label = value), color = \"black\", size = 2) +\n  labs(title = \"Correlation Matrix\")\n\n\n\n\n\n\n\n\n\nWage growth rate is highly correlated with CPI and US stock market moves in the same direction as the international market.\n\nTRS\nUnder the TRS plan, employees pay a fixed percentage of their paycheck into the pension fund, and after retirement, the employer (CUNY) continues to pay employees a fraction of their salary until death. For CUNY employees contribution rates are based on the employee’s annual salary and increase as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nHowever, the retirement benefit itself is calculated based on the final average salary of the employee, specifically, on the average salary from the final 3 years of employment as well as the overall tenure:\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\\((35\\% + 2\\% * (N-20)) * \\text{FAS}\\) if \\(N \\geq 20\\)\n\nFirst, we will create salary records. We will increase the salary every year based on corresponding wage growth data to determine final annual salary for the last 3 years of employment, which in turn, will be used to calculate the benefit available for distribution when retirement starts.\nFor a starting salary, we will use $50,000.\n\n# Initialize the starting salary\ninitial_salary &lt;- 50000\n# print(paste(\"starting salary: \", initial_salary))\n\n\n\nShow the code\n# subset data\nmerged_df_wide_use_selected &lt;- merged_df_wide_use |&gt;\n  select(\n    cal_month,\n    wage_growth_pct,\n    cpi,\n    us_equities_return_mo,\n    international_market_return_mo,\n    short_term_debt_return_mo,\n    bonds_return_mo\n  ) |&gt;\n  arrange(cal_month)\n\n\n# create a column with salary\nmerged_df_wide_use_selected$salary_adjusted &lt;- initial_salary\n\n# Loop through the dataset to adjust salary only every September after the 1st year of employment\nfor (i in 2:nrow(merged_df_wide_use_selected)) {\n  # Check if the current month is September\n  if (format(merged_df_wide_use_selected$cal_month[i], \"%m\") == \"09\") {\n    merged_df_wide_use_selected$salary_adjusted[i] &lt;- merged_df_wide_use_selected$salary_adjusted[i - 1] * (1 + merged_df_wide_use_selected$wage_growth_pct[i])\n  } else {\n    # Carry forward the previous salary for non-September months\n    merged_df_wide_use_selected$salary_adjusted[i] &lt;- merged_df_wide_use_selected$salary_adjusted[i - 1]\n  }\n}\n\n# determine contribution rate\n\nmerged_df_wide_use_selected$trs_contribution_rate &lt;- ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 45000, 0.03,\n  ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 55000, 0.035,\n    ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 75000, 0.045,\n      ifelse(merged_df_wide_use_selected$salary_adjusted &lt;= 100000, 0.0575, 0.06)\n    )\n  )\n)\n\n# calculate monthly contribution\nmerged_df_wide_use_selected$monthly_salary &lt;- merged_df_wide_use_selected$salary_adjusted / 12\n\nmerged_df_wide_use_selected$trs_monthly_contribution &lt;- merged_df_wide_use_selected$monthly_salary * merged_df_wide_use_selected$trs_contribution_rate\n\n# merged_df_wide_use_selected |&gt;\n#  summarize(total_trs_contributions = sum(trs_monthly_contribution))\n\n\nAs stated earlier, the starting retirement benefit value is driven by tenure and final salary (FAS), so this is what we will calculate next.\n\n\nShow the code\n# add a year column\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected |&gt;\n  mutate(cal_year = floor_date(cal_month, unit = \"year\"))\n\n# get number of years worked (tenure)\ntenure &lt;- merged_df_wide_use_selected |&gt;\n  summarize(tenure = n_distinct(cal_year)) |&gt;\n  pull(tenure)\n\n## calculate FAS value\n\n# extract the most recent 3 years\nlast_3_years &lt;- max(as.numeric(format(merged_df_wide_use_selected$cal_year, \"%Y\"))) - 2\n\n# filter data for the last 3 years\nfiltered_data &lt;- merged_df_wide_use_selected %&gt;%\n  filter(as.numeric(format(cal_year, \"%Y\")) &gt;= last_3_years)\n\n# get the maximum salary_adjusted for each year\nmax_salary_per_year &lt;- filtered_data %&gt;%\n  group_by(format(cal_year, \"%Y\")) %&gt;%\n  summarize(max_salary = max(salary_adjusted, na.rm = TRUE))\n\n# calculate the average of the maximum salary_adjusted values\nfas_value &lt;- mean(max_salary_per_year$max_salary, na.rm = TRUE)\n\n# create the function to calculate TRS benefit_value\ncalculate_trs_value &lt;- function(tenure, fas_value) {\n  if (tenure &lt; 20) {\n    trs_value &lt;- 0.0167 * fas_value * tenure\n  } else if (tenure == 20) {\n    trs_value &lt;- 0.0175 * fas_value * tenure\n  } else if (tenure &gt; 20) {\n    trs_value &lt;- (0.35 + 0.02 * (tenure - 20)) * fas_value\n  }\n  return(trs_value)\n}\n\ntrs_starting_benefit &lt;- calculate_trs_value(tenure, fas_value)\n\nprint(paste(\"starting salary: \", initial_salary))\n\n\n[1] \"starting salary:  50000\"\n\n\nShow the code\nprint(paste(\"tenure: \", tenure))\n\n\n[1] \"tenure:  17\"\n\n\nShow the code\nprint(paste(\"FAS: \", round(fas_value, 0)))\n\n\n[1] \"FAS:  81567\"\n\n\nShow the code\nprint(paste(\"TRS value in Y1: \", round(trs_starting_benefit, 0)))\n\n\n[1] \"TRS value in Y1:  23157\"\n\n\n\n\nORP\nThe ORP plan is more similar to a 401(k) plan offered by a private employer. The employee and the employer both make contributions to a retirement account which is then invested in the employee’s choice of mutual funds. At retirement, the employee has access to those funds and can choose to withdraw them at any rate desired. Under the ORP, both the employee and the employer make monthly contributions to the employee’s ORP account. These returns are calculated as a percentage of the employee’s annual salary. Specifically, the employee contributes at the same rate as the TRS:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer contribution is fixed at:\n\n8% for the first seven years of employment at CUNY.\n10% for all years thereafter.\n\nContributions are immediately invested according to the asset allocations:\nAs contributions allocations are subject to the participant’s age, we need to define and add the age of the employee upon joining CUNY into the model for OPR. Let’s use 35 as the age upon hire.\n\nage_input &lt;- 35\n# print(paste(\"age when hired at CUNY: \", age_input))\n\nWe will also need to define withdrawal rate to calculate the retirement distribution in the first year. We will use 4%.\n\nwi_rate &lt;- 0.04\n\nNow we calculate contributions under the ORP system and compare the 2 plans.\n\n\nShow the code\n# fill in missing data - since we do not have monthly investment returns for our first month of data, 9/2007,  we will use the following month's values instead.\n\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected %&gt;%\n  mutate(across(everything(), ~ ifelse(is.na(.), lead(.), .))) |&gt;\n  mutate(cal_month = as.Date(cal_month))\n\n# get min cal_month for calculation\nmin_ds &lt;- merged_df_wide_use_selected |&gt;\n  summarize(min_ds = min(cal_month)) |&gt;\n  mutate(min_ds = as.Date(min_ds)) |&gt;\n  pull(min_ds)\n\n# Calculate a proxy for date of birth\ndob &lt;- min_ds - (age_input * 365.25) # Account for leap years\ndob &lt;- format(dob, \"%Y-%m-%d\") # Format as YYYY-MM-DD\n\n# add a column with DOB to df\nmerged_df_wide_use_selected$dob &lt;- dob\n\n# calculate age over time\nmerged_df_wide_use_selected$age_actual &lt;- round(as.numeric(interval(merged_df_wide_use_selected$dob, merged_df_wide_use_selected$cal_month) / years(1)), 0)\n\n#### calculate allocation amounts by employee and CUNY\n\n# create monthly opr_contributions and rates by employee - it's the same as in TRS so we can just copy them over\nmerged_df_wide_use_selected$opr_contribution_rate &lt;- merged_df_wide_use_selected$trs_contribution_rate\nmerged_df_wide_use_selected$opr_contribution_employee &lt;- merged_df_wide_use_selected$trs_monthly_contribution\n\n### calculate contribution rates and amounts by employer/CUNY\n\n# calculate number of years employed for every record we have\n\nmerged_df_wide_use_selected &lt;- merged_df_wide_use_selected %&gt;%\n  mutate(years_employed0 = ceiling(as.numeric(difftime(cal_month, min(cal_month), units = \"days\")) / 365.25))\n\nmerged_df_wide_use_selected$years_employed &lt;- ifelse(merged_df_wide_use_selected$years_employed0 == 0, 1, merged_df_wide_use_selected$years_employed0)\n\n# calculate employer contribution %\n\nmerged_df_wide_use_selected$opr_contribution_rate_cuny &lt;- ifelse(merged_df_wide_use_selected$years_employed0 &lt;= 7, 0.08, 0.1)\n\n# calculate employer monthly contributions\n\nmerged_df_wide_use_selected$opr_contribution_cuny &lt;- merged_df_wide_use_selected$monthly_salary * merged_df_wide_use_selected$opr_contribution_rate_cuny\n\n\n### define contribution allocations\n\n# define allocation for US equity %\n\nmerged_df_wide_use_selected$us_equity_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.54,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.47,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.34,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.19, 0.54)\n    )\n  )\n)\n\n# define allocation for international equity %\n\nmerged_df_wide_use_selected$int_equity_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.36,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.32,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.23,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.13, 0.36)\n    )\n  )\n)\n\n# define allocation for bonds %\n\nmerged_df_wide_use_selected$bonds_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 49, 0.1,\n  ifelse(merged_df_wide_use_selected$age_actual &lt;= 59, 0.21,\n    ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.43,\n      ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.62, 0.1)\n    )\n  )\n)\n\n# define allocation for short term debt  %\n\nmerged_df_wide_use_selected$short_pct &lt;- ifelse(merged_df_wide_use_selected$age_actual &lt;= 74, 0.0,\n  ifelse(merged_df_wide_use_selected$age_actual &gt;= 75, 0.06, 0.06)\n)\n\n\n#### calculate total contribution amounts\n\nmerged_df_wide_use_selected$opr_total_contribution &lt;- merged_df_wide_use_selected$opr_contribution_employee + merged_df_wide_use_selected$opr_contribution_cuny\n\n\n#### calculate investment allocation\n\n# us equity\nmerged_df_wide_use_selected$opr_us_stocks &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$us_equity_pct\n\n# int equity\nmerged_df_wide_use_selected$opr_int_stocks &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$int_equity_pct\n\n# bonds\nmerged_df_wide_use_selected$opr_bond &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$bonds_pct\n\n# short\nmerged_df_wide_use_selected$opr_short &lt;- merged_df_wide_use_selected$opr_total_contribution * merged_df_wide_use_selected$short_pct\n\n# calculate future value of each type of invested contributions\n\nopr_returns &lt;- merged_df_wide_use_selected |&gt;\n  select(\n    cal_month,\n    us_equities_return_mo,\n    international_market_return_mo,\n    short_term_debt_return_mo,\n    bonds_return_mo,\n    opr_us_stocks,\n    opr_int_stocks,\n    opr_bond,\n    opr_short\n  ) |&gt;\n  mutate(net_total_return_us_stocks = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(us_equities_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_int_stocks = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(international_market_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_bonds = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(bonds_return_mo, default = 0))\n  )) |&gt;\n  mutate(net_total_return_short_debt = order_by(\n    desc(cal_month),\n    cumprod(1 + lead(short_term_debt_return_mo, default = 0))\n  ))\n\n\nopr &lt;- opr_returns |&gt;\n  summarize(\n    future_value_us_stocks = sum(opr_us_stocks * net_total_return_us_stocks),\n    future_value_int_stocks = sum(opr_int_stocks * net_total_return_int_stocks),\n    future_value_bonds = sum(opr_bond * net_total_return_bonds),\n    future_value_short_term = sum(opr_short * net_total_return_short_debt)\n  )\n\nopr$future_value_total_opr &lt;- rowSums(opr[, c(\n  \"future_value_us_stocks\",\n  \"future_value_int_stocks\",\n  \"future_value_bonds\",\n  \"future_value_short_term\"\n)])\n\n\n#  ORP withdrawal 1st year\nopr$opr_benefit_1year &lt;- opr$future_value_total_opr * wi_rate\n\n\n\n\nShow the code\n# trs_starting_benefit\norp_1y &lt;- opr$opr_benefit_1year\n\ndf_plans &lt;- data.frame(\n  metric = c(\"orp_1y\", \"trs_1y\"),\n  values = c(orp_1y, trs_starting_benefit)\n)\n\ndf_plans2 &lt;- as.data.frame(\n  pivot_wider(df_plans,\n    names_from = metric,\n    values_from = values\n  )\n)\n\n\ndf_plans2 &lt;- df_plans2 |&gt;\n  mutate(\n    orp_1mo = orp_1y / 12,\n    trs_1mo = trs_1y / 12,\n    delta = ((trs_1mo - orp_1mo) / orp_1mo)\n  )\n\n\ndf_plans2 |&gt;\n  mutate(\n    orp_1y = scales::dollar(orp_1y, accuracy = 1),\n    trs_1y = scales::dollar(trs_1y, accuracy = 1),\n    orp_1mo = scales::dollar(orp_1mo, accuracy = 1),\n    trs_1mo = scales::dollar(trs_1mo, accuracy = 1),\n    delta = scales::percent(delta, accuracy = 0.1)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"TRS vs ORP Comparison\",\n    subtitle = \"Retirement Benefit in Year 1 and Month 1 of Retirement\"\n  ) |&gt;\n  tab_options(\n    table.font.size = 12,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    orp_1y = \"ORP Benefit Year 1\",\n    trs_1y = \"TRS Benefit Year 1\",\n    orp_1mo = \"ORP Benefit Month 1\",\n    trs_1mo = \"TRS Benefit Month 1\",\n    delta = \"Delta\"\n  ) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  ) |&gt;\n  tab_style(\n    style = cell_fill(color = \"lightgrey\"),\n    locations = cells_body(\n      columns = everything(),\n      rows = seq(1, nrow(opr), by = 2)\n    )\n  )\n\n\n\n\n\n\n\n\nTRS vs ORP Comparison\n\n\nRetirement Benefit in Year 1 and Month 1 of Retirement\n\n\nORP Benefit Year 1\nTRS Benefit Year 1\nORP Benefit Month 1\nTRS Benefit Month 1\nDelta\n\n\n\n\n$10,040\n$23,157\n$837\n$1,930\n130.7%\n\n\n\n\n\n\n\nUnder the 4% withdrawal rule, the TRS plan would provide 2.3X times more benefit than the ORP plan (under 4% withdrawal rule) when retirement starts, which emphasizes the importance of having additional income sources for a given ORP participant. However, even the larger TRS benefit is likely not enough to cover the living expenses in retirement if it’s the only source of income.\n\n# life expectancy\nlife_exp &lt;- 75\n\n# create a variable for age at retirement\nage_retire &lt;- max(merged_df_wide_use_selected$age_actual)\n\nprint(paste(\"Age at Retirement: \", age_retire))\n\n[1] \"Age at Retirement:  51\"\n\n\n\n# calculate number of years in retirement\nyears_in_retirement &lt;- life_exp - age_retire\n\n\nprint(paste(\"Years in Retirement: \", years_in_retirement))\n\n[1] \"Years in Retirement:  24\"\n\n\n\n# long term avg cpi from task 4\nstats_df4 &lt;- as.data.frame(stats_df3)\nlong_term_avg_cpi &lt;- as.numeric(stats_df4$mean[1])\n\nprint(paste(\"Long-term CPI change: \", round(long_term_avg_cpi, 2)))\n\n[1] \"Long-term CPI change:  0.22\"\n\n\nNow that we defined all inputs, we can calculate the TRS benefit for each year in retirement.\n\n\nShow the code\n# create a df for inflation adjustment of TRS benefit\n\ntrs_var &lt;- as.numeric(df_plans2$trs_1y[1])\n\ntrs_df1 &lt;- data.frame(\n  cpi_reported = long_term_avg_cpi, # annual inflation adjustment\n  cpi_adj_rate = round(long_term_avg_cpi / 2, 1), # annual change\n  annual_trs_benefit = trs_var, # annual TRS benefit to be adjusted\n  years_in_retirement = 1:years_in_retirement # payments in retirement\n  #    trs_benefit_adj_an= trs_var*long_term_avg_cpi, #adjusted annual trs benefit\n  #    trs_benefit_adj_mo= trs_var*long_term_avg_cpi/12 #adjusted monthly trs benefit\n)\n\n\n# determine the size of the annual TRS adjustment\n\ntrs_df1$trs_adjustment_rate &lt;- ifelse(trs_df1$cpi_adj_rate &lt; 0.01, 0.01,\n  ifelse(merged_df_wide_use_selected$age_actual &gt; 0.03, 0.03, trs_df1$cpi_adj_rate)\n)\n\n# calculate net annual increases and adjusted annual benefit\ntrs_ann_adjustment_df &lt;- trs_df1 |&gt;\n  select(years_in_retirement, annual_trs_benefit, trs_adjustment_rate) |&gt;\n  mutate(net_total_return = order_by(\n    desc(years_in_retirement),\n    cumprod(1 + lead(trs_adjustment_rate, default = 0))\n  )) |&gt;\n  mutate(net_total_return_adj = rev(net_total_return)) |&gt;\n  mutate(trs_annual_benefit_adj = net_total_return_adj * annual_trs_benefit)\n\n\n\n# trs_plot_df1&lt;-trs_ann_adjustment_df |&gt;\n#  select(years_in_retirement,trs_annual_benefit_adj) |&gt;\n#  filter(years_in_retirement==1|years_in_retirement==max(years_in_retirement))\n\n# select vars for plotting\n\ntrs_plot1 &lt;- plot_ly(trs_ann_adjustment_df,\n  x = ~years_in_retirement, y = ~trs_annual_benefit_adj,\n  type = \"scatter\",\n  mode = \"lines+markers+text\",\n  text = ~ paste0(\"$\", round(trs_annual_benefit_adj, 0)), \n  textfont = list(size = 8),\n  textposition = \"top center\"\n) |&gt;\n  layout(\n    title = \"TRS Annual Benefit Adjusted for Inflation\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(\n      title = \"Annual Benefit ($)\",\n      tickformat = \"$,.0f\"\n    ),\n    legend = list(\n      orientation = \"h\", \n      x = 0.5, \n      xanchor = \"center\", \n      y = -0.2, \n      font = list(size = 8) \n    )\n  )\n\n\ntrs_plot1\n\n\n\n\n\n\nORP plan\nTo project the value of the ORP benefit distribution, we need to know the following:\n\nThe total value of ORP investment in the first year of retirement. We will use this to calculate amounts allocated to different types of investment going forward\nLong-run average return rates on different types of investment. We will use this to simulate investment portfolio growth over time in retirement.\nThe age at which the employee retired from CUNY. We need this to determine the allocation between different types of investment in 1st year of retirement\nAge of the retiree in each year of retirement. We need this to determine the allocation between different types of investment going forward.\nNumber of years in retirement. We need this to determine the number of projected periods.\nAnnual withdrawal rate. We need this to adjust ORP amount available every next year; here, we will use 4%.\n\nWe will start with consolidating records for the 1st year of retirement as all of these variables have already been calculated.\n\nyears_in_retirement\n\n[1] 24\n\n\n\nage_retire\n\n[1] 51\n\n\n\n\nShow the code\n# create an initial df\nopr_ret_df &lt;- data.frame(\n  years_in_retirement = 1:years_in_retirement,\n  age_when_retired = age_retire,\n  orp_total_y1 = opr$future_value_total_opr[1],\n  wrate = wi_rate\n)\n\n# add actual age calc\nopr_ret_df$age_actual &lt;- opr_ret_df$age_when_retired + opr_ret_df$years_in_retirement - 1\n\n# add us equity allocation%\nopr_ret_df$us_equity_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.54,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.47,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.34,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.19, 0.54)\n    )\n  )\n)\n\n# define allocation for international equity %\n\nopr_ret_df$int_equity_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.36,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.32,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.23,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.13, 0.36)\n    )\n  )\n)\n\n# define allocation for bonds %\n\nopr_ret_df$bonds_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 49, 0.1,\n  ifelse(opr_ret_df$age_actual &lt;= 59, 0.21,\n    ifelse(opr_ret_df$age_actual &lt;= 74, 0.43,\n      ifelse(opr_ret_df$age_actual &gt;= 75, 0.62, 0.1)\n    )\n  )\n)\n\n# define allocation for short term debt  %\n\nopr_ret_df$short_pct &lt;- ifelse(opr_ret_df$age_actual &lt;= 74, 0.0,\n  ifelse(opr_ret_df$age_actual &gt;= 75, 0.06, 0.06)\n)\n\n## create lon-run avg variables\n\nus_return &lt;- stats_df2 |&gt;\n  filter(metric == \"us_equities_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nint_return &lt;- stats_df2 |&gt;\n  filter(metric == \"international_market_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nbonds_return &lt;- stats_df2 |&gt;\n  filter(metric == \"bonds_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\nshort_return &lt;- stats_df2 |&gt;\n  filter(metric == \"short_term_debt_return_an\") |&gt;\n  select(mean) |&gt;\n  pull(mean)\n\n## add long-run rates to df\nopr_ret_df$us_return &lt;- us_return\nopr_ret_df$int_return &lt;- int_return\nopr_ret_df$bonds_return &lt;- bonds_return\nopr_ret_df$short_return &lt;- short_return\n\n\n\n## initialize columns\nopr_ret_df$ann_payment &lt;- NA\nopr_ret_df$orp_total_after_deduction &lt;- NA\nopr_ret_df$us_inv &lt;- NA\nopr_ret_df$int_inv &lt;- NA\nopr_ret_df$bonds_inv &lt;- NA\nopr_ret_df$short_inv &lt;- NA\nopr_ret_df$us_inv_growth &lt;- NA\nopr_ret_df$bonds_inv_growth &lt;- NA\nopr_ret_df$short_inv_growth &lt;- NA\nopr_ret_df$opr_total_eoy &lt;- NA\n\n# calculate annual growth after distributions\n\nfor (i in 1:nrow(opr_ret_df)) {\n  if (opr_ret_df$years_in_retirement[i] == 1) {\n    # first year\n    opr_ret_df$ann_payment[i] &lt;- opr_ret_df$wrate[i] * opr_ret_df$orp_total_y1[i]\n    opr_ret_df$orp_total_after_deduction[i] &lt;- opr_ret_df$orp_total_y1[i] - opr_ret_df$ann_payment[i]\n    opr_ret_df$us_inv[i] &lt;- opr_ret_df$us_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$int_inv[i] &lt;- opr_ret_df$int_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$bonds_inv[i] &lt;- opr_ret_df$bonds_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$short_inv[i] &lt;- opr_ret_df$short_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$us_inv_growth[i] &lt;- (1 + opr_ret_df$us_return[i]) * opr_ret_df$us_inv[i]\n    opr_ret_df$int_inv_growth[i] &lt;- (1 + opr_ret_df$int_return[i]) * opr_ret_df$int_inv[i]\n    opr_ret_df$bonds_inv_growth[i] &lt;- (1 + opr_ret_df$bonds_return[i]) * opr_ret_df$bonds_inv[i]\n    opr_ret_df$short_inv_growth[i] &lt;- (1 + opr_ret_df$short_return[i]) * opr_ret_df$short_inv[i]\n    opr_ret_df$opr_total_eoy[i] &lt;- opr_ret_df$us_inv_growth[i] + opr_ret_df$int_inv_growth[i] + opr_ret_df$bonds_inv_growth[i] + opr_ret_df$short_inv_growth[i]\n  } else {\n    # years 2 and after\n    opr_ret_df$ann_payment[i] &lt;- opr_ret_df$wrate[i] * opr_ret_df$opr_total_eoy[i - 1]\n    opr_ret_df$orp_total_after_deduction[i] &lt;- opr_ret_df$opr_total_eoy[i - 1] - opr_ret_df$ann_payment[i]\n    opr_ret_df$us_inv[i] &lt;- opr_ret_df$us_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$int_inv[i] &lt;- opr_ret_df$int_equity_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$bonds_inv[i] &lt;- opr_ret_df$bonds_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$short_inv[i] &lt;- opr_ret_df$short_pct[i] * opr_ret_df$orp_total_after_deduction[i]\n    opr_ret_df$us_inv_growth[i] &lt;- (1 + opr_ret_df$us_return[i]) * opr_ret_df$us_inv[i]\n    opr_ret_df$int_inv_growth[i] &lt;- (1 + opr_ret_df$int_return[i]) * opr_ret_df$int_inv[i]\n    opr_ret_df$bonds_inv_growth[i] &lt;- (1 + opr_ret_df$bonds_return[i]) * opr_ret_df$bonds_inv[i]\n    opr_ret_df$short_inv_growth[i] &lt;- (1 + opr_ret_df$short_return[i]) * opr_ret_df$short_inv[i]\n    opr_ret_df$opr_total_eoy[i] &lt;- opr_ret_df$us_inv_growth[i] + opr_ret_df$int_inv_growth[i] + opr_ret_df$bonds_inv_growth[i] + opr_ret_df$short_inv_growth[i]\n  }\n}\n\n## subset relevant records from ORP projections\nopr_comp_selected &lt;- opr_ret_df |&gt;\n  select(years_in_retirement, opr_total_eoy, ann_payment)\n## join with TRS forecast\ntrs_comp_selected &lt;- trs_ann_adjustment_df |&gt;\n  select(years_in_retirement, trs_annual_benefit_adj)\n\nret_plans_comp &lt;- inner_join(opr_comp_selected,\n  trs_comp_selected,\n  by = c(\"years_in_retirement\" = \"years_in_retirement\")\n)\n\n\nret_plans_comp &lt;- ret_plans_comp |&gt;\n  rename(\n    orp_available_investment = opr_total_eoy,\n    orp_annual_benefit = ann_payment,\n    trs_annual_benefit = trs_annual_benefit_adj\n  ) |&gt;\n  mutate(\n    trs_annual_benefit = round(trs_annual_benefit, 0),\n    orp_annual_benefit = round(orp_annual_benefit, 0),\n    orp_available_investment = round(orp_available_investment, 0)\n  ) |&gt;\n  mutate(\n    trs_monthly_benefit = round(trs_annual_benefit / 12, 0),\n    orp_monthly_benefit = round(orp_annual_benefit / 12, 0),\n    delta = round((trs_monthly_benefit - orp_monthly_benefit) / orp_monthly_benefit, 2)\n  )\n\n\n\n# Create the plot\nfig1 &lt;- plot_ly(ret_plans_comp) %&gt;%\n  # ORP Available Investment\n  add_bars(\n    x = ~years_in_retirement,\n    y = ~orp_available_investment,\n    name = \"ORP Available Investment\",\n    marker = list(color = \"lightgrey\", opacity = 0.5)\n  ) %&gt;%\n  # ORP Annual Benefit\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~orp_annual_benefit,\n    name = \"ORP Annual Benefit\",\n    line = list(color = \"darkblue\")\n  ) %&gt;%\n  # TRS Annual Benefit\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~trs_annual_benefit,\n    name = \"TRS Annual Benefit\",\n    line = list(color = \"red\")\n  ) %&gt;%\n  layout(\n    title = \"Comparison of ORP and TRS Annual Benefits\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(title = \"Available Funds($)\"),\n    barmode = \"overlay\", # Bars and lines on the same plot\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.2)\n  )\n\n# Display the plot\nfig1\n\n\n\n\n\n\n\n\nShow the code\nret_plans_comp &lt;- ret_plans_comp %&gt;%\n  mutate(delta_percent = delta * 100)\n\n# Create the plot\nfig2 &lt;- plot_ly(ret_plans_comp) %&gt;%\n  add_lines(\n    x = ~years_in_retirement,\n    y = ~delta_percent,\n    name = \"Delta b/w ORP and TRS Benefit\",\n    line = list(color = \"black\", dash = \"dot\"),\n    text = ~ paste0(round(delta_percent, 2), \"%\"),\n    textposition = \"top center\",\n    mode = \"lines+markers+text\"\n  ) %&gt;%\n  layout(\n    title = \"Delta between ORP and TRS Benefits\",\n    xaxis = list(title = \"Years in Retirement\"),\n    yaxis = list(title = \"Percentage Difference(%)\"),\n    legend = list(orientation = \"h\", x = 0.5, xanchor = \"center\", y = -0.2)\n  )\n\n# Display the plot\n#fig2\n\n\nUnder the existing assumptions, the TRS plan participant will receive a larger annual retirement benefit but the ORP plan participant will have a substantial amount of money left after death.\n\n\n\nTask 7.\n\nUsing your historical data, generate several (at least 200) “bootstrap histories” suitable for a Monte Carlo analysis. Use bootstrap sampling, i.e. sampling with replacement, to generate values for both the “while working” and “while retired” periods of the model; you do not need to assume constant long-term average values for the retirement predictions any more.\n\nTo check the validity of assumptions made in terms of long-run averages, we can resample the historical data and build 95% confidence intervals for the true mean of each distribution.\nWage Growth\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = wage_growth_pct) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the Code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0318   0.0349\n\n\nWe are 95% confident that the mean wage growth % is between 0.031 and 0.035, which means we were a bit too conservative with our assumption of long-run average of 0.03 based on historical data.\nCPI\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = cpi) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.199    0.235\n\n\nWe are 95% confident that the mean annual inflation is between 0.19 and 0.23, which means we our assumption of long-run average of 0.22 was reasonable.\nUS Stocks Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = us_equities_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0845    0.135\n\n\nWe are 95% confident that the mean US stock market return is between 0.08 and 0.13, which means we our assumption of long-run average of 0.11 was reasonable.\nIntl Stocks Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = international_market_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1  -0.0152   0.0440\n\n\nWe are 95% confident that the mean Intl stock market return is between -0.01 and 0.05, which means we our assumption of long-run average of 0.03 was reasonable.\nBonds Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = bonds_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   0.0191   0.0336\n\n\nWe are 95% confident that the mean bonds investment return is between 0.019 and 0.033, which means we our assumption of long-run average of 0.03 was reasonable.\nShort-Term Debt Return\n\n\nShow the code\nbootstrap_distribution &lt;- merged_df_wide_use %&gt;%\n  specify(response = short_term_debt_return_an) %&gt;%\n  generate(reps = 200) %&gt;%\n  calculate(stat = \"mean\")\n\n# bootstrap_distribution\n\npercentile_ci &lt;- bootstrap_distribution %&gt;%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nvisualize(bootstrap_distribution) +\n  shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\npercentile_ci\n\n\n# A tibble: 1 × 2\n   lower_ci upper_ci\n      &lt;dbl&gt;    &lt;dbl&gt;\n1 -0.000130 0.000386\n\n\nWe are 95% confident that the mean short-term debt investment return is between -0.0001 and 0.0003, which means we our assumption of long-run average of 0.00 was reasonable."
  },
  {
    "objectID": "for_course_project_v2.html",
    "href": "for_course_project_v2.html",
    "title": "What Areas of New York Are Most Affected by Unemployment?",
    "section": "",
    "text": "This report discusses one of the specific analytical questions that were analyzed in the group course project by our team, Apple Watch, and presented on December 11, 2024. The presentation is available upon request and the summary report can be accessed at this link. For this analysis, we will look at data from 2011 to 2022 due to availability of specific metrics. We will look at unemployment in NYC on the county- and zipcode levels. As in any data analysis project, we start with obtaining and preparing the data. We will use Census US and Fred databases and we will do that using tidycensus and fredr packages, respectively. These packages were specifically designed for the purposes of facilitating access to data via API. To execute the code in this analysis, API keys for Census and FRED are required and should be requested in advance - please refer to the notes in the code. We will also obtain and prepare SHAPE files for plotting and put together a mapping file for zip codes and counties.\nCode - installing and loading libraries\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tidycensus\")) install.packages(\"tidycensus\")\nlibrary(tidycensus)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"readr\")) install.packages(\"readr\")\nlibrary(readr)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tmap\")) install.packages(\"tmap\")\nlibrary(tmap)\nif (!require(\"knitr\")) install.packages(\"knitr\")\nlibrary(knitr)\nif (!require(\"ggridges\")) install.packages(\"ggridges\")\nlibrary(ggridges)\nif (!require(\"scales\")) install.packages(\"scales\")\nlibrary(scales)\nCode - Read in API keys\n# Load the FRED API key from a local file - Please request it in advance.\nfred_api_key &lt;- readLines(\"fred.txt\")\nfredr_set_key(fred_api_key)\n\n# You also need to initialize the Census API key - it's read in only once the very first time it's used so I commented this line out. Please use your own Census API key if needed.\n# census_api_key(\"xxxxx\", install = TRUE)\nCode - Getting the SHAPE files for NYC counties\n## counties\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\",\n              method=\"curl\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_county_sf &lt;- read_sf(fname_shp)\n#head(nyc_county_sf)\nCode - Getting the SHAPE files for NYC zip codes\nif(!file.exists(\"zipcodes_maps.zip\")){\n    download.file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\", \n              destfile=\"zipcodes_maps.zip\",\n              method=\"curl\")\n}\n\n\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"zipcodes_maps.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_zip_codes_sf &lt;- read_sf(fname_shp)\nCode - Get a mapping file for NYC Zip Codes and Counties\n# here's a list of NYC zipcodes \nurl &lt;- \"https://raw.githubusercontent.com/erikgregorywebb/nyc-housing/master/Data/nyc-zip-codes.csv\"\n\nresponse &lt;- request(url) |&gt;\n  req_perform()\n\nfile_path &lt;- \"nyc-zip-codes.csv\"\n\nwriteBin(response$body, file_path)\nnyc_zip_codes_list &lt;- read_csv(file_path)\n\nnyc_zip_codes_list&lt;-nyc_zip_codes_list|&gt;\n  mutate(zcta=as.character(ZipCode))\n\nnyc_zip_codes_sf2&lt;-inner_join(nyc_zip_codes_sf,\n                              nyc_zip_codes_list,by=c(\"GEOID10\"=\"zcta\")\n                              ) |&gt;\n  select(GEOID10,Borough,Neighborhood,geometry)\n\n#head(nyc_zip_codes_sf2)\nCode - Initialize tmap plotting mode\n# it needs to be done only once hence it's listed separately\ntmap_mode(\"plot\")\nCode - Getting FRED annual unemployment data for NYC counties\n# Search for unemployment rate data series for NY counties\nsearch_results &lt;- fredr_series_search_text(\"unemployment rate county\") \n\n# Filter for NY counties\nny_counties &lt;- search_results |&gt;\n  filter(grepl(\", NY$\", title, ignore.case = TRUE))  \n\n# Display the list of NY counties and their series IDs\nny_counties_list &lt;- ny_counties |&gt;\n  select(id, title)\nprint(ny_counties_list)\n\n\n# A tibble: 61 × 2\n   id                    title                                      \n   &lt;chr&gt;                 &lt;chr&gt;                                      \n 1 NYBRON5URN            Unemployment Rate in Bronx County, NY      \n 2 LAUCN360050000000003A Unemployment Rate in Bronx County, NY      \n 3 NYNEWY1URN            Unemployment Rate in New York County, NY   \n 4 LAUCN360610000000003A Unemployment Rate in New York County, NY   \n 5 NYKING7URN            Unemployment Rate in Kings County, NY      \n 6 LAUCN360470000000003A Unemployment Rate in Kings County, NY      \n 7 NYWEST9URN            Unemployment Rate in Westchester County, NY\n 8 LAUCN361190000000003A Unemployment Rate in Westchester County, NY\n 9 NYSUFF0URN            Unemployment Rate in Suffolk County, NY    \n10 NYQUEE1URN            Unemployment Rate in Queens County, NY     \n# ℹ 51 more rows\n\n\nCode - Getting FRED annual unemployment data for NYC counties\n# Fetch data for all NY counties\nny_unemployment_data &lt;- lapply(ny_counties$id, function(series_id) {\n  data &lt;- fredr(\n    series_id = series_id,\n    observation_start = as.Date(\"2010-12-31\"),  # Start date\n    observation_end = as.Date(\"2023-12-31\") ,    # End date\n  frequency         = \"a\",           # \"a\" for annual\n    aggregation_method = \"avg\"   \n    )\n  data$county &lt;- ny_counties$title[ny_counties$id == series_id]  # Add county name\n  return(data)\n}) |&gt;\n  bind_rows()  \n\n#### filter to NYC counties\n\nlist_of_ny_counties&lt;-c(\"New York\",\"Bronx\",\"Kings\",\"Queens\",\"Richmond\")\nunrate&lt;-\"Unemployment Rate in \"\nny_var&lt;-\" County, NY\"\n\nny_list &lt;- rep(ny_var, 5)\nunrate_list&lt;-rep(unrate,5)\n\n\nny_combined_list &lt;- mapply(function(x, y,z) paste(x, y,z, sep = \"\"), unrate_list,\n                           list_of_ny_counties,ny_list,SIMPLIFY = FALSE)\n\n# Filter \nny_unrate_county_filtered &lt;- ny_unemployment_data |&gt; \n  filter(county %in% ny_combined_list)\n\nny_unrate_county_filtered&lt;-ny_unrate_county_filtered |&gt;\n  mutate(county2=sub(\"Unemployment Rate in \", \"\", county),\n         county3=sub(\", NY\",\"\",county2)) |&gt;\n  select(date,value,county3)\n\n\n### there are several instances of multiple rates for the same county/year - we'll take avg\n\nny_unrate_county_filtered2&lt;-ny_unrate_county_filtered |&gt;\n  group_by(county3,date) |&gt;\n  summarise(unrate=mean(value)) |&gt;\n  ungroup()\n\nnyc_county_unrate_annual&lt;-ny_unrate_county_filtered2 |&gt;\n  mutate(year = year(as.Date(date))) |&gt;\n  mutate(county_name = case_when(\n    county3 == \"New York County\" ~ \"Manhattan\",\n    county3 == \"Bronx County\" ~ \"Bronx\",\n    county3 == \"Kings County\" ~ \"Brooklyn\",\n    county3 == \"Queens County\" ~ \"Queens\",\n    county3 == \"Richmond County\" ~ \"Staten Island\",\n    TRUE ~ \"Unknown\"\n  )) \n\nhead(nyc_county_unrate_annual)\n\n\n# A tibble: 6 × 5\n  county3      date       unrate  year county_name\n  &lt;chr&gt;        &lt;date&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 Bronx County 2010-01-01   12    2010 Bronx      \n2 Bronx County 2011-01-01   12    2011 Bronx      \n3 Bronx County 2012-01-01   12.5  2012 Bronx      \n4 Bronx County 2013-01-01   11.7  2013 Bronx      \n5 Bronx County 2014-01-01    9.5  2014 Bronx      \n6 Bronx County 2015-01-01    7.6  2015 Bronx\nCode - Unemployment Trend in NYC Counties\npalette_5counties&lt;-c(\n  \"dodgerblue2\",\"green4\",\"black\",\"orange\",\"#E31A1C\"\n)\n\n# create a chart\n\nnyc_unrate_annual2011&lt;-nyc_county_unrate_annual|&gt;\n  filter(year&gt;=2011&year&lt;=2022)|&gt;\n  mutate()\n\nnyc_annual_unrate_year_plot &lt;- plot_ly(nyc_unrate_annual2011,\n  x = ~year, y = ~unrate,\n  color = ~county_name,\n  type = \"scatter\",\n  mode = \"lines\",\n  colors = palette_5counties\n) |&gt;\n  layout(\n    title = \"Unemployment Rate by NYC Borough\",\n    xaxis = list(\n      title = \"Year\"\n   #    showgrid = FALSE\n      ),\n    yaxis = list(\n      title = \"Unemployment Rate, %\"\n    #   showgrid = FALSE\n      #,\n  #    tickformat = \".0%\",\n  #    range = c(0, 1)\n  \n    ),\n    legend = list(\n      orientation = \"h\", # Horizontal legend\n      x = 0.5, # Center horizontally\n      xanchor = \"center\", # Align center\n      y = -0.2, # Position below the plot\n      font = list(size = 8) # Smaller font size\n    )\n  )\n\n# Show the plot\n\nnyc_annual_unrate_year_plot\nUnemployment rates across NYC largely follow the same trend as the rest of the US, peaking during uncertain and turbulent times. During the Covid pandemic, the already higher trending unemployment in the Bronx was 3 times higher yet compared to the pre-pandemic levels. But even Manhattan, a much more advantaged area, experienced a significant spike (+2X) in unemployment rate.\nCode - Animated plot of annual unemployment in NYC counties\n###  create a df for plotting\n\ntmap_nyc_county_unrate_annual&lt;-left_join(nyc_county_unrate_annual,\n                                         nyc_county_sf,\n                                         by=c(\"county_name\"=\"boro_name\"))\n\ntmap_nyc_county_unrate_annual2&lt;-tmap_nyc_county_unrate_annual|&gt;\n  select(year,unrate,county_name,geometry) \n\n  tmap_nyc_county_unrate_annual2$label&lt;-paste(tmap_nyc_county_unrate_annual2$county_name,\n                                              tmap_nyc_county_unrate_annual2$unrate,\n                                              sep=\": \"\n                                              )\n\n\n### join maps data for tmap\n\nmap &lt;- tmap_nyc_county_unrate_annual2 |&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"unrate\", \n              title = \"Unemployment Rate\", \n              palette = \"Greys\",  # Adjust palette as desired\n              style = \"cont\") +   # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")+\n  tm_layout(frame = FALSE)\n\ninvisible(capture.output({tmap_animation(map, \n               filename = \"nyc_unemployment2.gif\", \n               width = 800, \n               height = 600, \n               delay = 60) # Adjust delay as desired\n}))\n\nknitr::include_graphics(\"nyc_unemployment2.gif\")\nThere are observable differences in unemployment across boroughs, with Bronx rates consistently trending higher than in other boroughs. But NYC boroughs are populous and large. For example, Brooklyn alone is the fourth most populous city not even in New York state but in the entire country. It’s not only big - it’s also very diverse. Within each borough, there are multiple neighborhoods that greatly differ from each other in a number of ways.\nCode - Getting data for unemployment in NYC zip codes\n## loop for all years - available from 2011 to 2022\nyears &lt;- lst(2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022)\n\n#list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes_list$ZipCode)) \n\n# Initialize an empty list to store results\nresults  &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)  \n  \n  tryCatch({\n    # Fetch ACS data for the current year\n    data &lt;- get_acs(\n      geography = \"zcta\",  \n      variables = c(\n        unemployed = \"B23025_005E\",  # Civilian labor force: Unemployed\n        labor_force = \"B23025_003E\",  # Total civilian labor force\n        total_population_16plus = \"B23025_001E\",  # Total population 16+\n        employed = \"B23025_004E\",  # Civilian labor force: Employed\n      median_income=\"B19013_001E\"\n        ),\n      year = year,\n      survey = \"acs5\"  # 5-year estimates\n    )\n    \n    # Store the data in the results list\n    results[[as.character(year)]] &lt;- data\n  }, error = function(e) {\n    message(\"Error fetching data for year: \", year, \": \", e$message)\n  })\n}\n\n# Combine all results into a single dataframe\ncombined_data &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data$zcta&lt;-gsub(\"ZCTA5 \",\"\",combined_data$NAME)\n#head(combined_data)\n\nnyc_zip_codes&lt;-nyc_zip_codes_list |&gt;\n  mutate(zcta=as.character(ZipCode))\n\ncombined_data_nyc&lt;-inner_join(combined_data,nyc_zip_codes_list, by=c(\"zcta\"=\"zcta\"))\n\ncombined_data_nyc2 &lt;- combined_data_nyc |&gt;\n      mutate(variable2 = recode(variable,\n                               \"B23025_005\" = \"unemployed\",\n                               \"B23025_003\" = \"labor_force\",\n                               \"B23025_001\" = \"total_population_16plus\",\n                               \"B23025_004\" = \"employed\",\n                               \"B19013_001\"=\"median_income\"\n                               ))\n\n## create a df with unemployed totals only\nunemployed_df&lt;-combined_data_nyc2 |&gt;\n  filter(variable2=='unemployed') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(unemployed=estimate)\n\n## create a df with labor force total only\nlaborforce_df&lt;-combined_data_nyc2 |&gt;\n  filter(variable2=='labor_force') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(laborforce=estimate)\n\n## create a df with median income total only\nmedian_income_df&lt;-combined_data_nyc2 |&gt;\n  filter(variable2=='median_income') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(median_income=estimate)\n\nnyc_unrate_zip_df&lt;-inner_join(unemployed_df,\n                              laborforce_df,\n                              by=c(\"year\"=\"year\",\"zcta\"=\"zcta\"))\n\nnyc_unrate_inc_zip_df&lt;-inner_join(nyc_unrate_zip_df,\n                              median_income_df,\n                              by=c(\"year\"=\"year\",\"zcta\"=\"zcta\"))\n\nnyc_unrate_inc_zip_df&lt;-nyc_unrate_inc_zip_df |&gt;\n  mutate(unrate=100*round(unemployed/laborforce,2), na.rm = TRUE)\n\nnyc_unrate_inc_zip_df &lt;- na.omit(nyc_unrate_inc_zip_df)\n\n\n## join zip code data and zip sf file\n\nzip_unrate_inc_for_sf&lt;-left_join(nyc_unrate_inc_zip_df,\n                             nyc_zip_codes_sf2,\n                             by=c(\"zcta\"=\"GEOID10\") )\n\n\ntmap_zip_unrate_inc_for_sf&lt;-zip_unrate_inc_for_sf|&gt;\n  select(year,unrate,median_income,zcta, Neighborhood,Borough,geometry)\nCode - plotting unemployment over time in NYC zipcodes\n### join maps data for tmap\nmap &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"unrate\", \n              title = \"Unemployment Rate\", \n              palette = \"Greys\",  # Adjust palette as desired\n              style = \"cont\",\n              fill.na = \"red\") +   # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_layout(frame = FALSE)\n # tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")\n\ninvisible(capture.output({tmap_animation(map, \n               filename = \"nyc_zip_unrate.gif\", \n               width = 800, \n               height = 600, \n               delay = 60) \n}))\n\nknitr::include_graphics(\"nyc_zip_unrate.gif\")\n####avg avg unrate by zip/borough\nCode - Average unemployment rates for zip codes\ntemp_df1&lt;-tmap_zip_unrate_inc_for_sf|&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  group_by(Borough,Neighborhood) |&gt;\n  summarise(avg_unrate_zip=mean(unrate))|&gt;\n  ungroup()\n\ntemp_df2&lt;-tmap_zip_unrate_inc_for_sf|&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  group_by(Borough) |&gt;\n  summarise(avg_unrate=mean(unrate))|&gt;\n  ungroup()\n\ntemp_df3&lt;-left_join(temp_df1,temp_df2,by=c(\"Borough\"=\"Borough\"))\n\ntemp_df3 &lt;-temp_df3 |&gt;\n  mutate(delta=round((avg_unrate_zip-avg_unrate),2)\n         )\n\ntemp_df4&lt;-sqldf(\n  \"\n  with a as (\n  select t.*,\n  row_number() over(partition by Borough order by delta asc ) as rn_min,\n  row_number() over(partition by Borough order by delta desc ) as rn_max\n  from temp_df3 t\n  )\n  \n  select Borough,\n  Neighborhood,\n  avg_unrate,\n  avg_unrate_zip,\n  delta\n  from a\n  where 1=1\n  and (rn_min=1 or rn_max=1)\n  \n  ;\n  \"\n)\n\n\ntemp_df4 |&gt;\n  mutate(avg_unrate=round(avg_unrate,2),\n         avg_unrate_zip=round(avg_unrate_zip,2)\n         ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Neighborhoods with Smallest and Largest Unemployment Rates\",\n    subtitle=\"2011-2022\"\n  ) |&gt;\ntab_options(\n    table.font.size = 8,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    avg_unrate = \"Borough Rate\",\n    avg_unrate_zip= \"Neighborhood Rate\",\n    delta = \"Delta from Borough Average (pp)\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\n\nNeighborhoods with Smallest and Largest Unemployment Rates\n\n\n2011-2022\n\n\nBorough\nNeighborhood\nBorough Rate\nNeighborhood Rate\nDelta from Borough Average (pp)\n\n\n\n\nBronx\nKingsbridge and Riverdale\n11.87\n8.12\n-3.74\n\n\nBronx\nCentral Bronx\n11.87\n15.67\n3.80\n\n\nBrooklyn\nGreenpoint\n8.63\n5.88\n-2.76\n\n\nBrooklyn\nBushwick and Williamsburg\n8.63\n10.97\n2.34\n\n\nManhattan\nUpper East Side\n6.80\n4.18\n-2.62\n\n\nManhattan\nCentral Harlem\n6.80\n11.57\n4.76\n\n\nQueens\nNortheast Queens\n7.77\n5.85\n-1.91\n\n\nQueens\nJamaica\n7.77\n11.20\n3.43\n\n\nStaten Island\nSouth Shore\n6.25\n5.45\n-0.80\n\n\nStaten Island\nPort Richmond\n6.25\n7.17\n0.92\nUnemployment rate in Greenpoint, one of the more affluent areas in Brooklyn, is almost 3 percentage points lower than the average for the county, whereas in Bushwick and Williamsburg it’s 2 percentage points above the average. And we see this in each and every borough: 1) in the Bronx, unemployment rate in Central Bronx is 3.8 percentage points higher and in Kingsbridge and Riverdale - 3.7 percentage points lower than the county average rate of 12%. 2) in Queens, unemployment rate in Central Bronx is 3.8 percentage points higher and in Kingsbridge and Riverdale - 3.7 percentage points lower than the county average rate of 8%. 3) in Manhattan,unemployment rate in Central Harlem is 4.76 percentage points higher and in Upper East Side - 2.62 percentage points lower than the county average rate of 7%. 4) in Staten Island, unemployment rate in Port Richmond is 0.9 percentage points higher and in South Shore - 0.8 percentage points lower than the county average rate of 6%.\nThese differences could be attributed to a number of factors, most of which could be traced back to characteristics of population within specific neighborhoods. While this analysis is outside of the scope of this specific question, we can take a look at median income by zip codes to see how financial wellbeing changes across the city and within counties.\nCode - Plotting median income in NYC zip codes\n### join maps data for tmap\n\nmap &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"median_income\", \n              title = \"Median Income\", \n              palette = \"Greens\",  # Adjust palette as desired\n              style = \"cont\",\n              fill.na = \"red\") +   # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_layout(frame = FALSE)\n # tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")\n\ninvisible(capture.output({tmap_animation(map, \n               filename = \"nyc_zip_median_income.gif\", \n               width = 800, \n               height = 600, \n               delay = 60) # \n}))\n\nknitr::include_graphics(\"nyc_zip_median_income.gif\")\nSimilarly to our observations of unemployment rates, different areas within counties appear to do significantly better - or worse - than their neighbors.\nCode - Average unemployment rates for zip codes\ntemp_df11&lt;-tmap_zip_unrate_inc_for_sf|&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  group_by(Borough,Neighborhood) |&gt;\n  summarise(avg_median_income_zip=mean(median_income))|&gt;\n  ungroup()\n\ntemp_df21&lt;-tmap_zip_unrate_inc_for_sf|&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  group_by(Borough) |&gt;\n  summarise(avg_median_income=mean(median_income))|&gt;\n  ungroup()\n\ntemp_df31&lt;-left_join(temp_df11,temp_df21,by=c(\"Borough\"=\"Borough\"))\n\ntemp_df31 &lt;-temp_df31 |&gt;\n  mutate(delta=round((avg_median_income_zip-avg_median_income)/avg_median_income,2)\n         )\n\ntemp_df41&lt;-sqldf(\n  \"\n  with a as (\n  select t.*,\n  row_number() over(partition by Borough order by delta asc ) as rn_min,\n  row_number() over(partition by Borough order by delta desc ) as rn_max\n  from temp_df31 t\n  )\n  \n  select Borough,\n  Neighborhood,\n  avg_median_income,\n  avg_median_income_zip,\n  delta\n  from a\n  where 1=1\n  and (rn_min=1 or rn_max=1)\n  \n  ;\n  \"\n)\n\n\ntemp_df41 |&gt;\n  mutate(avg_median_income=round(avg_median_income,0),\n         avg_median_income_zip=round(avg_median_income_zip,0),\n         delta=scales::percent(delta,accuracy=1)\n         ) |&gt;\n  mutate(avg_median_income=scales::dollar(avg_median_income),\n         avg_median_income_zip=scales::dollar(avg_median_income_zip)\n         ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Neighborhoods with Smallest and Largest Median Income\",\n    subtitle=\"2011-2022\"\n  ) |&gt;\ntab_options(\n    table.font.size = 8,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    avg_median_income = \"Borough Income\",\n    avg_median_income_zip= \"Neighborhood Income\",\n    delta = \"Delta vs Borough Average (%)\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\n\nNeighborhoods with Smallest and Largest Median Income\n\n\n2011-2022\n\n\nBorough\nNeighborhood\nBorough Income\nNeighborhood Income\nDelta vs Borough Average (%)\n\n\n\n\nBronx\nHunts Point and Mott Haven\n$43,203\n$25,680\n-41%\n\n\nBronx\nKingsbridge and Riverdale\n$43,203\n$70,931\n64%\n\n\nBrooklyn\nEast New York and New Lots\n$58,857\n$39,342\n-33%\n\n\nBrooklyn\nNorthwest Brooklyn\n$58,857\n$97,621\n66%\n\n\nManhattan\nEast Harlem\n$95,368\n$30,806\n-68%\n\n\nManhattan\nLower Manhattan\n$95,368\n$156,023\n64%\n\n\nQueens\nWest Queens\n$69,152\n$57,176\n-17%\n\n\nQueens\nNortheast Queens\n$69,152\n$87,204\n26%\n\n\nStaten Island\nStapleton and St. George\n$77,817\n$64,223\n-17%\n\n\nStaten Island\nSouth Shore\n$77,817\n$91,771\n18%\nCode - Ridge plot of median income by borough\ncustom_colors &lt;- c(\n  \"Staten Island\" = \"#E31A1C\",\n  \"Bronx\" = \"dodgerblue2\",\n  \"Brooklyn\" = \"green4\",\n  \"Queens\" = \"orange\",\n  \"Manhattan\" = \"black\"\n)\n\nggplot(tmap_zip_unrate_inc_for_sf,\n       aes(x = median_income,\n           y = Borough,\n           fill = Borough)) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_manual(values = custom_colors) +\n  scale_x_continuous(labels = label_number(scale = 1, big.mark = \",\")) +\n  labs(\n    title = \"Distribution of Median Income in Neighborhoods by Boroughs\",\n    x = \"Median Income\",\n    y = \"Borough\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\nPlotting median income for all available years and neighborhoods, we can see the underlying differences among counties more clearly. Majority of neighborhoods in Brooklyn and the Bronx tend to be more low-income, whereas Queens appears to be solidly middle-class, and Manhattan has both poverty and extreme wealth.\n# Create the scatter plot\nunrate_mi_plot &lt;- plot_ly(\n  data = tmap_zip_unrate_inc_for_sf,\n  x = ~median_income,\n  y = ~unrate,\n  color = ~Borough,\n  type = \"scatter\",\n  mode = \"markers\",\n  colors = palette_5counties,\n  marker = list(size = 4),\n  text = ~Borough, # Hover text\n  hoverinfo = \"text+x+y\"\n) |&gt;\n  layout(\n    title = \"Scatter Plot of Unemployment Rate vs Median Income by Borough\",\n    xaxis = list(title = \"Median Income\"),\n    yaxis = list(title = \"Unemployment Rate\")\n  )\n\nunrate_mi_plot\nThe nature of the relationship between median income and unemployment could be seen in this chart - it’s negative and relatively strong. We can also check if there exists a regression relation between the two:\ntmap_zip_unrate_inc_for_sf&lt;-tmap_zip_unrate_inc_for_sf|&gt;\n  mutate(median_income_k=median_income/1000,\n         unrate_pct=unrate/100)\n\nplot(unrate_pct~median_income_k,\n     data=tmap_zip_unrate_inc_for_sf,\n     xlab=\"Median Income ('000s)\",\n     ylab=\"Unemployment Rate\")\n\n# Adding fitted line\nlinear_model&lt;-lm(unrate_pct~median_income_k,\n          data=tmap_zip_unrate_inc_for_sf)\nabline(linear_model,col=\"red\")\n\n\n\n\n\n\n\n# Estimation of regression function\nsummary(linear_model)\n\n\nCall:\nlm(formula = unrate_pct ~ median_income_k, data = tmap_zip_unrate_inc_for_sf)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.095270 -0.017590 -0.003278  0.014747  0.121576 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.327e-01  1.381e-03   96.12   &lt;2e-16 ***\nmedian_income_k -7.225e-04  1.777e-05  -40.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0272 on 2098 degrees of freedom\nMultiple R-squared:  0.4408,    Adjusted R-squared:  0.4405 \nF-statistic:  1654 on 1 and 2098 DF,  p-value: &lt; 2.2e-16\n44% of variability in unemployment rate is reduced when the median income is considered. Based on a very small p-value (~0.0) of F test, we can conclude that this linear regression model is significant and useful and median income could indeed be a significant predictor of unemployment rate. However, because the cause-and-effect relationship between income and unemployment rate is not clear - higher median income could very well be driven by lower unemployment rate - we suggest considering other variables."
  },
  {
    "objectID": "for_course_project_v2.html#get-unrate-by-zip-code-from-census-data",
    "href": "for_course_project_v2.html#get-unrate-by-zip-code-from-census-data",
    "title": "Unemployment in New York",
    "section": "get unrate by zip code from census data",
    "text": "get unrate by zip code from census data\n\n# get a list of all variables from 5 year estimates\nvariables &lt;- load_variables(year = 2022, dataset = \"acs5\", cache = TRUE)\nvariables\n\n# A tibble: 28,152 × 4\n   name        label                                    concept        geography\n   &lt;chr&gt;       &lt;chr&gt;                                    &lt;chr&gt;          &lt;chr&gt;    \n 1 B01001A_001 Estimate!!Total:                         Sex by Age (W… tract    \n 2 B01001A_002 Estimate!!Total:!!Male:                  Sex by Age (W… tract    \n 3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years   Sex by Age (W… tract    \n 4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years    Sex by Age (W… tract    \n 5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years  Sex by Age (W… tract    \n 6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years  Sex by Age (W… tract    \n 7 B01001A_007 Estimate!!Total:!!Male:!!18 and 19 years Sex by Age (W… tract    \n 8 B01001A_008 Estimate!!Total:!!Male:!!20 to 24 years  Sex by Age (W… tract    \n 9 B01001A_009 Estimate!!Total:!!Male:!!25 to 29 years  Sex by Age (W… tract    \n10 B01001A_010 Estimate!!Total:!!Male:!!30 to 34 years  Sex by Age (W… tract    \n# ℹ 28,142 more rows\n\n\n\n# test data for 1 year    \n\nunemployment_data &lt;- get_acs(\n    geography = \"zcta\",  # ZIP Code Tabulation Areas\n    variables = c(\n      unemployed = \"B23025_005E\",  # civilian labor force: Unemployed\n      labor_force = \"B23025_003E\",  # total civilian labor force\n      total_population_16plus=\"B23025_001E\", # total population 16+\n      employed = \"B23025_004E\"  # civilian labor force: employed\n       ),\n    year = 2011,  # Specify the ACS year\n    survey = \"acs5\"  # 5-year estimates\n  )\n\n\n## loop for all years - available from 2011 to 2022\nyears &lt;- lst(2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022)\n\n#list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes_list$ZipCode)) \n\n# Initialize an empty list to store results\nresults  &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)  # Print progress\n  \n  tryCatch({\n    # Fetch ACS data for the current year\n    data &lt;- get_acs(\n      geography = \"zcta\",  # ZIP Code Tabulation Areas\n      variables = c(\n        unemployed = \"B23025_005E\",  # Civilian labor force: Unemployed\n        labor_force = \"B23025_003E\",  # Total civilian labor force\n        total_population_16plus = \"B23025_001E\",  # Total population 16+\n        employed = \"B23025_004E\",  # Civilian labor force: Employed\n      median_income=\"B19013_001E\"\n        ),\n      year = year,\n      survey = \"acs5\"  # 5-year estimates\n    )\n    \n    # Store the data in the results list\n    results[[as.character(year)]] &lt;- data\n  }, error = function(e) {\n    message(\"Error fetching data for year: \", year, \": \", e$message)\n  })\n}\n\nFetching data for year: 2011\n\n\nGetting data from the 2007-2011 5-year ACS\n\n\nFetching data for year: 2012\n\n\nGetting data from the 2008-2012 5-year ACS\n\n\nFetching data for year: 2013\n\n\nGetting data from the 2009-2013 5-year ACS\n\n\nFetching data for year: 2014\n\n\nGetting data from the 2010-2014 5-year ACS\n\n\nFetching data for year: 2015\n\n\nGetting data from the 2011-2015 5-year ACS\n\n\nFetching data for year: 2016\n\n\nGetting data from the 2012-2016 5-year ACS\n\n\nFetching data for year: 2017\n\n\nGetting data from the 2013-2017 5-year ACS\n\n\nFetching data for year: 2018\n\n\nGetting data from the 2014-2018 5-year ACS\n\n\nFetching data for year: 2019\n\n\nGetting data from the 2015-2019 5-year ACS\n\n\nFetching data for year: 2020\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nFetching data for year: 2021\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nFetching data for year: 2022\n\n\nGetting data from the 2018-2022 5-year ACS\n\n# Combine all results into a single dataframe\ncombined_data &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data$zcta&lt;-gsub(\"ZCTA5 \",\"\",combined_data$NAME)\n#head(combined_data)\n\nnyc_zip_codes&lt;-nyc_zip_codes_list |&gt;\n  mutate(zcta=as.character(ZipCode))\n\ncombined_data_nyc&lt;-inner_join(combined_data,nyc_zip_codes_list, by=c(\"zcta\"=\"zcta\"))\nhead(combined_data_nyc)\n\n# A tibble: 6 × 10\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… B23025_…     1515   277 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… B23025_…      379   117 10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… B23025_…     3010   470 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… B23025_…     1698   363 10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… B23025_…      749   214 10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… B23025_…      757   194 10471 Bronx   Kingsbridge…   10471\n\n\n\n#vars_df&lt;-as.data.frame(variables)\n#head(vars_df)\n#\n#\ncombined_data_nyc2 &lt;- combined_data_nyc %&gt;%\n      mutate(variable2 = recode(variable,\n                               \"B23025_005\" = \"unemployed\",\n                               \"B23025_003\" = \"labor_force\",\n                               \"B23025_001\" = \"total_population_16plus\",\n                               \"B23025_004\" = \"employed\",\n                               \"B19013_001\"=\"median_income\"\n                               ))\nhead(combined_data_nyc2)\n\n# A tibble: 6 × 11\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… B23025_…     1515   277 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… B23025_…      379   117 10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… B23025_…     3010   470 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… B23025_…     1698   363 10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… B23025_…      749   214 10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… B23025_…      757   194 10471 Bronx   Kingsbridge…   10471\n# ℹ 1 more variable: variable2 &lt;chr&gt;\n\n\n\n## create a df with unemployed totals only\nunemployed_df&lt;-combined_data_nyc2 |&gt;\n  filter(variable2=='unemployed') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(unemployed=estimate)\n\n## create a df with labor force total only\nlaborforce_df&lt;-combined_data_nyc2 |&gt;\n  filter(variable2=='labor_force') |&gt;\n  select(year,estimate,zcta) |&gt;\n  rename(laborforce=estimate)\n\n\nnyc_unrate_zip_df&lt;-inner_join(unemployed_df,\n                              laborforce_df,\n                              by=c(\"year\"=\"year\",\"zcta\"=\"zcta\"))\n\nnyc_unrate_zip_df&lt;-nyc_unrate_zip_df |&gt;\n  mutate(unrate=round(unemployed/laborforce,2))\n\nhead(nyc_unrate_zip_df)\n\n# A tibble: 6 × 5\n  year  unemployed zcta  laborforce unrate\n  &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n1 2011        1515 10301      18694   0.08\n2 2011         379 10307       6828   0.06\n3 2011        3010 10460      23504   0.13\n4 2011        1698 10465      21366   0.08\n5 2011         749 10470       7772   0.1 \n6 2011         757 10471      11241   0.07\n\n\n\n#pivot wide for maps\n\nnyc_unrate_zip_df_wide&lt;-pivot_wider(nyc_unrate_zip_df,\n                                    id_cols=zcta,\n                                    names_from=year,\n                                    values_from=unrate)\n\n\nnyc_zip_codes_sf3&lt;-nyc_zip_codes_sf2 |&gt;\n  select(GEOID10,Neighborhood,geometry)\n\nnyc_zip_map_merged&lt;-inner_join(nyc_zip_codes_sf3,\n                           nyc_unrate_zip_df_wide,\n                           by=c(\"GEOID10\"=\"zcta\"))\n\n\n## check plot\n## \n## ##check plot\n  \nggplot(nyc_zip_map_merged, \n       aes(geometry=geometry, \n           fill = `2011`)) + \n    geom_sf()+\n     scale_fill_gradient(low = \"white\", high = \"black\") +\n   geom_sf_text(aes(label = Neighborhood), size = 1, color = \"red\") +\ntheme_void()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\n\ntest for other metrics\n###1 year\n\n# test data for 1 year    \n\nunemployment_data &lt;- get_acs(\n    geography = \"zcta\",  # ZIP Code Tabulation Areas\n    variables = c(\n#      male=\"S0501_C01_002E\",\n#female=\"S0501_C01_003E\"\n      \nmedian_income=\"B19013_001E\"\n       ),\n    year = 2011,  # Specify the ACS year\n    survey = \"acs5\"  # 5-year estimates\n  )\n\nGetting data from the 2007-2011 5-year ACS\n\n\n\nunemployment_data\n\n# A tibble: 33,120 × 5\n   GEOID   NAME        variable   estimate   moe\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n 1 0135004 ZCTA5 35004 B19013_001    55386 10300\n 2 0135005 ZCTA5 35005 B19013_001    44590  5748\n 3 0135006 ZCTA5 35006 B19013_001    49750  7300\n 4 0135007 ZCTA5 35007 B19013_001    64701  5389\n 5 0135010 ZCTA5 35010 B19013_001    38737  2251\n 6 0135013 ZCTA5 35013 B19013_001    30750 20874\n 7 0135014 ZCTA5 35014 B19013_001    33200  5944\n 8 0135016 ZCTA5 35016 B19013_001    43315  3265\n 9 0135019 ZCTA5 35019 B19013_001    44904 15283\n10 0135020 ZCTA5 35020 B19013_001    24061  1557\n# ℹ 33,110 more rows\n\n\n\n## loop for all years - available until 2022\nyears &lt;- lst(2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022)\n\n#list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes$ZipCode)) \n\n# Initialize an empty list to store results\nresults  &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)  # Print progress\n  \n  tryCatch({\n    # Fetch ACS data for the current year\n    data &lt;- get_acs(\n      geography = \"zcta\",  # ZIP Code Tabulation Areas\n      variables = c(\n        v1=\"S0501_C01_002E\", #\nv2=\"S0501_C01_003E\"\n\n      ),\n      year = year,\n      survey = \"acs5\"  # 5-year estimates\n    )\n    \n    # Store the data in the results list\n    results[[as.character(year)]] &lt;- data\n  }, error = function(e) {\n    message(\"Error fetching data for year: \", year, \": \", e$message)\n  })\n}\n\nFetching data for year: 2011\n\n\nGetting data from the 2007-2011 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2012\n\n\nGetting data from the 2008-2012 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2013\n\n\nGetting data from the 2009-2013 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2014\n\n\nGetting data from the 2010-2014 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2015\n\n\nGetting data from the 2011-2015 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2016\n\n\nGetting data from the 2012-2016 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2017\n\n\nGetting data from the 2013-2017 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2018\n\n\nGetting data from the 2014-2018 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2019\n\n\nGetting data from the 2015-2019 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2020\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2021\n\n\nGetting data from the 2017-2021 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n\nFetching data for year: 2022\n\n\nGetting data from the 2018-2022 5-year ACS\n\n\nUsing the ACS Subject Tables\n\n# Combine all results into a single dataframe\ncombined_data &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data$zcta&lt;-gsub(\"ZCTA5 \",\"\",combined_data$NAME)\n#head(combined_data)\n\nnyc_zip_codes&lt;-nyc_zip_codes |&gt;\n  mutate(zcta=as.character(ZipCode))\n\ncombined_data_nyc&lt;-inner_join(combined_data,nyc_zip_codes, by=c(\"zcta\"=\"zcta\"))\nhead(combined_data_nyc)\n\n# A tibble: 6 × 10\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… S0501_C…     48.6   1.5 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… S0501_C…     NA    NA   10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… S0501_C…     47.4   1.3 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… S0501_C…     NA    NA   10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… S0501_C…     NA    NA   10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… S0501_C…     NA    NA   10471 Bronx   Kingsbridge…   10471\n\n\n\nacs_vars &lt;- load_variables(2022, \"acs5\", cache = TRUE)\nview(acs_vars)  # Browse the available variables\n\n\n# Filter for NY counties\nstate_list &lt;- search_results %&gt;%\n  filter(grepl(\", NY$\", title, ignore.case = TRUE))  # Match titles ending with \", NY\"\n\n# Display the list of NY counties and their series IDs\nny_counties_list &lt;- ny_counties %&gt;%\n  select(id, title)\nprint(ny_counties_list)\n\n# A tibble: 61 × 2\n   id                    title                                      \n   &lt;chr&gt;                 &lt;chr&gt;                                      \n 1 NYBRON5URN            Unemployment Rate in Bronx County, NY      \n 2 LAUCN360050000000003A Unemployment Rate in Bronx County, NY      \n 3 NYNEWY1URN            Unemployment Rate in New York County, NY   \n 4 LAUCN360610000000003A Unemployment Rate in New York County, NY   \n 5 NYKING7URN            Unemployment Rate in Kings County, NY      \n 6 LAUCN360470000000003A Unemployment Rate in Kings County, NY      \n 7 NYWEST9URN            Unemployment Rate in Westchester County, NY\n 8 LAUCN361190000000003A Unemployment Rate in Westchester County, NY\n 9 NYSUFF0URN            Unemployment Rate in Suffolk County, NY    \n10 NYQUEE1URN            Unemployment Rate in Queens County, NY     \n# ℹ 51 more rows\n\n# Fetch data for all NY counties\nny_unemployment_data &lt;- lapply(ny_counties$id, function(series_id) {\n  data &lt;- fredr(\n    series_id = series_id,\n    observation_start = as.Date(\"2005-01-01\"),  # Start date\n    observation_end = as.Date(\"2019-12-31\")     # End date\n  )\n  data$county &lt;- ny_counties$title[ny_counties$id == series_id]  # Add county name\n  return(data)\n}) %&gt;%\n  bind_rows()  # Combine all data into a single data frame\n\n# View a snippet of the data\nhead(ny_unemployment_data)\n\n# A tibble: 6 × 6\n  date       series_id  value realtime_start realtime_end county                \n  &lt;date&gt;     &lt;chr&gt;      &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;       &lt;chr&gt;                 \n1 2005-01-01 NYBRON5URN   8.1 2024-12-08     2024-12-08   Unemployment Rate in …\n2 2005-02-01 NYBRON5URN   8.1 2024-12-08     2024-12-08   Unemployment Rate in …\n3 2005-03-01 NYBRON5URN   7.2 2024-12-08     2024-12-08   Unemployment Rate in …\n4 2005-04-01 NYBRON5URN   7   2024-12-08     2024-12-08   Unemployment Rate in …\n5 2005-05-01 NYBRON5URN   7   2024-12-08     2024-12-08   Unemployment Rate in …\n6 2005-06-01 NYBRON5URN   7.1 2024-12-08     2024-12-08   Unemployment Rate in …\n\n\n\nuk_unemp &lt;- fredr_series_observations(\n  series_id = \"AURUKM\",\n  observation_start = as.Date(\"1990-01-01\"),\n  observation_end = as.Date(\"2016-12-01\")\n)"
  },
  {
    "objectID": "test_gif.html",
    "href": "test_gif.html",
    "title": "Unemployment in New York",
    "section": "",
    "text": "Data Prep\nIn this section, we obtain and prepare data for analysis.\nInstalling and Loading Libraries\n\n\nShow the code\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tidycensus\")) install.packages(\"tidycensus\")\nlibrary(tidycensus)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"readr\")) install.packages(\"readr\")\nlibrary(readr)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tmap\")) install.packages(\"tmap\")\nlibrary(tmap)\nif (!require(\"knitr\")) install.packages(\"knitr\")\nlibrary(knitr)\n\n\nReading in API Key\n\n# Load the FRED API key from a local file\nfred_api_key &lt;- readLines(\"fred.txt\")\nfredr_set_key(fred_api_key)\n\n\nGetting Shape Files\nCounty\n\n## counties\n\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n    download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n              destfile=\"nyc_borough_boundaries.zip\",\n              method=\"curl\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_county_sf &lt;- read_sf(fname_shp)\nhead(nyc_county_sf)\n\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS 84\n# A tibble: 5 × 5\n  boro_code boro_name      shape_area shape_leng                        geometry\n      &lt;dbl&gt; &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;              &lt;MULTIPOLYGON [°]&gt;\n1         3 Brooklyn      1934142776.    728147. (((-73.86327 40.58388, -73.863…\n2         5 Staten Island 1623618684.    325910. (((-74.05051 40.56642, -74.050…\n3         1 Manhattan      636646082.    360038. (((-74.01093 40.68449, -74.011…\n4         2 Bronx         1187174772.    463181. (((-73.89681 40.79581, -73.896…\n5         4 Queens        3041418004.    888197. (((-73.82645 40.59053, -73.826…\n\n\nzip codes \n\n### \n\nif(!file.exists(\"zipcodes_maps.zip\")){\n    download.file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\", \n              destfile=\"zipcodes_maps.zip\",\n              method=\"curl\")\n}\n\n##-\ntd &lt;- tempdir(); \nzip_contents &lt;- unzip(\"zipcodes_maps.zip\", \n                      exdir = td)\n    \nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_zip_codes_sf &lt;- read_sf(fname_shp)\nhead(nyc_zip_codes_sf)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.8705 ymin: 32.23344 xmax: -85.44988 ymax: 34.5136\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  ZCTA5CE10 AFFGEOID10     GEOID10   ALAND10 AWATER10                   geometry\n  &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;         &lt;MULTIPOLYGON [°]&gt;\n1 36083     8600000US36083 36083   659750662  5522919 (((-85.63225 32.28098, -8…\n2 35441     8600000US35441 35441   172850429  8749105 (((-87.83287 32.84437, -8…\n3 35051     8600000US35051 35051   280236456  5427285 (((-86.74384 33.25002, -8…\n4 35121     8600000US35121 35121   372736030  5349303 (((-86.58527 33.94743, -8…\n5 35058     8600000US35058 35058   178039922  3109259 (((-86.87884 34.21196, -8…\n6 35619     8600000US35619 35619   337059534  1410483 (((-87.28511 34.32882, -8…\n\n\nnyc zip codes mapping doc\n\n# here's a list of NYC zipcodes \nurl &lt;- \"https://raw.githubusercontent.com/erikgregorywebb/nyc-housing/master/Data/nyc-zip-codes.csv\"\n\nresponse &lt;- request(url) %&gt;%\n  req_perform()\n\nfile_path &lt;- \"nyc-zip-codes.csv\"\n\nwriteBin(response$body, file_path)\nnyc_zip_codes_list &lt;- read_csv(file_path)\n\nhead(nyc_zip_codes_list)\n\n# A tibble: 6 × 3\n  Borough Neighborhood           ZipCode\n  &lt;chr&gt;   &lt;chr&gt;                    &lt;dbl&gt;\n1 Bronx   Central Bronx            10453\n2 Bronx   Central Bronx            10457\n3 Bronx   Central Bronx            10460\n4 Bronx   Bronx Park and Fordham   10458\n5 Bronx   Bronx Park and Fordham   10467\n6 Bronx   Bronx Park and Fordham   10468\n\n\n\nnyc_zip_codes_list&lt;-nyc_zip_codes_list|&gt;\n  mutate(zcta=as.character(ZipCode))\n\n\nnyc_zip_codes_sf2&lt;-inner_join(nyc_zip_codes_sf,\n                              nyc_zip_codes_list,by=c(\"GEOID10\"=\"zcta\")\n                              ) |&gt;\n  select(GEOID10,Borough,Neighborhood,geometry)\n\nhead(nyc_zip_codes_sf2)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.98814 ymin: 40.64933 xmax: -73.83094 ymax: 40.86634\nGeodetic CRS:  NAD83\n# A tibble: 6 × 4\n  GEOID10 Borough   Neighborhood                                        geometry\n  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;                                     &lt;MULTIPOLYGON [°]&gt;\n1 10040   Manhattan Inwood and Washington Heights (((-73.93519 40.86027, -73.93…\n2 11207   Brooklyn  East New York and New Lots    (((-73.90885 40.6934, -73.903…\n3 10456   Bronx     High Bridge and Morrisania    (((-73.91903 40.83309, -73.91…\n4 11417   Queens    Southwest Queens              (((-73.86235 40.67916, -73.85…\n5 11104   Queens    Northwest Queens              (((-73.92763 40.7366, -73.925…\n6 10024   Manhattan Upper West Side               (((-73.98814 40.78141, -73.97…\n\n\n\n\n\nFRED API DATA\n\n# Search for unemployment rate data series for NY counties\nsearch_results &lt;- fredr_series_search_text(\"unemployment rate county\") \n\n# Filter for NY counties\nny_counties &lt;- search_results %&gt;%\n  filter(grepl(\", NY$\", title, ignore.case = TRUE))  # Match titles ending with \", NY\"\n\n# Display the list of NY counties and their series IDs\nny_counties_list &lt;- ny_counties %&gt;%\n  select(id, title)\nprint(ny_counties_list)\n\n# A tibble: 61 × 2\n   id                    title                                      \n   &lt;chr&gt;                 &lt;chr&gt;                                      \n 1 NYBRON5URN            Unemployment Rate in Bronx County, NY      \n 2 LAUCN360050000000003A Unemployment Rate in Bronx County, NY      \n 3 NYNEWY1URN            Unemployment Rate in New York County, NY   \n 4 LAUCN360610000000003A Unemployment Rate in New York County, NY   \n 5 NYKING7URN            Unemployment Rate in Kings County, NY      \n 6 LAUCN360470000000003A Unemployment Rate in Kings County, NY      \n 7 NYWEST9URN            Unemployment Rate in Westchester County, NY\n 8 LAUCN361190000000003A Unemployment Rate in Westchester County, NY\n 9 NYSUFF0URN            Unemployment Rate in Suffolk County, NY    \n10 NYQUEE1URN            Unemployment Rate in Queens County, NY     \n# ℹ 51 more rows\n\n# Fetch data for all NY counties\nny_unemployment_data &lt;- lapply(ny_counties$id, function(series_id) {\n  data &lt;- fredr(\n    series_id = series_id,\n    observation_start = as.Date(\"2000-01-01\"),  # Start date\n    observation_end = as.Date(\"2023-12-31\") ,    # End date\n  frequency         = \"a\",           # \"a\" for annual\n    aggregation_method = \"avg\"   \n    )\n  data$county &lt;- ny_counties$title[ny_counties$id == series_id]  # Add county name\n  return(data)\n}) %&gt;%\n  bind_rows()  # Combine all data into a single data frame\n\n#### filter to NYC counties\n\nlist_of_ny_counties&lt;-c(\"New York\",\"Bronx\",\"Kings\",\"Queens\",\"Richmond\")\nunrate&lt;-\"Unemployment Rate in \"\nny_var&lt;-\" County, NY\"\n\nny_list &lt;- rep(ny_var, 5)\nunrate_list&lt;-rep(unrate,5)\n\n\nny_combined_list &lt;- mapply(function(x, y,z) paste(x, y,z, sep = \"\"), unrate_list,\n                           list_of_ny_counties,ny_list,SIMPLIFY = FALSE)\n\n# Filter \nny_unrate_county_filtered &lt;- ny_unemployment_data %&gt;% \n  filter(county %in% ny_combined_list)\n\nny_unrate_county_filtered&lt;-ny_unrate_county_filtered |&gt;\n  mutate(county2=sub(\"Unemployment Rate in \", \"\", county),\n         county3=sub(\", NY\",\"\",county2)) |&gt;\n  select(date,value,county3)\n\n\n### there are several instances of multiple rates for the same county/year - we'll take avg\n\nny_unrate_county_filtered2&lt;-ny_unrate_county_filtered |&gt;\n  group_by(county3,date) |&gt;\n  summarise(unrate=mean(value)) |&gt;\n  ungroup()\n\nnyc_county_unrate_annual&lt;-ny_unrate_county_filtered2 |&gt;\n  mutate(year = year(as.Date(date))) |&gt;\n  mutate(county_name = case_when(\n    county3 == \"New York County\" ~ \"Manhattan\",\n    county3 == \"Bronx County\" ~ \"Bronx\",\n    county3 == \"Kings County\" ~ \"Brooklyn\",\n    county3 == \"Queens County\" ~ \"Queens\",\n    county3 == \"Richmond County\" ~ \"Staten Island\",\n    TRUE ~ \"Unknown\"\n  )) \n\nhead(nyc_county_unrate_annual)\n\n# A tibble: 6 × 5\n  county3      date       unrate  year county_name\n  &lt;chr&gt;        &lt;date&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;      \n1 Bronx County 2000-01-01   7.1   2000 Bronx      \n2 Bronx County 2001-01-01   7.4   2001 Bronx      \n3 Bronx County 2002-01-01   9.75  2002 Bronx      \n4 Bronx County 2003-01-01  10.5   2003 Bronx      \n5 Bronx County 2004-01-01   9.1   2004 Bronx      \n6 Bronx County 2005-01-01   7.4   2005 Bronx      \n\n\n\nanimated plot - county-level\n\n###  create a df tmap\n\ntmap_nyc_county_unrate_annual&lt;-left_join(nyc_county_unrate_annual,\n                                         nyc_county_sf,\n                                         by=c(\"county_name\"=\"boro_name\"))\n\ntmap_nyc_county_unrate_annual2&lt;-tmap_nyc_county_unrate_annual|&gt;\n  select(year,unrate,county_name,geometry) \n\n  tmap_nyc_county_unrate_annual2$label&lt;-paste(tmap_nyc_county_unrate_annual2$county_name,\n                                              tmap_nyc_county_unrate_annual2$unrate,\n                                              sep=\": \"\n                                              )\n\n\n# set tmap mode \ntmap_mode(\"plot\")\n\n\n\n\nNYC unemployemt over the years - animated plot\n\n### join maps data for tmap\n\nmap &lt;- tmap_nyc_county_unrate_annual2 |&gt;\n  filter(year&gt;=2011 & year&lt;=2022) |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"unrate\", \n              title = \"Unemployment Rate\", \n              palette = \"Greys\",  # Adjust palette as desired\n              style = \"cont\") +   # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")+\n  tm_layout(frame = FALSE)\n\ninvisible(capture.output({\n  tmap_animation(map, \n               filename = \"test.gif\", \n               width = 800, \n               height = 600, \n               delay = 30) # Adjust delay as desired\n}))\n\n\nknitr::include_graphics(\"test.gif\")"
  },
  {
    "objectID": "summary_report.html",
    "href": "summary_report.html",
    "title": "Summary Report",
    "section": "",
    "text": "What does unemployment look like over time?\n\n\n\n\n\n\n\n\n\nThis animated map provides a clear and intuitive answer to the question “What does unemployment look like over time?” by offering a visual representation of unemployment trends across the United States from one year to the next. The map uses color shading to show the unemployment rate for each state, with darker shades indicating higher unemployment and lighter shades representing lower rates. By watching the animation, viewers can easily observe how unemployment changes over time, on a national and state level. The animation highlights major patterns, such as regions that experience persistently high or low unemployment and significant countrywide shifts, such as in 2020, which we know can be attributed to the COVID-19 pandemic. This visual contributes to our overarching question in a fashion of not visually showcasing the “what” of unemployment trends but specifically the “where” and “when”.\n\nThis visualization is the same data as the animated visualization - average yearly unemployment rates for each state and the United States overall from 2011 to 2022. This visualization draws focus to New York and the Countrywide average through the red and black lines, respectively. This layered design makes it easy to compare New York’s performance against the broader trends at both the state and national levels. This chart shows a steady decline in unemployment across most states, including New York, from 2011 until 2020. The sharp spike in 2020 reflects the impact of the COVID-19 pandemic, which we have now seen several times throughout the data.\nAn interesting result is how closely New York’s average unemployment follows the countrywide unemployment rates up until 2020, when the pandemic took place. Many of us were a part of, or know of, the drastic economic effect the pandemic had on New York City, which encompasses much of the population and economy of New York state. We also see that, since then, New York has still remained above the countrywide unemployment level, which can perhaps be attributed to the post-pandemic continued attempt at economic recovery.  This visualization highlights unemployment rates over time for black, white and the countrywide average groups, which we find in other visualizations as well. This visual specifically highlights the gaps between each group’s rates and the national average with dashed lines. The blue line for Black unemployment rates consistently remains above both the national average (grey line) and White unemployment rates (red line), illustrating a persistent disparity.\nThis analysis contributes to our overall study by showcasing the persistent impact of race as a key factor in unemployment disparities. While this chart focuses on nationwide data, it provides a framework for understanding how similar trends might manifest within NYC. Identifying such disparities at the national level sets the stage for further analysis to explore whether similar racial gaps exist in NYC’s unemployment data, helping to answer the overarching question about the demographic factors driving these trends.\n\nThis visual ties in with the visual we see above, providing a numeric display of the difference between black and white unemployment rates, from 2011 to 2022. At the bottom of the table displays the average difference across all years, of 4.72%. This gap is wider than the 2.9% difference found in Q2 2024 from the study conducted by the Economic Policy Institute. \nThis visualization explores one of the three demographic factors, race, which we chose to explore as an attempt at drawing conclusions to our overarching question of what demographic factors best explain NYC’s unemployment trends.\nThis visual gives us insight into the unemployment trends across racial groups in the United States from 2011 to 2022, and perhaps sheds light to how unemployment disproportionately impacts marginalized groups.\nThe gray line represents the countrywide average unemployment rate, displayed prominently for context. The colored lines represent unemployment trends for specific racial groups: Black (blue), Hispanic (orange), White (red), and Asian (green). The chart reveals several insights. First, unemployment rates for Black individuals consistently remain higher than those for other groups and the countrywide average. Hispanic unemployment rates generally follow a similar trajectory but at slightly lower levels. On the other hand, Asian and White unemployment rates tend to stay below the countrywide average, indicating comparatively lower unemployment levels for these groups overall.\nThis gives us our first into race as one of our three demographic factors we continue to explore through each analytical question.\n\nWe find that, when we take each of the available races (black, hispanic, and white) and compare against the men’s countrywide unemployment rate, the exact same trends follow to the racial unemployment rate v countrywide overall (without crossing with gender). Consistently, the black unemployment rate is the highest, followed by hispanic, with the countrywide average falling below, followed by the white unemployment rate falling below the countrywide average.\n\nAgain, when looking at the same available race x gender unemployment rates (black, hispanic, and white), women follow the same trend as races in general and men x race. This overwhelmingly displays the racial disparity in unemployment rates, which we continue to look into further throughout the project.\n\n\nWhat areas of New York are most affected by unemployment?\nUnemployment rates across NYC largely follow the same trend as the rest of the US, peaking during uncertain and turbulent times. During the Covid pandemic, the already higher trending unemployment in the Bronx was 3 times higher yet compared to the pre-pandemic levels. But even Manhattan, a much more advantaged area, experienced a significant spike in unemployment rate.\n\nThere are observable differences in unemployment across boroughs, with Bronx rates consistently trending higher than in other boroughs.\n\n\n\n\n\n\n\n\n\nBut NYC boroughs are populous and large. For example, Brooklyn alone is the fourth most populous city not even in New York state but in the entire country. It’s not only big - it’s also very diverse. Within each borough, there are multiple neighborhoods that greatly differ from each other in a number of ways. \nUnemployment rate in Greenpoint, one of the more affluent areas in Brooklyn, is almost 3 percentage points lower than the average for the county, whereas in Bushwick and Williamsburg it’s 2 percentage points above the average. And we see this in each and every borough.\n\n\nWhat factors affect unemployment in New York?\nIn this part of the project, we focused on some of New York’s demographics data to find what factors affect unemployment in New York. After gathering data from FRED and Census, we created a regression model using some of the variables explored earlier. The regression model helped us identify which of the demographic characteristics have a strong relationship with unemployment in New York and can therefore be used to estimate the unemployment rate of an area of New York with a given demographic split of the population.\nBased on our understanding of the demographic characteristics and the regression model created, we found that the level of education is one of the most important factors related to unemployment in New York. We believe this makes sense given that the more education an individual has, the more opportunities that individual might have given some jobs require certain levels of education. Also, education level is something that can be changed for almost any individual regardless of other factors such as age and gender. However, it is important to note that there are many important characteristics that could impact unemployment, but the scope of this project is demographic factors.\nIn the following graph we can see that the unemployment rate estimated by our model based on the demographic factors selected aligns with the actual unemployment rate seen in our data. Furthermore, we found that the demographic characteristics selected help explain 96% of the variability in New York’s unemployment rate."
  },
  {
    "objectID": "individual_report.html",
    "href": "individual_report.html",
    "title": "What Areas of New York Are Most Affected by Unemployment?",
    "section": "",
    "text": "This report discusses one of the specific analytical questions that were analyzed in the group course project by our team, Apple Watch, and presented on December 11, 2024. The presentation is available upon request and the summary report can be accessed at this link. For this analysis, we will look at data from 2011 to 2022 due to availability of specific metrics. We will look at unemployment in NYC on the county- and zipcode levels. As in any data analysis project, we start with obtaining and preparing the data. We will use Census US and FRED databases and we will do that using tidycensus and fredr packages, respectively. These packages were specifically designed for the purposes of facilitating access to data via API. To execute the code in this analysis, API keys for Census and FRED are required and should be requested in advance - please refer to the notes in the code. We will also obtain and prepare SHAPE files for plotting and put together a mapping file for zip codes and counties.\n\n\nCode - installing and loading libraries\n# Installing and loading libraries\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\nif (!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nlibrary(dplyr)\nif (!require(\"psych\")) install.packages(\"psych\")\nlibrary(psych)\nif (!require(\"gt\")) install.packages(\"gt\")\nlibrary(gt)\nif (!require(\"formattable\")) install.packages(\"formattable\")\nlibrary(formattable)\nif (!require(\"sqldf\")) install.packages(\"sqldf\")\nlibrary(sqldf)\nif (!require(\"plotly\")) install.packages(\"plotly\")\nlibrary(plotly)\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\nif (!require(\"RColorBrewer\")) install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tidycensus\")) install.packages(\"tidycensus\")\nlibrary(tidycensus)\nif (!require(\"httr2\")) install.packages(\"httr2\")\nlibrary(httr2)\nif (!require(\"readr\")) install.packages(\"readr\")\nlibrary(readr)\nif (!require(\"sf\")) install.packages(\"sf\")\nlibrary(sf)\nif (!require(\"fredr\")) install.packages(\"fredr\")\nlibrary(fredr)\nif (!require(\"tmap\")) install.packages(\"tmap\")\nlibrary(tmap)\nif (!require(\"knitr\")) install.packages(\"knitr\")\nlibrary(knitr)\nif (!require(\"ggridges\")) install.packages(\"ggridges\")\nlibrary(ggridges)\nif (!require(\"scales\")) install.packages(\"scales\")\nlibrary(scales)\n\n\n\n\nCode - Read in API keys\n# Load the FRED API key from a local file - Please request it in advance.\nfred_api_key &lt;- readLines(\"fred.txt\")\nfredr_set_key(fred_api_key)\n\n# You also need to initialize the Census API key - it's read in only once the very first time it's used so I commented this line out. Please use your own Census API key if needed.\n# census_api_key(\"xxxxx\", install = TRUE)\n\n\n\n\nCode - Getting the SHAPE files for NYC counties\n## counties\n\nif (!file.exists(\"nyc_borough_boundaries.zip\")) {\n  download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\",\n    destfile = \"nyc_borough_boundaries.zip\",\n    method = \"curl\"\n  )\n}\n\n## -\ntd &lt;- tempdir()\nzip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\",\n  exdir = td\n)\n\nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_county_sf &lt;- read_sf(fname_shp)\n# head(nyc_county_sf)\n\n\n\n\nCode - Getting the SHAPE files for NYC zip codes\nif (!file.exists(\"zipcodes_maps.zip\")) {\n  download.file(\"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_zcta510_500k.zip\",\n    destfile = \"zipcodes_maps.zip\",\n    method = \"curl\"\n  )\n}\n\n\ntd &lt;- tempdir()\nzip_contents &lt;- unzip(\"zipcodes_maps.zip\",\n  exdir = td\n)\n\nfname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\nnyc_zip_codes_sf &lt;- read_sf(fname_shp)\n\n\n\n\nCode - Get a mapping file for NYC Zip Codes and Counties\n# here's a list of NYC zipcodes\nurl &lt;- \"https://raw.githubusercontent.com/erikgregorywebb/nyc-housing/master/Data/nyc-zip-codes.csv\"\n\nresponse &lt;- request(url) |&gt;\n  req_perform()\n\nfile_path &lt;- \"nyc-zip-codes.csv\"\n\nwriteBin(response$body, file_path)\nnyc_zip_codes_list &lt;- read_csv(file_path)\n\nnyc_zip_codes_list &lt;- nyc_zip_codes_list |&gt;\n  mutate(zcta = as.character(ZipCode))\n\nnyc_zip_codes_sf2 &lt;- inner_join(nyc_zip_codes_sf,\n  nyc_zip_codes_list,\n  by = c(\"GEOID10\" = \"zcta\")\n) |&gt;\n  select(GEOID10, Borough, Neighborhood, geometry)\n\n# head(nyc_zip_codes_sf2)\n\n\n\n\nCode - Initialize tmap plotting mode\n# it needs to be done only once hence it's listed separately\ntmap_mode(\"plot\")\n\n\n\n\nCode - Getting FRED annual unemployment data for NYC counties\n# Search for unemployment rate data series for NY counties\nsearch_results &lt;- fredr_series_search_text(\"unemployment rate county\")\n\n# Filter for NY counties\nny_counties &lt;- search_results |&gt;\n  filter(grepl(\", NY$\", title, ignore.case = TRUE))\n\n# Display the list of NY counties and their series IDs\n# ny_counties_list &lt;- ny_counties |&gt;\n#  select(id, title)\n# print(ny_counties_list)\n\n# Fetch data for all NY counties\nny_unemployment_data &lt;- lapply(ny_counties$id, function(series_id) {\n  data &lt;- fredr(\n    series_id = series_id,\n    observation_start = as.Date(\"2010-12-31\"),\n    observation_end = as.Date(\"2023-12-31\"), \n    frequency = \"a\", # \"a\" for annual\n    aggregation_method = \"avg\"\n  )\n  data$county &lt;- ny_counties$title[ny_counties$id == series_id] \n  return(data)\n}) |&gt;\n  bind_rows()\n\n#### filter to NYC counties\n\nlist_of_ny_counties &lt;- c(\"New York\", \"Bronx\", \"Kings\", \"Queens\", \"Richmond\")\nunrate &lt;- \"Unemployment Rate in \"\nny_var &lt;- \" County, NY\"\n\nny_list &lt;- rep(ny_var, 5)\nunrate_list &lt;- rep(unrate, 5)\n\nny_combined_list &lt;- mapply(function(x, y, z) paste(x, y, z, sep = \"\"), unrate_list,\n  list_of_ny_counties, ny_list,\n  SIMPLIFY = FALSE\n)\n\n# Filter\nny_unrate_county_filtered &lt;- ny_unemployment_data |&gt;\n  filter(county %in% ny_combined_list)\n\nny_unrate_county_filtered &lt;- ny_unrate_county_filtered |&gt;\n  mutate(\n    county2 = sub(\"Unemployment Rate in \", \"\", county),\n    county3 = sub(\", NY\", \"\", county2)\n  ) |&gt;\n  select(date, value, county3)\n\n\n### there are several instances of multiple rates for the same county/year - we'll take avg\n\nny_unrate_county_filtered2 &lt;- ny_unrate_county_filtered |&gt;\n  group_by(county3, date) |&gt;\n  summarise(unrate = mean(value)) |&gt;\n  ungroup()\n\nnyc_county_unrate_annual &lt;- ny_unrate_county_filtered2 |&gt;\n  mutate(year = year(as.Date(date))) |&gt;\n  mutate(county_name = case_when(\n    county3 == \"New York County\" ~ \"Manhattan\",\n    county3 == \"Bronx County\" ~ \"Bronx\",\n    county3 == \"Kings County\" ~ \"Brooklyn\",\n    county3 == \"Queens County\" ~ \"Queens\",\n    county3 == \"Richmond County\" ~ \"Staten Island\",\n    TRUE ~ \"Unknown\"\n  ))\n\n# head(nyc_county_unrate_annual)\n\n\n\n\nCode - Unemployment Trend in NYC Counties\npalette_5counties &lt;- c(\n  \"dodgerblue2\", \"green4\", \"black\", \"orange\", \"#E31A1C\"\n)\n\n# create a chart\n\nnyc_unrate_annual2011 &lt;- nyc_county_unrate_annual |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  mutate()\n\nnyc_annual_unrate_year_plot &lt;- plot_ly(nyc_unrate_annual2011,\n  x = ~year, y = ~unrate,\n  color = ~county_name,\n  type = \"scatter\",\n  mode = \"lines\",\n  colors = palette_5counties\n) |&gt;\n  layout(\n    title = \"Unemployment Rate by NYC Borough\",\n    xaxis = list(\n      title = \"Year\"\n      #    showgrid = FALSE\n    ),\n    yaxis = list(\n      title = \"Unemployment Rate, %\"\n      #   showgrid = FALSE\n      # ,\n      #    tickformat = \".0%\",\n      #    range = c(0, 1)\n    ),\n    legend = list(\n      orientation = \"h\", \n      x = 0.5, \n      xanchor = \"center\", \n      y = -0.2, \n      font = list(size = 8) \n    )\n  )\n\n# Show the plot\n\nnyc_annual_unrate_year_plot\n\n\n\n\n\n\nUnemployment rates across NYC largely follow the same trend as the rest of the US, peaking during uncertain and turbulent times. During the Covid pandemic, the already higher trending unemployment in the Bronx was 3 times higher yet compared to the pre-pandemic levels. But even Manhattan, a much more advantaged area, experienced a significant spike (+2X) in unemployment rate.\n\n\nCode - Animated plot of annual unemployment in NYC counties\n###  create a df for plotting\n\ntmap_nyc_county_unrate_annual &lt;- left_join(nyc_county_unrate_annual,\n  nyc_county_sf,\n  by = c(\"county_name\" = \"boro_name\")\n)\n\ntmap_nyc_county_unrate_annual2 &lt;- tmap_nyc_county_unrate_annual |&gt;\n  select(year, unrate, county_name, geometry)\n\ntmap_nyc_county_unrate_annual2$label &lt;- paste(tmap_nyc_county_unrate_annual2$county_name,\n  tmap_nyc_county_unrate_annual2$unrate,\n  sep = \": \"\n)\n\n\n### join maps data for tmap\n\nmap &lt;- tmap_nyc_county_unrate_annual2 |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"unrate\",\n    title = \"Unemployment Rate\",\n    palette = \"Greys\", # \n    style = \"cont\"\n  ) + \n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\") +\n  tm_layout(frame = FALSE)\n\ninvisible(capture.output({\n  tmap_animation(map,\n    filename = \"nyc_unemployment2.gif\",\n    width = 800,\n    height = 600,\n    delay = 60\n  ) \n}))\n\n\n\nknitr::include_graphics(\"nyc_unemployment2.gif\")\n\n\n\n\n\n\n\n\nThere are observable differences in unemployment across boroughs, with Bronx rates consistently trending higher than in other boroughs. But NYC boroughs are populous and large. For example, Brooklyn alone is the fourth most populous city not even in New York state but in the entire country. It’s not only big - it’s also very diverse. Within each borough, there are multiple neighborhoods that greatly differ from each other in a number of ways.\n\n\nCode - Getting data for unemployment in NYC zip codes\n## loop for all years - available from 2011 to 2022\nyears &lt;- lst(2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022)\n\n# list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes_list$ZipCode))\n\n# Initialize an empty list to store results\nresults &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)\n\n  tryCatch(\n    {\n      # Fetch ACS data for the current year\n      data &lt;- get_acs(\n        geography = \"zcta\",\n        variables = c(\n          unemployed = \"B23025_005E\", # Civilian labor force: Unemployed\n          labor_force = \"B23025_003E\", # Total civilian labor force\n          total_population_16plus = \"B23025_001E\", # Total population 16+\n          employed = \"B23025_004E\", # Civilian labor force: Employed\n          median_income = \"B19013_001E\"\n        ),\n        year = year,\n        survey = \"acs5\" # 5-year estimates\n      )\n\n      # Store the data in the results list\n      results[[as.character(year)]] &lt;- data\n    },\n    error = function(e) {\n      message(\"Error fetching data for year: \", year, \": \", e$message)\n    }\n  )\n}\n\n# Combine all results into a single dataframe\ncombined_data &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data$zcta &lt;- gsub(\"ZCTA5 \", \"\", combined_data$NAME)\n# head(combined_data)\n\nnyc_zip_codes &lt;- nyc_zip_codes_list |&gt;\n  mutate(zcta = as.character(ZipCode))\n\ncombined_data_nyc &lt;- inner_join(combined_data, nyc_zip_codes_list, by = c(\"zcta\" = \"zcta\"))\n\ncombined_data_nyc2 &lt;- combined_data_nyc |&gt;\n  mutate(variable2 = recode(variable,\n    \"B23025_005\" = \"unemployed\",\n    \"B23025_003\" = \"labor_force\",\n    \"B23025_001\" = \"total_population_16plus\",\n    \"B23025_004\" = \"employed\",\n    \"B19013_001\" = \"median_income\"\n  ))\n\n## create a df with unemployed totals only\nunemployed_df &lt;- combined_data_nyc2 |&gt;\n  filter(variable2 == \"unemployed\") |&gt;\n  select(year, estimate, zcta) |&gt;\n  rename(unemployed = estimate)\n\n## create a df with labor force total only\nlaborforce_df &lt;- combined_data_nyc2 |&gt;\n  filter(variable2 == \"labor_force\") |&gt;\n  select(year, estimate, zcta) |&gt;\n  rename(laborforce = estimate)\n\n## create a df with median income total only\nmedian_income_df &lt;- combined_data_nyc2 |&gt;\n  filter(variable2 == \"median_income\") |&gt;\n  select(year, estimate, zcta) |&gt;\n  rename(median_income = estimate)\n\nnyc_unrate_zip_df &lt;- inner_join(unemployed_df,\n  laborforce_df,\n  by = c(\"year\" = \"year\", \"zcta\" = \"zcta\")\n)\n\nnyc_unrate_inc_zip_df &lt;- inner_join(nyc_unrate_zip_df,\n  median_income_df,\n  by = c(\"year\" = \"year\", \"zcta\" = \"zcta\")\n)\n\nnyc_unrate_inc_zip_df &lt;- nyc_unrate_inc_zip_df |&gt;\n  mutate(unrate = 100 * round(unemployed / laborforce, 2), na.rm = TRUE)\n\nnyc_unrate_inc_zip_df &lt;- na.omit(nyc_unrate_inc_zip_df)\n\n\n## join zip code data and zip sf file\n\nzip_unrate_inc_for_sf &lt;- left_join(nyc_unrate_inc_zip_df,\n  nyc_zip_codes_sf2,\n  by = c(\"zcta\" = \"GEOID10\")\n)\n\ntmap_zip_unrate_inc_for_sf &lt;- zip_unrate_inc_for_sf |&gt;\n  select(year, unrate, median_income, zcta, Neighborhood, Borough, geometry)\n\n\n\n\nCode - Plotting unemployment over time in NYC zipcodes\n### join maps data for tmap\nmap &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"unrate\",\n    title = \"Unemployment Rate\",\n    palette = \"Blues\", \n    style = \"cont\",\n    fill.na = \"red\"\n  ) + # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_layout(frame = FALSE)\n# tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")\n\ninvisible(capture.output({\n  tmap_animation(map,\n    filename = \"nyc_zip_unrate.gif\",\n    width = 800,\n    height = 600,\n    delay = 60\n  )\n}))\n\n\n\nknitr::include_graphics(\"nyc_zip_unrate.gif\")\n\n\n\n\n\n\n\n\n\n\nCode - Average unemployment rates for zip codes\ntemp_df1 &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  group_by(Borough, Neighborhood) |&gt;\n  summarise(avg_unrate_zip = mean(unrate)) |&gt;\n  ungroup()\n\ntemp_df2 &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  group_by(Borough) |&gt;\n  summarise(avg_unrate = mean(unrate)) |&gt;\n  ungroup()\n\ntemp_df3 &lt;- left_join(temp_df1, temp_df2, by = c(\"Borough\" = \"Borough\"))\n\ntemp_df3 &lt;- temp_df3 |&gt;\n  mutate(delta = round((avg_unrate_zip - avg_unrate), 2))\n\ntemp_df4 &lt;- sqldf(\n  \"\n  with a as (\n  select t.*,\n  row_number() over(partition by Borough order by delta asc ) as rn_min,\n  row_number() over(partition by Borough order by delta desc ) as rn_max\n  from temp_df3 t\n  )\n\n  select Borough,\n  Neighborhood,\n  avg_unrate,\n  avg_unrate_zip,\n  delta\n  from a\n  where 1=1\n  and (rn_min=1 or rn_max=1)\n\n  ;\n  \"\n)\n\n\ntemp_df4 |&gt;\n  mutate(\n    avg_unrate = round(avg_unrate, 2),\n    avg_unrate_zip = round(avg_unrate_zip, 2)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Neighborhoods with Smallest and Largest Unemployment Rates\",\n    subtitle = \"2011-2022\"\n  ) |&gt;\n  tab_options(\n    table.font.size = 10,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    avg_unrate = \"Borough Rate\",\n    avg_unrate_zip = \"Neighborhood Rate\",\n    delta = \"Delta from Borough Average (pp)\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\n\nNeighborhoods with Smallest and Largest Unemployment Rates\n\n\n2011-2022\n\n\nBorough\nNeighborhood\nBorough Rate\nNeighborhood Rate\nDelta from Borough Average (pp)\n\n\n\n\nBronx\nKingsbridge and Riverdale\n11.87\n8.12\n-3.74\n\n\nBronx\nCentral Bronx\n11.87\n15.67\n3.80\n\n\nBrooklyn\nGreenpoint\n8.63\n5.88\n-2.76\n\n\nBrooklyn\nBushwick and Williamsburg\n8.63\n10.97\n2.34\n\n\nManhattan\nUpper East Side\n6.80\n4.18\n-2.62\n\n\nManhattan\nCentral Harlem\n6.80\n11.57\n4.76\n\n\nQueens\nNortheast Queens\n7.77\n5.85\n-1.91\n\n\nQueens\nJamaica\n7.77\n11.20\n3.43\n\n\nStaten Island\nSouth Shore\n6.25\n5.45\n-0.80\n\n\nStaten Island\nPort Richmond\n6.25\n7.17\n0.92\n\n\n\n\n\n\n\nUnemployment rate in Greenpoint, one of the more affluent areas in Brooklyn, is almost 3 percentage points lower than the average for the county, whereas in Bushwick and Williamsburg it’s 2 percentage points above the average. And we see this in each and every borough:\n1) in the Bronx, unemployment rate in Central Bronx is 3.8 percentage points higher and in Kingsbridge and Riverdale - 3.7 percentage points lower than the county average rate of 12%.\n2) in Queens, unemployment rate in Central Bronx is 3.8 percentage points higher and in Kingsbridge and Riverdale - 3.7 percentage points lower than the county average rate of 8%.\n3) in Manhattan, unemployment rate in Central Harlem is 4.76 percentage points higher and in Upper East Side - 2.62 percentage points lower than the county average rate of 7%.\n4) in Staten Island, unemployment rate in Port Richmond is 0.9 percentage points higher and in South Shore - 0.8 percentage points lower than the county average rate of 6%.\nThese differences could be attributed to a number of factors, most of which could be traced back to characteristics of population within specific neighborhoods. While this analysis is outside of the scope of this specific question, we can take a look at median income by zip codes to see whether the unemployment in a given neighborhood can be linked to a financial wellbeing of its residence.\n\n\nCode - Plotting median income in NYC zip codes\n### join maps data for tmap\n\nmap &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"median_income\",\n    title = \"Median Income\",\n    palette = \"Greens\", # Adjust palette as desired\n    style = \"cont\",\n    fill.na = \"red\"\n  ) + # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_layout(frame = FALSE)\n# tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")\n\ninvisible(capture.output({\n  tmap_animation(map,\n    filename = \"nyc_zip_median_income.gif\",\n    width = 800,\n    height = 600,\n    delay = 60\n  ) #\n}))\n\nknitr::include_graphics(\"nyc_zip_median_income.gif\")\n\n\n\n\n\n\n\n\n\nSimilarly to our observations of unemployment rates, different areas within counties appear to do significantly better - or worse - than their neighbors.\n\n\nCode - Average unemployment rates for zip codes\ntemp_df11 &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  group_by(Borough, Neighborhood) |&gt;\n  summarise(avg_median_income_zip = mean(median_income)) |&gt;\n  ungroup()\n\ntemp_df21 &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  filter(year &gt;= 2011 & year &lt;= 2022) |&gt;\n  group_by(Borough) |&gt;\n  summarise(avg_median_income = mean(median_income)) |&gt;\n  ungroup()\n\ntemp_df31 &lt;- left_join(temp_df11, temp_df21, by = c(\"Borough\" = \"Borough\"))\n\ntemp_df31 &lt;- temp_df31 |&gt;\n  mutate(delta = round((avg_median_income_zip - avg_median_income) / avg_median_income, 2))\n\ntemp_df41 &lt;- sqldf(\n  \"\n  with a as (\n  select t.*,\n  row_number() over(partition by Borough order by delta asc ) as rn_min,\n  row_number() over(partition by Borough order by delta desc ) as rn_max\n  from temp_df31 t\n  )\n\n  select Borough,\n  Neighborhood,\n  avg_median_income,\n  avg_median_income_zip,\n  delta\n  from a\n  where 1=1\n  and (rn_min=1 or rn_max=1)\n\n  ;\n  \"\n)\n\n\ntemp_df41 |&gt;\n  mutate(\n    avg_median_income = round(avg_median_income, 0),\n    avg_median_income_zip = round(avg_median_income_zip, 0),\n    delta = scales::percent(delta, accuracy = 1)\n  ) |&gt;\n  mutate(\n    avg_median_income = scales::dollar(avg_median_income),\n    avg_median_income_zip = scales::dollar(avg_median_income_zip)\n  ) |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Neighborhoods with Smallest and Largest Median Income\",\n    subtitle = \"2011-2022\"\n  ) |&gt;\n  tab_options(\n    table.font.size = 10,\n    heading.align = \"center\"\n  ) |&gt;\n  cols_label(\n    avg_median_income = \"Borough Income\",\n    avg_median_income_zip = \"Neighborhood Income\",\n    delta = \"Delta vs Borough Average (%)\"\n  ) |&gt;\n  cols_align(\n    align = \"left\",\n    columns = everything()\n  )\n\n\n\n\n\n\n\n\nNeighborhoods with Smallest and Largest Median Income\n\n\n2011-2022\n\n\nBorough\nNeighborhood\nBorough Income\nNeighborhood Income\nDelta vs Borough Average (%)\n\n\n\n\nBronx\nHunts Point and Mott Haven\n$43,203\n$25,680\n-41%\n\n\nBronx\nKingsbridge and Riverdale\n$43,203\n$70,931\n64%\n\n\nBrooklyn\nEast New York and New Lots\n$58,857\n$39,342\n-33%\n\n\nBrooklyn\nNorthwest Brooklyn\n$58,857\n$97,621\n66%\n\n\nManhattan\nEast Harlem\n$95,368\n$30,806\n-68%\n\n\nManhattan\nLower Manhattan\n$95,368\n$156,023\n64%\n\n\nQueens\nWest Queens\n$69,152\n$57,176\n-17%\n\n\nQueens\nNortheast Queens\n$69,152\n$87,204\n26%\n\n\nStaten Island\nStapleton and St. George\n$77,817\n$64,223\n-17%\n\n\nStaten Island\nSouth Shore\n$77,817\n$91,771\n18%\n\n\n\n\n\n\n\n\n\nCode - Ridge plot of median income by borough\ncustom_colors &lt;- c(\n  \"Staten Island\" = \"#E31A1C\",\n  \"Bronx\" = \"dodgerblue2\",\n  \"Brooklyn\" = \"green4\",\n  \"Queens\" = \"orange\",\n  \"Manhattan\" = \"black\"\n)\n\nggplot(\n  tmap_zip_unrate_inc_for_sf,\n  aes(\n    x = median_income,\n    y = Borough,\n    fill = Borough\n  )\n) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_manual(values = custom_colors) +\n  scale_x_continuous(labels = label_number(scale = 1, big.mark = \",\")) +\n  labs(\n    title = \"Distribution of Median Income in Neighborhoods by Boroughs\",\n    x = \"Median Income\",\n    y = \"Borough\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nPlotting median income for all available years and neighborhoods, we can see the underlying differences among counties more clearly. Majority of neighborhoods in Brooklyn and the Bronx tend to be more low-income, whereas Queens appears to be solidly middle-class, and Manhattan has both poverty and extreme wealth.\n\n\nCode - Scatter plot of unemployment Vs median income\nunrate_mi_plot &lt;- plot_ly(\n  data = tmap_zip_unrate_inc_for_sf,\n  x = ~median_income,\n  y = ~unrate,\n  color = ~Borough,\n  type = \"scatter\",\n  mode = \"markers\",\n  colors = palette_5counties,\n  marker = list(size = 4),\n  text = ~Borough, # Hover text\n  hoverinfo = \"text+x+y\"\n) |&gt;\n  layout(\n    title = \"Scatter Plot of Unemployment Rate vs Median Income by Borough\",\n    xaxis = list(title = \"Median Income\"),\n    yaxis = list(title = \"Unemployment Rate\")\n  )\n\nunrate_mi_plot\n\n\n\n\n\n\nThe nature of the relationship between median income and unemployment could be seen in this chart - it’s negative and relatively strong. We can also check if there exists a regression relation between the two:\n\n\nCode - Plotting fitted regression line\ntmap_zip_unrate_inc_for_sf &lt;- tmap_zip_unrate_inc_for_sf |&gt;\n  mutate(\n    median_income_k = median_income / 1000,\n    unrate_pct = unrate / 100\n  )\n\n# Scatter plot with regression line\nggplot(tmap_zip_unrate_inc_for_sf,\n       aes(x = median_income_k,\n           y = unrate_pct)) +\n  geom_point(color = \"grey\", size = 1.5) +        \n  geom_smooth(method = \"lm\", color = \"red\", se = TRUE) +\n  labs(\n    title = \"Scatter Plot with Fitted Regression Line\",\n    x = \"Median Income ('000s)\",\n    y = \"Unemployment Rate\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode - Checking regression relation\nlinear_model &lt;- lm(unrate_pct ~ median_income_k,\n  data = tmap_zip_unrate_inc_for_sf\n)\n\n# Estimation of regression function\nsummary(linear_model)\n\n\n\nCall:\nlm(formula = unrate_pct ~ median_income_k, data = tmap_zip_unrate_inc_for_sf)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.095270 -0.017590 -0.003278  0.014747  0.121576 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.327e-01  1.381e-03   96.12   &lt;2e-16 ***\nmedian_income_k -7.225e-04  1.777e-05  -40.67   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0272 on 2098 degrees of freedom\nMultiple R-squared:  0.4408,    Adjusted R-squared:  0.4405 \nF-statistic:  1654 on 1 and 2098 DF,  p-value: &lt; 2.2e-16\n\n\n44% of variability in unemployment rate is reduced when the median income is considered. Based on a very small p-value (~0.0) of F test, we can conclude that this linear regression model is significant and useful and median income could indeed be a significant predictor of unemployment rate. Median income and unemployment rate have a correlation coefficient of -0.66 However, because the cause-and-effect relationship between income and unemployment rate is not clear - higher median income could very well be driven by lower unemployment rate - we suggest considering other variables for further analysis."
  },
  {
    "objectID": "for_course_project_v2.html#racial-diversity",
    "href": "for_course_project_v2.html#racial-diversity",
    "title": "What Areas of New York Are Most Affected by Unemployment?",
    "section": "racial diversity",
    "text": "racial diversity\n\n\nShow the code\n## loop for all years - available from 2011 to 2022\nyears &lt;- lst(2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022)\n\n#list of NYC zipcodes\nzctas &lt;- as.list(as.character(nyc_zip_codes_list$ZipCode)) \n\n# Initialize an empty list to store results\nresults  &lt;- list()\n\n# Loop through each year\nfor (year in years) {\n  message(\"Fetching data for year: \", year)  # Print progress\n  \n  tryCatch({\n    # Fetch ACS data for the current year\n    data &lt;- get_acs(\n      geography = \"zcta\",  # ZIP Code Tabulation Areas\n      variables = c(\n     white_alone = \"B02001_002E\",  # White alone\n    black_or_african_american_alone = \"B02001_003E\",  # Black or African American alone\n    american_indian_and_alaska_native_alone = \"B02001_004E\",  # American Indian and Alaska Native alone\n    asian_alone = \"B02001_005E\",  # Asian alone\n    native_hawaiian_and_other_pacific_islander_alone = \"B02001_006E\",  # Native Hawaiian and Other Pacific Islander alone\n    some_other_race_alone = \"B02001_007E\",  # Some other race alone\n    two_or_more_races = \"B02001_008E\",  # Two or more races\n    hispanic_or_latino = \"B03001_003E\"  # Hi\n        ),\n      year = year,\n      survey = \"acs5\"  # 5-year estimates\n    )\n    \n    # Store the data in the results list\n    results[[as.character(year)]] &lt;- data\n  }, error = function(e) {\n    message(\"Error fetching data for year: \", year, \": \", e$message)\n  })\n}\n\n# Combine all results into a single dataframe\ncombined_data_race &lt;- bind_rows(results, .id = \"year\")\n\ncombined_data_race$zcta&lt;-gsub(\"ZCTA5 \",\"\",combined_data_race$NAME)\n#head(combined_data)\n\nnyc_zip_codes&lt;-nyc_zip_codes_list |&gt;\n  mutate(zcta=as.character(ZipCode))\n\ncombined_data_race_nyc&lt;-inner_join(combined_data_race,nyc_zip_codes_list, by=c(\"zcta\"=\"zcta\"))\n\n\n\ncombined_data_race2 &lt;- combined_data_race_nyc %&gt;%\n      mutate(variable2 = recode(variable,\n      \"B02001_002\"=\"white_alone\",  # White alone\n     \"B02001_003\"=\"black_or_african_american_alone\",  # Black or African American alone\n     \"B02001_004\"=\"american_indian_and_alaska_native_alone\",  # American Indian and Alaska Native alone\n     \"B02001_005\"=\"asian_alone\",  # Asian alone\n     \"B02001_006\"=\"native_hawaiian_and_other_pacific_islander_alone\",  # Native Hawaiian and Other Pacific Islander alone\n     \"B02001_007\"=\"some_other_race_alone\",  # Some other race alone\n     \"B02001_008\"=\"two_or_more_races\",  # Two or more races\n     \"B03001_003\"=\"hispanic_or_latino\"  # Hisp\n\n                               ))\n\nhead(combined_data_race2)\n\n# A tibble: 6 × 11\n  year  GEOID   NAME  variable estimate   moe zcta  Borough Neighborhood ZipCode\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 2011  3610301 ZCTA… B02001_…    24018  1515 10301 Staten… Stapleton a…   10301\n2 2011  3610307 ZCTA… B02001_…    13721   794 10307 Staten… South Shore    10307\n3 2011  3610460 ZCTA… B02001_…     7426   907 10460 Bronx   Central Bro…   10460\n4 2011  3610465 ZCTA… B02001_…    28721  1561 10465 Bronx   Southeast B…   10465\n5 2011  3610470 ZCTA… B02001_…     6849   547 10470 Bronx   Northeast B…   10470\n6 2011  3610471 ZCTA… B02001_…    16085  1025 10471 Bronx   Kingsbridge…   10471\n# ℹ 1 more variable: variable2 &lt;chr&gt;\n\n\n\nrace_zip&lt;-combined_data_race2 |&gt;\n  select(-moe)\n\nrace_zip_pivot&lt;-pivot_wider(race_zip,\n                             id_cols = c(\"year\",\"GEOID\",\"NAME\",\"zcta\",\"Borough\",      \"Neighborhood\",\"ZipCode\"),\n  names_from = variable2,\n  values_from = estimate)\n\nhead(race_zip_pivot)\n\n# A tibble: 6 × 15\n  year  GEOID   NAME        zcta  Borough       Neighborhood ZipCode white_alone\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 2011  3610301 ZCTA5 10301 10301 Staten Island Stapleton a…   10301       24018\n2 2011  3610307 ZCTA5 10307 10307 Staten Island South Shore    10307       13721\n3 2011  3610460 ZCTA5 10460 10460 Bronx         Central Bro…   10460        7426\n4 2011  3610465 ZCTA5 10465 10465 Bronx         Southeast B…   10465       28721\n5 2011  3610470 ZCTA5 10470 10470 Bronx         Northeast B…   10470        6849\n6 2011  3610471 ZCTA5 10471 10471 Bronx         Kingsbridge…   10471       16085\n# ℹ 7 more variables: black_or_african_american_alone &lt;dbl&gt;,\n#   american_indian_and_alaska_native_alone &lt;dbl&gt;, asian_alone &lt;dbl&gt;,\n#   native_hawaiian_and_other_pacific_islander_alone &lt;dbl&gt;,\n#   some_other_race_alone &lt;dbl&gt;, two_or_more_races &lt;dbl&gt;,\n#   hispanic_or_latino &lt;dbl&gt;\n\n\n\nnumeric_cols2 &lt;- 8:15\n\nrow_sum2 &lt;- rowSums(race_zip_pivot[, numeric_cols2])\n\nrace_zip_pivot[, paste0(\"pct_\", names(race_zip_pivot)[numeric_cols2])] &lt;- race_zip_pivot[, numeric_cols2] / row_sum2 * 100\n\nhead(race_zip_pivot)\n\n# A tibble: 6 × 23\n  year  GEOID   NAME        zcta  Borough       Neighborhood ZipCode white_alone\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 2011  3610301 ZCTA5 10301 10301 Staten Island Stapleton a…   10301       24018\n2 2011  3610307 ZCTA5 10307 10307 Staten Island South Shore    10307       13721\n3 2011  3610460 ZCTA5 10460 10460 Bronx         Central Bro…   10460        7426\n4 2011  3610465 ZCTA5 10465 10465 Bronx         Southeast B…   10465       28721\n5 2011  3610470 ZCTA5 10470 10470 Bronx         Northeast B…   10470        6849\n6 2011  3610471 ZCTA5 10471 10471 Bronx         Kingsbridge…   10471       16085\n# ℹ 15 more variables: black_or_african_american_alone &lt;dbl&gt;,\n#   american_indian_and_alaska_native_alone &lt;dbl&gt;, asian_alone &lt;dbl&gt;,\n#   native_hawaiian_and_other_pacific_islander_alone &lt;dbl&gt;,\n#   some_other_race_alone &lt;dbl&gt;, two_or_more_races &lt;dbl&gt;,\n#   hispanic_or_latino &lt;dbl&gt;, pct_white_alone &lt;dbl&gt;,\n#   pct_black_or_african_american_alone &lt;dbl&gt;,\n#   pct_american_indian_and_alaska_native_alone &lt;dbl&gt;, pct_asian_alone &lt;dbl&gt;, …\n\n\n\n## join zip code data and zip sf file\n\nrace_zip_pivot_for_sf&lt;-left_join(race_zip_pivot,\n                             nyc_zip_codes_sf2,\n                             by=c(\"zcta\"=\"GEOID10\") )\n\ntmap_race_zip_pivot_for_sf&lt;-race_zip_pivot_for_sf|&gt;\n  select(year,\n         pct_white_alone,                                     \npct_black_or_african_american_alone,                \npct_american_indian_and_alaska_native_alone,        \npct_asian_alone,                                     \npct_native_hawaiian_and_other_pacific_islander_alone,\npct_some_other_race_alone,\npct_two_or_more_races,                           \npct_hispanic_or_latino,\nzcta,\nNeighborhood.x,\nBorough.x,\ngeometry) \n\n\n\nShow the code\n### join maps data for tmap\n\n\nmap &lt;- race_zip_pivot_for_sf |&gt;\n  st_as_sf() |&gt;\n  tm_shape() +\n  tm_polygons(\"pct_black_or_african_american_alone\", \n              title = \"Percentage of Population with Graduate Degrees\", \n              palette = \"Greys\",  # Adjust palette as desired\n              style = \"cont\",\n              fill.na = \"red\") +   # Continuous scale\n  tm_facets(along = \"year\", free.coords = FALSE) +\n  tm_layout(frame = FALSE)\n # tm_text(\"label\", size = 1, col = \"red\", fontface = \"bold\")\n\ninvisible(capture.output({tmap_animation(map, \n               filename = \"nyc_zip_black.gif\", \n               width = 800, \n               height = 600, \n               delay = 45) \n}))\n\nknitr::include_graphics(\"nyc_zip_black.gif\")\n\n\n\n\n\n\n\n\n\nwhite_alone = \"B02001_002E\",  # White alone\nblack_or_african_american_alone = \"B02001_003E\",  # Black or African American alone\namerican_indian_and_alaska_native_alone = \"B02001_004E\",  # American Indian and Alaska Native alone\nasian_alone = \"B02001_005E\",  # Asian alone\nnative_hawaiian_and_other_pacific_islander_alone = \"B02001_006E\",  # Native Hawaiian and Other Pacific Islander alone\nsome_other_race_alone = \"B02001_007E\",  # Some other race alone\ntwo_or_more_races = \"B02001_008E\",  # Two or more races\nhispanic_or_latino = \"B03001_003E\",  # Hi\n“B02001_002”=“white_alone”, # White alone “B02001_003”=“black_or_african_american_alone”, # Black or African American alone “B02001_004”=“american_indian_and_alaska_native_alone”, # American Indian and Alaska Native alone “B02001_005”=“asian_alone”, # Asian alone “B02001_006”=“native_hawaiian_and_other_pacific_islander_alone”, # Native Hawaiian and Other Pacific Islander alone “B02001_007”=“some_other_race_alone”, # Some other race alone “B02001_008”=“two_or_more_races”, # Two or more races “B03001_003”=“hispanic_or_latino” # Hisp\n white_alone = \"B02001_002E\",  # White alone\nblack_or_african_american_alone = \"B02001_003E\",  # Black or African American alone\namerican_indian_and_alaska_native_alone = \"B02001_004E\",  # American Indian and Alaska Native alone\nasian_alone = \"B02001_005E\",  # Asian alone\nnative_hawaiian_and_other_pacific_islander_alone = \"B02001_006E\",  # Native Hawaiian and Other Pacific Islander alone\nsome_other_race_alone = \"B02001_007E\",  # Some other race alone\ntwo_or_more_races = \"B02001_008E\",  # Two or more races\nhispanic_or_latino = \"B03001_003E\"  # Hi"
  }
]